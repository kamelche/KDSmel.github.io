<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Nancy&#39;s Notes</title>
  
  <subtitle>Code changes world!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://nancyyanyu.github.io/"/>
  <updated>2020-07-16T22:43:29.428Z</updated>
  <id>https://nancyyanyu.github.io/</id>
  
  <author>
    <name>Nancy Yan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A/B Testing : Pitfalls, Baysian &amp; Math Behind</title>
    <link href="https://nancyyanyu.github.io/posts/db6945d3/"/>
    <id>https://nancyyanyu.github.io/posts/db6945d3/</id>
    <published>2020-07-16T20:22:59.000Z</published>
    <updated>2020-07-16T22:43:29.428Z</updated>
    
    <content type="html"><![CDATA[<p>My study note of various A/B testing resources. I do not own any credit for the any idea, knowledge, code below.</p><a id="more"></a><h1 id="common-pitfalls">Common Pitfalls</h1><h2 id="network-effect">Network Effect</h2><p><strong>Limitations</strong> of per-user assignment to run an A/B test on <strong><em>user-to-user features</em></strong> or product relies heavily on interaction between users:</p><ol type="1"><li><p>If you let the test group use video chat with anybody, the people in the control group wouldn't really be a control group because they're getting exposed to this new video chat feature.</p></li><li><p>Can't measure &quot;<strong>higher-order effects</strong>&quot; (also known as <strong>network effects</strong>)</p><p><strong>Network effects</strong>: occur when the changes induced by a new feature leak out of the test group and affect behavior in the control group as well.</p><ul><li>The change would theoretically improve the experience for test group as well as the control group</li><li>Higher-order effects might create an illusory change that disappears once you roll out a feature out to everybody.</li></ul></li></ol><h3 id="solution-using-per-community-random-assignment"><strong>Solution: Using per-<em>community</em> random assignment</strong></h3><blockquote><p>a &quot;<strong>community</strong>&quot; is any group of users whose interactions are primarily directed to other users within the same group.</p></blockquote><p><strong>How to define a &quot;community&quot;?</strong></p><p>Model the relationships between users with a <a href="https://en.wikipedia.org/wiki/Social_graph" target="_blank" rel="noopener">social graph</a>, each user is a node, and edges are placed between nodes that have had some interaction. And then apply <strong>graph partitioning</strong> algorithms like <a href="https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf" target="_blank" rel="noopener">Normalized Cuts</a> to find isolated, non-interacting groups.</p><p><strong>Pitfall</strong>: In dating apps, the community is really defined by &quot;anybody that's near you&quot; as opposed to &quot;people you have a history of interacting with&quot;.</p><h3 id="solution-defining-geographic-communities">Solution: Defining geographic communities</h3><p>Define 'optimal' regions by drawing boundaries that maximizes the <u>number of connections within each region</u> relative to the <u>number of connections that occur across regions</u> using Shi &amp; Malik's <a href="https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf" target="_blank" rel="noopener">Normalized Cut</a> algorithm for graph partitioning.</p><ol type="1"><li><p>Build a graph to describe how many messages were sent between pairs of US/Canadian locations as the square adjacency matrix <strong>A</strong> (size <em>num_Locations</em>-by-<em>num_Locations</em>). The value at <strong>A</strong>[<em>i</em>,<em>j</em>] set to the number of messages sent between users in location <em>i</em> to users in location <em>j</em></p></li><li><p>Calculate the graph <em>Laplacian</em>, <strong>L</strong>, from the <em>adjacency</em> matrix, <strong>A</strong>, and the <em>degree matrix</em>, <strong>D</strong>, as <strong>L</strong> = <strong>D</strong> - <strong>A</strong>. The degree matrix is just a diagonal matrix with the degree of each node on the diagonal and zeros everywhere else.</p></li></ol><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/6n-graph2.svg/350px-6n-graph2.svg.png">,<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e6f63ff7144996fb004e3d4b211c2c73c5e59fbd"></p><ol start="3" type="1"><li>Calculate the eigendecomposition of <strong>L</strong>, and inspect its eigenvalues. Each fully-isolated (<em>i.e.</em>, disconnected) sets of nodes will be represented by an eigenvector with an associated eigenvalue of 0. Further segmentation of these regions is done by looking at the patterns in the eigenvectors with the <em>k</em> smallest, non-zero eigenvalues. The value of <em>k</em> can be varied depending on how many regions you want to make.</li></ol><h2 id="history-effect">History Effect</h2><p><strong>History Effect</strong> happens when an event from the outside world skews your testing data</p><p>A traffic spike that‚Äôs a direct result of an unusual event means there‚Äôs a high probability that those visitors might have different needs, wants and browsing behaviours.</p><p>Because this traffic is only <em>temporary</em>, it means your test data could shift completely during this event and could result in making one of your variations win when in reality, with your regular traffic, it should‚Äôve lost.</p><p><strong>Solution</strong></p><ul><li><strong>Prevention</strong>: run your test for longer to get some of your usual traffic into the mix as well.</li><li>Be aware of the fluctuations and differences in your traffic: when you‚Äôre aware of what‚Äôs happening, dig deeper in Google Analytics to analyze variations‚Äô performance, and then recognize if your winning variation is indeed a winner.</li><li>Launch a test that‚Äôs only targeting traffic coming from certain traffic sources (excluding the most variable traffic source from the test entirely)</li><li>Never analyze a test solely by testing tools like <a href="https://vwo.com/" target="_blank" rel="noopener">VWO</a>, <a href="http://optimizely.com/" target="_blank" rel="noopener">Optimizely</a>. They don‚Äôt allow you to dig as deeply into your analysis as with <em>Google Analytics</em>: use your testing tool for running your tests. Use Google Analytics to analyze them.</li></ul><h2 id="instrumentation-effect">Instrumentation Effect</h2><p><strong>Instrumentation Effect</strong> happens when there are problems with your testing tools or test variations that cause your data to be flawed. -&gt; companies new to testing</p><p>A common example is when the code of one or more of your variations is not functioning properly with all devices or browser types, and often, without the company who is running the test even being aware.</p><p><strong>Solution</strong></p><ul><li><p>rigorous <strong>Quality Assurance (QA)</strong> checks: performing cross-browser and cross-device testing on new variations, and trying out variations under different user scenarios.</p><p>To test on different devices and tools, you can use online tools such as <a href="http://www.browserstack.com/" target="_blank" rel="noopener">Browserstack</a> (which guarantees 100% accuracy for cross-browser functionalities), or <a href="https://www.browserling.com/" target="_blank" rel="noopener">Browserling</a>.</p></li></ul><h2 id="selection-effect">Selection Effect</h2><p>Selection Effect: Each traffic source brings its own type of visitors, and you can‚Äôt assume that traffic from a certain channel mirrors the behaviors, context, mindset and needs of the totality of your usual traffic.</p><p>‚Äç</p><p><em>if you run an A/B test and the traffic sample is not a good representative of the average visitors to your website then you are not going to get an accurate insight on how your website visitors respond to different landing page variations (unless off course if you are running your test only for a particular traffic segment).</em></p><p><strong>Solution</strong></p><ul><li>When you‚Äôre analyzing the test results, make sure to <strong>segment by sources</strong> in order to see the real data that lies behind averages.</li></ul><h2 id="novelty-effect">Novelty Effect</h2><p><strong>Novelty Effect</strong> happens when the engagement and interaction with one of your variations is substantially higher than previously, but only temporarily ‚Äì giving you a false positive.</p><p><strong>Solution</strong></p><ul><li><p>Run your test for at least 4 weeks. In most cases, 4 weeks will be enough time to start seeing the novelty wear off and the test results begin to regulate.</p></li><li>Keep tracking the winning variation's performance to ensure its long-term performance.</li><li><a href="https://splitbase.com/blog/conversion-research-ecommerce/#Whatrsquos_a_session_recording" target="_blank" rel="noopener">analyze session recordings</a> and <a href="https://splitbase.com/blog/conversion-research-ecommerce/#Method_2_User_Testing" target="_blank" rel="noopener">run usability tests</a> in order to understand the user behavior that‚Äôs happening on your site.</li><li><p>Cohort analysis: <em>Segment your visitors into new and returning visitors and compare the conversion rates.‚Äç</em> If it‚Äôs just the Novelty Effect, the new offer will win with new visitors. Eventually, as returning visitors get accustomed to the new changes, the offer will win with them, too</p></li></ul><h2 id="statistical-regression">Statistical Regression</h2><p><strong>Statistical Regression</strong> (regression to the mean):if a variable is extreme on its first measurement, it will tend to be closer to the average on its second measurement.</p><blockquote><p>Launched an A/B test and noticed wild fluctuations during the first few days of it being live.</p></blockquote><p>What this means is that if you end a test too early or based only on reaching statistical significance, you‚Äôll likely see a false positive.</p><p><strong>Solution</strong></p><ul><li><p>You can't avoid Statistical Regression, but can avoid to let it ruin your A/B test results by <em>not ending your test solely based on when you reach statistical significance</em>.</p></li><li><p>Before you end a test, make sure you‚Äôve had a large enough sample size.</p></li></ul><h1 id="bayesian-ab-testing">Bayesian A/B Testing</h1><p><strong>The Frequentist approach</strong>: a Frequentist method makes predictions on the underlying truths of the experiment using only data from the <em>current</em> experiment.</p><p><strong>The Bayesian approach</strong>: <em>past knowledge of similar experiments is encoded into a statistical device known as a prior, and this prior is combined with current experiment data to make a conclusion on the test at hand.</em></p><p><strong>Difference:</strong></p><ul><li><p>The biggest distinction is that Bayesian probability specifies some <strong>prior probability</strong>.</p></li><li><p>The metric you want to study is, for the frequentists, a simple number, while for bayesians, a <strong>distribution</strong>.</p></li></ul><h1 id="math-behind-ab-testing">Math Behind A/B Testing</h1><h2 id="set-up-the-experiment">Set Up The Experiment</h2><h3 id="baseline-conversion-rate-and-minimum-difference-between-test-and-control">Baseline Conversion Rate and minimum difference between test and control</h3><p>For our example, we want to use our test to confirm that the changes we make to our signup process will result in at least a 2% increase in our sign up rate. We currently sign up 10 out of 100 users who are offered a premium account.</p><p><strong>baseline conversion rate</strong>: the current rate at which we sign up new users under the existing design.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bcr = <span class="number">0.10</span>  <span class="comment"># baseline conversion rate</span></span><br><span class="line">d_hat = <span class="number">0.02</span>  <span class="comment"># difference between the groups</span></span><br></pre></td></tr></table></figure><h3 id="control-group-a-and-test-group-b">Control Group (A) and Test Group (B)</h3><p>Initially, we will collect 1000 users for each group and serve the current signup page to the control group and a new signup page to the test group.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # A is control; B is test</span></span><br><span class="line">N_A = <span class="number">1000</span></span><br><span class="line">N_B = <span class="number">1000</span></span><br></pre></td></tr></table></figure><h2 id="run-the-test">Run the Test</h2><h3 id="generate-fake-data">Generate Fake data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(N_A, N_B, p_A, p_B, days=None, control_label=<span class="string">'A'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  test_label=<span class="string">'B'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Returns a pandas dataframe with fake CTR data</span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        N_A (int): sample size for control group</span></span><br><span class="line"><span class="string">        N_B (int): sample size for test group</span></span><br><span class="line"><span class="string">            Note: final sample size may not match N_A provided because the</span></span><br><span class="line"><span class="string">            group at each row is chosen at random (50/50).</span></span><br><span class="line"><span class="string">        p_A (float): conversion rate; conversion rate of control group</span></span><br><span class="line"><span class="string">        p_B (float): conversion rate; conversion rate of test group</span></span><br><span class="line"><span class="string">        days (int): optional; if provided, a column for 'ts' will be included</span></span><br><span class="line"><span class="string">            to divide the data in chunks of time</span></span><br><span class="line"><span class="string">            Note: overflow data will be included in an extra day</span></span><br><span class="line"><span class="string">        control_label (str)</span></span><br><span class="line"><span class="string">        test_label (str)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        df (df)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initiate empty container</span></span><br><span class="line">    data = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># total amount of rows in the data</span></span><br><span class="line">    N = N_A + N_B</span><br><span class="line"></span><br><span class="line">    <span class="comment"># distribute events based on proportion of group size</span></span><br><span class="line">    group_bern = scs.bernoulli(N_A / (N_A + N_B))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initiate bernoulli distributions from which to randomly sample</span></span><br><span class="line">    A_bern = scs.bernoulli(p_A)</span><br><span class="line">    B_bern = scs.bernoulli(p_B)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="comment"># initite empty row</span></span><br><span class="line">        row = &#123;&#125;</span><br><span class="line">        <span class="comment"># for 'ts' column</span></span><br><span class="line">        <span class="keyword">if</span> days <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> type(days) == int:</span><br><span class="line">                row[<span class="string">'ts'</span>] = idx // (N // days)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">"Provide an integer for the days parameter."</span>)</span><br><span class="line">        <span class="comment"># assign group based on 50/50 probability</span></span><br><span class="line">        row[<span class="string">'group'</span>] = group_bern.rvs()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> row[<span class="string">'group'</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># assign conversion based on provided parameters</span></span><br><span class="line">            row[<span class="string">'converted'</span>] = A_bern.rvs()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            row[<span class="string">'converted'</span>] = B_bern.rvs()</span><br><span class="line">        <span class="comment"># collect row into data container</span></span><br><span class="line">        data.append(row)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert data into pandas dataframe</span></span><br><span class="line">    df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># transform group labels of 0s and 1s to user-defined group labels</span></span><br><span class="line">    df[<span class="string">'group'</span>] = df[<span class="string">'group'</span>].apply(</span><br><span class="line">        <span class="keyword">lambda</span> x: control_label <span class="keyword">if</span> x == <span class="number">0</span> <span class="keyword">else</span> test_label)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ab_data = generate_data(N_A, N_B, bcr, bcr+d_hat)</span><br><span class="line">ab_data.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }        .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>group</th><th>converted</th></tr></thead><tbody><tr><th>0</th><td>B</td><td>0</td></tr><tr><th>1</th><td>A</td><td>0</td></tr><tr><th>2</th><td>B</td><td>0</td></tr><tr><th>3</th><td>B</td><td>0</td></tr><tr><th>4</th><td>A</td><td>0</td></tr></tbody></table></div><h2 id="summary-of-ab-data">Summary of AB data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ab_summary = ab_data.pivot_table(</span><br><span class="line">    index=<span class="string">'group'</span>, values=<span class="string">'converted'</span>, aggfunc=np.sum)</span><br><span class="line">ab_summary[<span class="string">'total'</span>] = ab_data.pivot_table(</span><br><span class="line">    index=<span class="string">'group'</span>, values=<span class="string">'converted'</span>, aggfunc=<span class="keyword">lambda</span> x: len(x))</span><br><span class="line">ab_summary[<span class="string">'rate'</span>] = ab_data.pivot_table(values=<span class="string">'converted'</span>, index=<span class="string">'group'</span>)</span><br><span class="line">ab_summary</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }        .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>converted</th><th>total</th><th>rate</th></tr><tr><th>group</th><th></th><th></th><th></th></tr></thead><tbody><tr><th>A</th><td>107</td><td>1002</td><td>0.106786</td></tr><tr><th>B</th><td>132</td><td>998</td><td>0.132265</td></tr></tbody></table></div><p>It looks like the difference in conversion rates between the two groups is 0.026 which is greater than the <span class="math inline">\(\hat{d}\)</span> we initially wanted of 0.02. <strong>This is a good sign but this is not enough evidence for us to confidently go with the new design.</strong> At this point we have not measured how confident we are in this result. This can be mitigated by looking at the distributions of the two groups.</p><h2 id="compare-the-two-groups">Compare the Two Groups</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A_converted, B_converted = ab_summary[<span class="string">'converted'</span>][<span class="string">'A'</span>],ab_summary[<span class="string">'converted'</span>][<span class="string">'B'</span>]</span><br><span class="line">A_total,B_total = ab_summary[<span class="string">'total'</span>][<span class="string">'A'</span>],ab_summary[<span class="string">'total'</span>][<span class="string">'B'</span>]</span><br><span class="line">p_A,p_B = ab_summary[<span class="string">'rate'</span>][<span class="string">'A'</span>],ab_summary[<span class="string">'rate'</span>][<span class="string">'B'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">xa = np.linspace(A_converted<span class="number">-49</span>, A_converted+<span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line">ya = scs.binom(A_total, p_A).pmf(xa)</span><br><span class="line">xb = np.linspace(B_converted<span class="number">-49</span>, B_converted+<span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line">yb = scs.binom(B_total, p_B).pmf(xb)</span><br><span class="line"></span><br><span class="line">ax.bar(xa, ya, alpha=<span class="number">0.5</span>)</span><br><span class="line">ax.bar(xb, yb, alpha=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'converted'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'probability'</span>)</span><br></pre></td></tr></table></figure><p>Text(0, 0.5, 'probability')</p><figure><img src="Math%20behind_15_1.png" alt="png"><figcaption>png</figcaption></figure><p>We can see that the test group converted more users than the control group. We can also see that the peak of the test group results is lower than the control group. How do we interpret the difference in peak probability? We should focus instead on the conversion rate so that we have an apples-to-apples comparison. In order to calculate this, we need to standardize the data and compare the probability of successes, p, for each group.</p><h3 id="bernoulli-distribution-and-the-central-limit-theorem">Bernoulli Distribution and the Central Limit Theorem</h3><p>To do this, first, consider the <strong>Bernoulli</strong> distribution for the <em>control</em> group.</p><p><span class="math display">\[X \sim \mathcal{Bernoulli}(p)\]</span></p><p>where p is the conversion probability of the control group.</p><p>According to the properties of the Bernoulli distribution, the mean and variance are as follows:</p><p><span class="math display">\[E(X)=p \\Var(X)=p(1-p)\]</span></p><p>According to the <strong>central limit theorem</strong>, by calculating many <strong>sample means</strong> we can approximate the true mean of the population, <span class="math inline">\(ùúá\)</span>, from which the data for the control group was taken. The distribution of the sample means, <span class="math inline">\(p\)</span>, will be normally distributed around the true mean with a standard deviation equal to the <strong>standard error of the mean</strong>. The equation for this is given as: <span class="math display">\[\sigma_{\bar{x}}=\frac{s}{\sqrt{n}} = \sqrt{\frac{p(1-p)}{n}}\]</span></p><p>Therefore, we can represent both groups as a normal distribution with the following properties: <span class="math display">\[\hat{p} \sim \mathcal{N}(p,\sqrt{\frac{p(1-p)}{n}})\]</span></p><p>The same can be done for the test group. So, we will have two normal distributions for <span class="math inline">\(p_A\)</span> and <span class="math inline">\(p_B\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># standard error of the mean for both groups</span></span><br><span class="line"></span><br><span class="line">SE_A=np.sqrt(p_A*(<span class="number">1</span>-p_A)/A_total)</span><br><span class="line">SE_B=np.sqrt(p_B*(<span class="number">1</span>-p_B)/B_total)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the null and alternative hypothesis</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">.2</span>, <span class="number">1000</span>)</span><br><span class="line">yA = scs.norm(p_A, SE_A).pdf(x)</span><br><span class="line">ax.plot(x, yA)</span><br><span class="line">ax.axvline(x=p_A, c=<span class="string">'red'</span>, alpha=<span class="number">0.5</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">yB = scs.norm(p_B, SE_B).pdf(x)</span><br><span class="line">ax.plot(x, yB)</span><br><span class="line">ax.axvline(x=p_B, c=<span class="string">'blue'</span>, alpha=<span class="number">0.5</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Converted Proportion'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'PDF'</span>)</span><br></pre></td></tr></table></figure><p>Text(0, 0.5, 'PDF')</p><figure><img src="Math%20behind_17_1.png" alt="png"><figcaption>png</figcaption></figure><p>The dashed lines represent the mean conversion rate for each group. The distance between the red dashed line and the blue dashed line is equal to mean difference between the control and test group. <span class="math inline">\(\hat{d}\)</span> is the distribution of the difference between random variables from the two groups. <span class="math display">\[\hat{d}=\hat{p_B}-\hat{p_A}\]</span></p><h3 id="variance-of-the-sum">Variance of the Sum</h3><p>Recall that the null hypothesis states that the <strong>difference in probability</strong> between the two groups is <span class="math inline">\(0\)</span>. Therefore, the <em>mean</em> for this normal distribution will be at <span class="math inline">\(0\)</span>.</p><p>A basic property of variance is that the variance of the sum of two random independent variables is the sum of the variances.</p><p><span class="math display">\[Var(X-Y)=Var(X)+Var(Y)\]</span></p><p>This means that the <em>null hypothesis</em> and <em>alternative hypothesis</em> will have the same variance which will be the sum of the variances for the control group and the test group.</p><p><span class="math display">\[Var(\hat{d})=Var({\hat{p_B}-\hat{p_A}})=Var(\hat{p_B})+Var(\hat{p_A})=\frac{\hat{p_B}(1-\hat{p_B})}{n_B}+\frac{\hat{p_A}(1-\hat{p_A})}{n_A}\]</span></p><p>The standard deviation can then be calculated as:</p><p><span class="math display">\[SE=\sqrt{Var(\hat{d})}=\sqrt{\frac{s_A^2}{n_B}+\frac{s_B^2}{n_A}}\]</span></p><p>and we get the <strong><em>Satterthwaite approximation</em></strong> for <strong>pooled standard error</strong>. If we calculate the <strong>pooled probability</strong> and use the pooled probability to calculate the standard deviation for both groups, we get:</p><p><span class="math display">\[\sigma=\sqrt{Var(\hat{d})}=\sqrt{\frac{s_A^2}{n_B}+\frac{s_B^2}{n_A}}=\sqrt{\frac{s_{pool}^2}{n_B}+\frac{s_{pool}^2}{n_A}} = \sqrt{\hat{p}_{pool}(1-\hat{p}_{pool})\frac{1}{n_B}+\frac{1}{n_A}}\]</span></p><p>where: <span class="math display">\[\hat{p}_{pool}=\frac{p_An_A+p_Bn_B}{n_B+n_A}\]</span></p><h3 id="compare-the-null-hypothesis-vs.-the-alternative-hypothesis">Compare the Null Hypothesis vs. the Alternative Hypothesis</h3><p><strong>null hypothesis</strong> : the change in the design made for the test group would <strong>result in no change</strong> in the conversion rate.</p><p><strong>alternative hypothesis</strong> : the change in the design for the test group would <strong>result in an improvement</strong> (or reduction) in the conversion rate.</p><p>The <strong>null hypothesis</strong> will be a <em>normal distribution</em> with a mean of zero and a standard deviation equal to the pooled standard error.</p><p><span class="math display">\[H_0: d=0 \\\hat{d}_0 \sim \mathcal{N}(0,SE_{pool})\]</span></p><p>The <strong>alternative hypothesis</strong> has the same standard deviation as the null hypothesis, but the <em>mean</em> will be located at the difference in the conversion rate, <span class="math inline">\(\hat{d}\)</span>. This makes sense because we can calculate the difference in the conversion rates directly from the data, but the normal distribution represents the possible values our experiment could have given us.</p><p><span class="math display">\[H_1: d=p_B-p_A \\\hat{d}_1 \sim \mathcal{N}(d,SE_{pool})\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">z_val</span><span class="params">(sig_level=<span class="number">0.05</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Returns the z value for a given significance level"""</span></span><br><span class="line">    z_dist = scs.norm()</span><br><span class="line">    area = <span class="number">1</span> - sig_level/<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> z_dist.ppf(area)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">confidence_interval</span><span class="params">(sample_mean=<span class="number">0</span>, sample_std=<span class="number">1</span>, sample_size=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        sig_level=<span class="number">0.05</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Returns the confidence interval as a tuple"""</span></span><br><span class="line">    z = z_val(sig_level)</span><br><span class="line"></span><br><span class="line">    left = sample_mean - z * sample_std / np.sqrt(sample_size)</span><br><span class="line">    right = sample_mean + z * sample_std / np.sqrt(sample_size)</span><br><span class="line">    <span class="keyword">return</span> (left, right)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">n = N_A + N_B</span><br><span class="line">bcr = p_A  </span><br><span class="line">d=<span class="number">0</span></span><br><span class="line">d_hat = p_B - p_A</span><br><span class="line"></span><br><span class="line"><span class="comment"># define parameters to find pooled standard error</span></span><br><span class="line">X_A = bcr * N_A</span><br><span class="line">X_B = (bcr + d_hat) * N_B</span><br><span class="line"></span><br><span class="line"><span class="comment"># pooled probability for two samples</span></span><br><span class="line">p_pool=(X_A + X_B) / (N_A + N_B)</span><br><span class="line">SE_pool = np.sqrt(p_pool * (<span class="number">1</span> - p_pool) * (<span class="number">1</span> / N_A + <span class="number">1</span> / N_B))</span><br><span class="line"></span><br><span class="line"><span class="comment"># null hypothesis: d ~ N(0,SE_pool)</span></span><br><span class="line">d=<span class="number">0</span></span><br><span class="line">x = np.linspace(d - <span class="number">12</span> * SE_pool, d + <span class="number">12</span> * SE_pool, <span class="number">1000</span>)</span><br><span class="line">y = scs.norm(d, SE_pool).pdf(x)</span><br><span class="line"><span class="comment"># null hypothsis's confidence interval</span></span><br><span class="line">left, right = confidence_interval(sample_mean=d, sample_std=SE_pool,</span><br><span class="line">                                  sig_level=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># althernative hypothsis: d ~ N(p_B-p_A,SE_pool)</span></span><br><span class="line">x_1 = np.linspace(d_hat - <span class="number">12</span> * SE_pool, d_hat + <span class="number">12</span> * SE_pool, <span class="number">1000</span>)</span><br><span class="line">y_1 = scs.norm(d_hat, SE_pool).pdf(x_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># althernative hypothsis's confidence interval</span></span><br><span class="line">left_1, right_1 = confidence_interval(sample_mean=d_hat, sample_std=SE_pool,</span><br><span class="line">                                      sig_level=<span class="number">0.05</span>)</span><br><span class="line">alternative = scs.norm(d_hat, SE_pool)</span><br><span class="line">null=scs.norm(d, SE_pool)</span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line">ax.plot(x, y, c=<span class="string">'blue'</span>,label=<span class="string">"Null"</span>)</span><br><span class="line">ax.axvline(left, c=<span class="string">'blue'</span>, linestyle=<span class="string">'--'</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">ax.axvline(right, c=<span class="string">'blue'</span>, linestyle=<span class="string">'--'</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">x_=np.linspace(<span class="number">-12</span> * SE_pool, <span class="number">12</span> * SE_pool,<span class="number">1000</span>)</span><br><span class="line">ax.fill_between(x_, <span class="number">0</span>, alternative.pdf(x_), color=<span class="string">'green'</span>, alpha=<span class="number">0.25</span>, \</span><br><span class="line">                where=(x_ &gt; right))</span><br><span class="line">ax.fill_between(x_, <span class="number">0</span>, alternative.pdf(x_), color=<span class="string">'pink'</span>, alpha=<span class="number">0.25</span>, \</span><br><span class="line">                where=(x_ &lt;= right))</span><br><span class="line">ax.fill_between(x_, <span class="number">0</span>, null.pdf(x_), color=<span class="string">'black'</span>, alpha=<span class="number">0.35</span>, \</span><br><span class="line">                where=(x_ &gt; right))</span><br><span class="line">ax.text(<span class="number">5</span> * SE_pool, null.pdf(<span class="number">0</span>),</span><br><span class="line">                <span class="string">'power = &#123;0:.3f&#125;'</span>.format(<span class="number">1</span> - alternative.cdf(right)),</span><br><span class="line">                fontsize=<span class="number">12</span>, ha=<span class="string">'right'</span>, color=<span class="string">'k'</span>)</span><br><span class="line">ax.text(<span class="number">-3</span> * SE_pool, null.pdf(<span class="number">0</span>),</span><br><span class="line">                <span class="string">'alpha right = &#123;0:.3f&#125;'</span>.format(<span class="number">1</span> - null.cdf(right)),</span><br><span class="line">                fontsize=<span class="number">12</span>, ha=<span class="string">'right'</span>, color=<span class="string">'k'</span>)</span><br><span class="line">ax.plot(x_1, y_1, c=<span class="string">'red'</span>,label=<span class="string">"Alternative"</span>)</span><br><span class="line"><span class="comment"># ax.axvline(left_1, c='red', linestyle='--', alpha=0.5)</span></span><br><span class="line"><span class="comment"># ax.axvline(right_1, c='red', linestyle='--', alpha=0.5)</span></span><br><span class="line"></span><br><span class="line">ax.set_xlim(<span class="number">-8</span> * SE_pool, <span class="number">8</span> * SE_pool)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'d'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'PDF'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="Math%20behind_22_0.png" alt="png"><figcaption>png</figcaption></figure><p>Visually, the plot for the null and alternative hypothesis looks very similar to the other plots above. Fortunately, both curves are identical in shape, so we can just compare the distance between the means of the two distributions. We can see that the alternative hypothesis curve suggests that the test group has a higher conversion rate than the control group. This plot also can be used to directly determine the statistical power.</p><h2 id="statistical-power-and-significance-level">Statistical Power and Significance Level</h2><p>The green shaded area represents the <strong>statistical power</strong>, calculated by finding the <strong><em>area under the alternative hypothesis distribution and outside of the confidence interval of the null hypothesis</em></strong>.</p><p>After running our experiment, we get a resulting conversion rate for both groups. If we calculate the difference between the conversion rates, we end up with one result, the difference or the effect of the design change. Our task is to determine which population this result came from, the null hypothesis or the alternative hypothesis.</p><p>If the alternative design is truly better, the <strong><em>power</em></strong> is the probability that we <em>accept the alternative hypothesis and reject the null hypothesis</em> and is equal to the area shaded green (<strong><em>true positive</em></strong>).</p><p>The opposite area under the alternative curve is the probability that we <em>accept the null hypothesis and reject the alternative hypothesis</em> (<strong><em>false negative</em></strong>). This is referred to as <span class="math inline">\(\beta\)</span> in A/B testing or pink shaded area.</p><p>If the null hypothesis is true and there truly is <strong>no difference</strong> between the control and test groups, then the <em>significance level</em> <span class="math inline">\(= \alpha\)</span> is the probability that we would <em>reject the null hypothesis and accept the alternative hypothesis</em> (<strong>false positive</strong>). A <strong>false positive</strong> is when we mistakenly conclude that the new design is better. This value is low because we want to limit this probability.</p><p>A typical 95% confidence level for an A/B test corresponds to a significance level of 0.05. <span class="math display">\[\alpha=1-\text{confidence interval}\]</span></p><p><img src="confusion.png"></p><p>Experiments are typically set up for a <em>minimum desired power</em> of 80%. If our new design is truly better, we want our experiment to show that there is at least an 80% probability that this is the case. Unfortunately, our current experiment only has a power of 0.419.</p><p>We know that if we <strong>increase the sample size</strong> for each group, we will <em>decrease the pooled variance</em> for our null and alternative hypothesis. This will <em>make our distributions much narrower</em> and may <em>increase the statistical power</em>. Let‚Äôs take a look at how sample size will directly affect our results.</p><h2 id="sample-size">Sample Size</h2><p>You will need the <strong>baseline conversion rate (bcr)</strong> and the <strong>minimum detectable effect</strong>, which is the minimum difference between the control and test group that you or your team will determine to be worth the investment of making the design change in the first place.</p><p><strong>Equation for minimum sample size</strong>: <span class="math display">\[n=\frac{2p_{pool}(1-p_{pool})(z_{\beta}+z_{\alpha/2})^2}{(p_B-p_A)^2}\]</span></p><p><img src="sample_size.png"></p><p><strong>Derivation of sample size formula</strong>:</p><p><span class="math display">\[\begin{align}&amp;\text{critical value}=\mu+z_{\alpha/2}*SE \\\text{Thus  }\quad &amp;0+z_{\alpha/2}*SE=\hat{d}-SE \cdot z_{power} \\\text{We can get  }\quad &amp;z_{power} = \frac{ \hat{d}-z_{\alpha/2} \cdot SE}{SE} \\\text{As  }\quad &amp;SE=\sqrt{\frac{\sigma^2}{n_1}+\frac{\sigma^2}{n_2}} \\ \text{If ratio $r=\frac{n_2}{n_1}$  }\quad &amp;SE=\sqrt{\frac{\sigma^2}{n_1}+\frac{\sigma^2}{rn_2}} \\ &amp;z_{power} = \frac{\hat{d}}{\sqrt{\frac{\sigma^2}{n_1}+\frac{\sigma^2}{rn_2}}} -z_{\alpha/2} \\&amp;z_{power} =\frac{\hat{d}}{\sqrt{\frac{(r+1)\sigma^2}{rn_1}}} -z_{\alpha/2} \\&amp;(r+1)\sigma^2(z_{power}+z_{\alpha/2})^2 =rn_1\hat{d}^2 \\&amp;n_1=\frac{r+1}{r}\frac{\sigma^2 (z_{power}+z_{\alpha/2})^2 }{\hat{d}^2} \\\text{If r=1(equal groups)  } &amp;n_1=\frac{2\sigma^2 (z_{power}+z_{\alpha/2})^2 }{\hat{d}^2} \\\end{align}\]</span></p><h4 id="sample-size-formula-for-difference-in-means">Sample Size Formula for difference in means</h4><p><span class="math display">\[n_1=\frac{r+1}{r}\frac{\sigma^2 (z_{power}+z_{\alpha/2})^2 }{\hat{d}^2}\]</span></p><p>where <span class="math display">\[\begin{align}n1 &amp;= \text{size of smaller group} \\r &amp;= \text{ratio of larger group to smaller group} \\\sigma &amp;= \text{SD of the charateristic} \\difference &amp;= \text{meaningful difference in means of 2 groups} \\z_{power} &amp;= \text{corresponds to power (0.84 for power = 0.8)} \\z_{\alpha/2}&amp;= \text{corresponds to 2-tailed significance level (1.96 for $\alpha$=0.05)} \\\end{align}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">power=<span class="number">0.8</span></span><br><span class="line">sig_level=<span class="number">0.05</span></span><br><span class="line">mde=<span class="number">0.02</span></span><br><span class="line">bcr=<span class="number">0.1</span></span><br><span class="line"><span class="comment"># standard normal distribution to determine z-values</span></span><br><span class="line">standard_norm = scs.norm(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># find Z_beta from desired power</span></span><br><span class="line">Z_beta = standard_norm.ppf(power)</span><br><span class="line"></span><br><span class="line"><span class="comment"># find Z_alpha</span></span><br><span class="line">Z_alpha = standard_norm.ppf(<span class="number">1</span>-sig_level/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># average of probabilities from both groups</span></span><br><span class="line">pooled_prob = (bcr + bcr+mde) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">min_N = (<span class="number">2</span> * pooled_prob * (<span class="number">1</span> - pooled_prob) * (Z_beta + Z_alpha)**<span class="number">2</span></span><br><span class="line">         / mde**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Minimum sample size:"</span>,int(min_N))</span><br></pre></td></tr></table></figure><p>Minimum sample size: 3842</p><h1 id="reference">Reference</h1><p><a href="https://tech.okcupid.com/the-pitfalls-of-a-b-testing-in-social-networks/" target="_blank" rel="noopener">The pitfalls of A/B testing in social networks</a></p><p><a href="https://splitbase.com/blog/ab-testing-threats" target="_blank" rel="noopener">5 Validity Threats That Will Make Your A/B Tests Useless</a></p><p><a href="https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f" target="_blank" rel="noopener">The Math Behind A/B Testing with Example Python Code</a></p><p><a href="https://medium.com/convoy-tech/the-power-of-bayesian-a-b-testing-f859d2219d5" target="_blank" rel="noopener">The Power of Bayesian A/B Testing</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;My study note of various A/B testing resources. I do not own any credit for the any idea, knowledge, code below.&lt;/p&gt;
    
    </summary>
    
      <category term="Product Sense" scheme="https://nancyyanyu.github.io/categories/Product-Sense/"/>
    
    
      <category term="ab-testing" scheme="https://nancyyanyu.github.io/tags/ab-testing/"/>
    
  </entry>
  
  <entry>
    <title>Probability &amp; Statistics Fundamental</title>
    <link href="https://nancyyanyu.github.io/posts/c27004a0/"/>
    <id>https://nancyyanyu.github.io/posts/c27004a0/</id>
    <published>2020-07-15T20:14:40.000Z</published>
    <updated>2020-07-15T23:02:52.194Z</updated>
    
    <content type="html"><![CDATA[<p>Basic Probability &amp; Statistics knowledge.</p><a id="more"></a><h1 id="random-variables">Random Variables</h1><p><strong>Bernoulli</strong></p><ul><li><strong>Story:</strong> A trial is performed with probability <span class="math inline">\(p\)</span> of 'succes', and <span class="math inline">\(X\)</span> is the indicator of success: <span class="math inline">\(1\)</span> means success, <span class="math inline">\(0\)</span> means failure.</li><li><strong>PMF:</strong> <span class="math inline">\(\begin{align}p(X=1) &amp;= p \\p(X=0) &amp;= 1 ‚àí p \end{align}\)</span></li><li><strong>Expectation:</strong> <span class="math inline">\(p\)</span></li><li><strong>Variance:</strong> <span class="math inline">\(p(1-p)\)</span></li></ul><p><strong>Binomial</strong></p><ul><li><strong>Story:</strong> <span class="math inline">\(X\)</span> is the number of 'successes' that we will achieve in <span class="math inline">\(n\)</span> independent trials, where each trial is either a success or a failure, each with the same probability <span class="math inline">\(p\)</span> of success.</li><li><strong>PMF:</strong> <span class="math inline">\(p(k)=\binom{n}{k} p^k(1-p)^{n-k}\)</span></li><li><strong>Expectation:</strong> <span class="math inline">\(np\)</span></li><li><p><strong>Variance:</strong> <span class="math inline">\(np(1-p)\)</span></p></li><li><strong>Property:</strong> Let <span class="math inline">\(X \sim \mathcal{Bin}(n,p), Y \sim \mathcal{Bin}(m,p)\)</span> with <span class="math inline">\(X \text{ independent with } Y\)</span>.<ul><li><strong>Redefine success</strong> : <span class="math inline">\(n-X \sim \mathcal{Bin}(n,1-p)\)</span></li><li><strong>Sum</strong>: <span class="math inline">\(X+Y \sim \mathcal{Bin}(n+m,p)\)</span></li><li><strong>Binomial-Poisson Relationship</strong>: <span class="math inline">\(\mathcal{Bin}(n, p)\)</span> is approximately <span class="math inline">\(\mathcal{Pois}(\lambda=np)\)</span> if <span class="math inline">\(p\)</span> is small and <span class="math inline">\(n\)</span> is large.</li><li><strong>Binomial-Normal Relationship</strong>: <span class="math inline">\(\mathcal{Bin}(n, p)\)</span> is approximately <span class="math inline">\(\mathcal{N}(np,np(1-p))\)</span> if <span class="math inline">\(n\)</span> is large and <span class="math inline">\(p\)</span> is not near <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>.</li><li><strong>Binomial-Bernoulli Relationship</strong>:let <span class="math inline">\(X_1, X_2, . . . , X_n\)</span> be <em>independent</em> <strong>Bernoulli random</strong> variables with <span class="math inline">\(p(X_i = 1) = p\)</span>. Then <span class="math inline">\(Y = X_1 + X_2 + ¬∑¬∑¬∑ + X_n\)</span> is a <strong>Binomial random variable</strong>.</li></ul></li></ul><p><strong>Geometric</strong></p><ul><li><strong>Story:</strong> <span class="math inline">\(X\)</span> is the number of ``failures&quot; that we will achieve before we achieve our first success. Our successes have probability <span class="math inline">\(p\)</span>.</li><li><strong>PMF:</strong> <span class="math inline">\(p(k)=P(X=k)=(1-p)^{k}p, \quad k=1,2,3\)</span></li><li><strong>Expectation:</strong> <span class="math inline">\(\frac{1-p}{p}\)</span></li><li><strong>Variance:</strong> <span class="math inline">\(\frac{1-p}{p^2}\)</span></li></ul><p><strong>Negative Binomial Distributions</strong></p><ul><li><strong>Story:</strong><span class="math inline">\(X\)</span> is the number of &quot;failures&quot; that we will have before we achieve our <span class="math inline">\(r\)</span>th success. Our successes have probability <span class="math inline">\(p\)</span>.</li><li><strong>PMF:</strong> <span class="math inline">\(p(X=k)=\left(\begin{array}{c}r+k-1\\ r-1\end{array}\right) p^r(1-p)^{k}\)</span></li><li><strong>Expectation:</strong> <span class="math inline">\(\frac{r(1-p)}{p}\)</span></li><li><strong>Variance:</strong> <span class="math inline">\(\frac{r(1-p)}{p^2}\)</span></li></ul><p><strong>Poisson Distribution</strong></p><ul><li><strong>Story: </strong> There are rare events (low probability events) that occur many different ways (high possibilities of occurences) at an average rate of <strong><span class="math inline">\(\lambda\)</span> occurrences per unit space or time</strong>. The number of events that occur in that unit of space or time is <span class="math inline">\(X\)</span>.</li><li><strong>Example</strong>: A certain busy intersection has an average of 2 accidents per month. Since an accident is a low probability event that can happen many different ways, it is reasonable to model the <strong>number of accidents in a month at that intersection</strong> as <span class="math inline">\(\mathcal{Pois}(2)\)</span>. Then the number of accidents that happen in two months at that intersection is distributed <span class="math inline">\(\mathcal{Pois}(4)\)</span></li><li><strong>PMF:</strong> <span class="math inline">\(p(X=k)=\frac{e^{-\lambda}\lambda^k }{k!}, \quad k=0,1,2,3\)</span></li><li><strong>Expectation:</strong> <span class="math inline">\(\lambda\)</span></li><li><strong>Variance:</strong> <span class="math inline">\(\lambda\)</span></li><li><strong>Property:</strong> Let <span class="math inline">\(X \sim \mathcal{Pois}(\lambda_1)\)</span> and <span class="math inline">\(Y \sim \mathcal{Pois}(\lambda_2)\)</span>, with <span class="math inline">\(X \perp \!\!\! \perp Y\)</span>.<ul><li><strong>Sum</strong>: <span class="math inline">\(X + Y \sim \mathcal{Pois}(\lambda_1 + \lambda_2)\)</span></li><li><strong>Conditional</strong>: <span class="math inline">\(X | (X + Y = n) \sim \mathcal{Bin}\left(n, \frac{\lambda_1}{\lambda_1 + \lambda_2}\right)\)</span></li><li><strong>Chicken-egg</strong>: If there are <span class="math inline">\(Z \sim \mathcal{Pois}(\lambda)\)</span> items and we randomly and independently &quot;accept&quot; each item with probability <span class="math inline">\(p\)</span>, then the number of accepted items <span class="math inline">\(Z_1 \sim \mathcal{Pois}(\lambda p)\)</span>, and the number of rejected items <span class="math inline">\(Z_2 \sim \mathcal{Pois}(\lambda (1-p))\)</span>, and <span class="math inline">\(Z_1 \perp \!\!\! \perp Z_2\)</span></li></ul></li></ul><p><strong>Uniform Distribution</strong></p><ul><li><strong>Story: </strong>A <strong>uniform random variable</strong> on the interval <span class="math inline">\([a, b]\)</span> is a model for what we mean when we say ‚Äúchoose a number at random between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</li><li><strong>PMF:</strong> <span class="math inline">\(f(x)=\frac{1}{b-a}, x\in (a,b)\)</span></li><li><strong>Expectation:</strong> <span class="math inline">\(\frac{a+b}{2}\)</span></li><li><strong>Variance:</strong> <span class="math inline">\(\frac{(b-a)^2}{12}\)</span></li><li><strong>Property:</strong> For a Uniform distribution, the probability of a draw from any interval within the support is proportional to the length of the interval</li></ul><p><strong>Normal Distribution</strong> <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span></p><ul><li><strong>Central Limit Theorem</strong>: the sample mean of i.i.d.~r.v.s will approach a Normal distribution as the sample size grows, regardless of the initial distribution.</li><li><strong>Location-Scale Transformation</strong>: Every time we shift a Normal r.v.~(by adding a constant) or rescale a Normal (by multiplying by a constant), we change it to another Normal r.v. For any Normal <span class="math inline">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, we can transform it to the standard <span class="math inline">\(\mathcal{N}(0, 1)\)</span> by: <span class="math inline">\(Z= \frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1)\)</span></li><li><strong>Standard Normal</strong>: The Standard Normal, <span class="math inline">\(Z \sim \mathcal{N}(0, 1)\)</span>, has mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>. Its CDF is denoted by <span class="math inline">\(\Phi\)</span>.</li><li><strong>PMF:</strong> <span class="math inline">\(f(x)=\frac{1}{\sigma \sqrt{2 \pi} }e^{-\frac{(x-\mu)^2}{2\sigma^2}}, x\in (-\infty,\infty)\)</span></li><li><strong>Expectation:</strong> <span class="math inline">\(\mu\)</span></li><li><strong>Variance:</strong> <span class="math inline">\(\sigma^2\)</span></li></ul><p><strong>Exponential Distribution</strong></p><ul><li><p><strong>Story: </strong> the waiting times between rare events. <span class="math inline">\(\lambda\)</span> is the rate parameter, the next event arrives at a rate of 1 per <span class="math inline">\(1/\lambda\)</span> (hour/minute) on average. The expected time until the next event is <span class="math inline">\(1/\lambda\)</span></p></li><li><p><strong>PMF:</strong> <span class="math inline">\(f(x)=\lambda e^{-\lambda x}, x\in (0,\infty)\)</span></p></li><li><p><strong>Expectation:</strong> <span class="math inline">\(\frac{1}{\lambda}\)</span></p></li><li><p><strong>Variance:</strong> <span class="math inline">\(\frac{1}{\lambda^2}\)</span></p></li><li><p><strong>Property:</strong></p><ul><li><p><strong>Expos as a rescaled Expo(1)</strong>: <span class="math inline">\(Y \sim \mathcal{Expo}(\lambda) \rightarrow X = \lambda Y \sim \mathcal{Expo}(1)\)</span></p></li><li><p><strong>Memorylessness</strong>: The Exponential Distribution is the <em>only</em> <strong>continuous memoryless</strong> distribution. The memoryless property says that for <span class="math inline">\(X \sim \mathcal{Expo}(\lambda)\)</span> and any positive numbers <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span>,</p><p><span class="math display">\[P(X &gt; s + t | X &gt; s) = P(X &gt; t)\]</span></p><p>Equivalently, <span class="math display">\[X - a | (X &gt; a) \sim \mathcal{Expo}(\lambda)\]</span></p></li><li><p><strong>Min of Expos</strong>: If we have independent <span class="math inline">\(X_i \sim \mathcal{Expo}(\lambda_i)\)</span>, then <span class="math inline">\(\min(X_1, \dots, X_k) \sim \mathcal{Expo}(\lambda_1 + \lambda_2 + \dots + \lambda_k)\)</span></p></li><li><p><strong>Max of Expos</strong>: If we have i.i.d.~<span class="math inline">\(X_i \sim \mathcal{Expo}(\lambda)\)</span>, then <span class="math inline">\(\max(X_1, \dots, X_k)\)</span> has the same distribution as <span class="math inline">\(Y_1+Y_2+\dots+Y_k\)</span>, where <span class="math inline">\(Y_j \sim \mathcal{Expo}(j\lambda)\)</span> and the <span class="math inline">\(Y_j\)</span> are independent</p></li></ul></li></ul><p><strong>Chi-Square</strong> <span class="math inline">\(\chi^2_n\)</span></p><ul><li><strong>Story: </strong>A Chi-Square(<span class="math inline">\(n\)</span>) is the <strong>sum of the squares</strong> of <span class="math inline">\(n\)</span> independent <strong>standard Normal</strong> r.v.s. <span class="math display">\[\mathcal{X} \textrm{ is distributed as } Z_1^2 + Z_2^2 + \dots + Z_n^2 \textrm{ for i.i.d.~$Z_i \sim \mathcal{N}(0,1)$} \\\mathcal{X} \sim \text{Gamma}(n/2,1/2)\]</span></li></ul><p><strong>Student-t</strong> <span class="math inline">\(t_n\)</span></p><ul><li><strong>Story: </strong> Let <span class="math inline">\(X_1,...X_n\)</span> be i.i.id Normal r.v.s <span class="math inline">\(\sim \mathcal{N}(\mu, \sigma^2)\)</span></li></ul><p><span class="math display">\[\textrm{sample mean: } \bar{X}=\frac{1}{n}\sum_{i=1}^nX_i \\\textrm{sample variance: }S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2 \\\frac{\bar{X}-\mu}{S/\sqrt{n}} \sim t(n-1)\]</span></p><h1 id="lln-clt">LLN &amp; CLT</h1><p><strong>Law of Large Numbers (LLN)</strong></p><p>Let <span class="math inline">\(X_1, X_2, X_3 \dots\)</span> be i.i.d.~with mean <span class="math inline">\(\mu\)</span>. The <span class="math inline">\(\textbf{sample mean}\)</span> is<br><span class="math display">\[\bar{X}_n = \frac{X_1 + X_2 + X_3 + \dots + X_n}{n}\]</span></p><p>The <span class="math inline">\(\textbf{Law of Large Numbers}\)</span> states that as <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\bar{X}_n \to \mu\)</span> with probability <span class="math inline">\(1\)</span>.</p><p>For example, in flips of a coin with probability <span class="math inline">\(p\)</span> of Heads, let <span class="math inline">\(X_j\)</span> be the indicator of the <span class="math inline">\(j\)</span>th flip being Heads. Then LLN says the proportion of Heads converges to <span class="math inline">\(p\)</span> (with probability <span class="math inline">\(1\)</span>).</p><p><strong>Central Limit Theorem (CLT)</strong></p><p><strong>Explanation</strong>: the sample mean of i.i.d.~r.v.s will approach a Normal distribution as the sample size grows, regardless of the initial distribution.</p><p>We can use the <strong>Central Limit Theorem</strong> to approximate the distribution of a random variable <span class="math inline">\(Y=X_1+X_2+\dots+X_n\)</span> that is a sum of <span class="math inline">\(n\)</span> i.i.d. random variables <span class="math inline">\(X_i\)</span>. Let <span class="math inline">\(E(Y) = \mu_Y\)</span> and <span class="math inline">\(Var(Y) = \sigma^2_Y\)</span>. The CLT says <span class="math display">\[Y \dot{\,\sim\,} \mathcal{N}(\mu_Y, \sigma^2_Y)\]</span> If the <span class="math inline">\(X_i\)</span> are i.i.d.~with mean <span class="math inline">\(\mu_X\)</span> and variance <span class="math inline">\(\sigma^2_X\)</span>, then <span class="math inline">\(\mu_Y = n \mu_X\)</span> and <span class="math inline">\(\sigma^2_Y = n \sigma^2_X\)</span>. For the sample mean <span class="math inline">\(\bar{X}_n\)</span>, the CLT says <span class="math display">\[\bar{X}_n = \frac{1}{n}(X_1 + X_2 + \dots + X_n) \dot{\,\sim\,} \mathcal{N}(\mu_X, \sigma^2_X/n)\]</span></p><p><strong>Asymptotic Distributions using CLT</strong></p><p>We use <span class="math inline">\(\xrightarrow{D}\)</span> to denote <em>converges in distribution to</em> as <span class="math inline">\(n \to \infty\)</span>. The CLT says that if we standardize the sum <span class="math inline">\(X_1 + \dots + X_n\)</span> then the distribution of the sum converges to <span class="math inline">\(\mathcal{N}(0,1)\)</span> as <span class="math inline">\(n \to \infty\)</span>: <span class="math display">\[\frac{1}{\sigma\sqrt{n}} (X_1 + \dots + X_n - n\mu_X) \xrightarrow{D} \mathcal{N}(0, 1)\]</span> In other words, the CDF of the left-hand side goes to the standard Normal CDF, <span class="math inline">\(\Phi\)</span>. In terms of the sample mean, the CLT says <span class="math display">\[\frac{(\bar{X}_n - \mu_X)}{\sigma_X/\sqrt{n} } \xrightarrow{D} \mathcal{N}(0, 1)\]</span> <strong>Assumptions of CLT</strong></p><ul><li><p><strong>Randomization Condition:</strong> The data must be sampled randomly.</p></li><li><p><strong>Independence Assumption</strong>: The sample values must be independent of each other. This means that the occurrence of one event has no influence on the next event. Usually, if we know that people or items were selected randomly we can assume that the independence assumption is met.</p></li><li><p><strong>10% Condition:</strong> When the sample is drawn without replacement (usually the case), the sample size, <em>n</em>, should be no more than 10% of the population.</p></li><li><p><strong>Sample Size Assumption:</strong> The sample size must be sufficiently large. Although the Central Limit Theorem tells us that we can use a Normal model to think about the behavior of sample means when the sample size is large enough, it does not tell us how large that should be. If the population is very skewed, you will need a pretty large sample size to use the CLT, however if the population is unimodal and symmetric, even small samples are acceptable. So think about your sample size in terms of what you know about the population and decide whether the sample is large enough. In general a sample size of 30 is considered sufficient if the sample is unimodal (and meets the 10% condition).</p></li></ul><h1 id="the-mean-variance-and-standard-deviation">The Mean, Variance and Standard Deviation</h1><p><strong>Population:</strong> whatever unit it is you are measuring something.</p><p><strong>Population parameters:</strong> the parameters that determine how a distribution fits the population data</p><ul><li><strong>mean</strong> and <strong>standard deviation</strong> of the normal curve, which represents the population.</li></ul><p>We rarely, if ever, have population data, so we always <em>estimate</em> the <u>population parameters</u> using a relatively small sample.</p><blockquote><p>The reason why we want to know the <u>population parameters</u> is to ensure that the results drawn from our experiment are <strong><em>reproducible</em></strong>.</p></blockquote><p>The more data we have, the more <em>confidence</em> we can have in the accuracy of the estimates.</p><ul><li><strong>P-values &amp; confidence intervals</strong> : quantify the confidence in the esitmated parameters - &gt; tell us that while the estimates are different, they are not <em>significantly</em> different.</li></ul><p>By estimating the population parameters and quantifying our confidence in them, we can generate results that are reprducible in future experiments.</p><p><span class="math inline">\(\bar{x}\)</span> : <strong>estimated mean</strong> or <strong>sample mean</strong>.</p><p><span class="math inline">\(\mu\)</span>: <strong>population mean</strong></p><p>The estimated mean <span class="math inline">\(\bar{x}\)</span> is different from the population mean <span class="math inline">\(\mu\)</span>, but with more and more data, <span class="math inline">\(\bar{x}\)</span> should get closer and closer.</p><p><strong>Population Variance</strong> = <span class="math inline">\(\frac{\sum{(x-\mu)^2}}{n}\)</span> <span class="math inline">\(\leftarrow\)</span> this is the formula we use to <em>calculate</em>, not <em>estimate</em>, the population variance.</p><p><strong>Population Standard Deviation</strong>=<span class="math inline">\(\sqrt{\frac{\sum{(x-\mu)^2}}{n}}\)</span></p><p>Note: we almost never calcualte the population mean, population variance and standard deviation.</p><p><strong>Estimated Population Variance (sample variance)</strong> <span class="math inline">\(s^2= \frac{\sum{(x-\bar{x})^2}}{n-1}\)</span></p><ul><li>Dividing by <strong><em>n-1</em></strong> compensates for the fact that we are calcualting differences from the <strong>sample mean</strong> instead of the <strong>population mean</strong>, otherwise we would consistently underestimate the variance around the population mean.</li><li>This is because the <em>differences between the data and the sample mean</em> tend to be smaller than the <em>differences between the data and the population mean</em>. Thus the differences around the population mean result in a larger average.</li></ul><p>‚Äã <span class="math display">\[\frac{\sum{(x-\bar{x})^2}}{n-1} &lt; \frac{\sum{(x-\mu)^2}}{n}\]</span></p><p><strong>Chi-Squared relation with sample variance</strong>: <span class="math inline">\(\frac{s^2(n-1)}{\sigma^2} \sim \mathcal{X}^2_{n-1}\)</span></p><p><strong>Estimated Population Mean</strong> = <span class="math inline">\(\frac{\sum_i x_i}{n}\)</span></p><h2 id="sd-v.s.-se">SD v.s. SE</h2><blockquote><p><strong>The standard deviation of the means is called The Standard Error</strong></p></blockquote><p>The <strong>Standard Deviation</strong> quantifies the variation within a set of data points.</p><p>The <strong>Stardard Error</strong> quantifies the variation in the <em>means</em> from <strong><em>multiple sets</em></strong> of data.</p><ul><li>The confusing thing is that the SE can be estimated from a single set of data. In almost all cases, <strong>you should plot he SD</strong>, since graphs are usually intended to describe the data that you measured.</li></ul><h1 id="hypothesis-testing">Hypothesis Testing</h1><p><strong>Null Hypothesis</strong>: there is <strong><em>no difference</em></strong> between things <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(H_0\)</span></p><p><strong>Alternative Hypothesis</strong>: there is a <strong><em>difference</em></strong> between things<span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(H_1\)</span></p><p>Outcome: A decision about whether or not to <strong><em>reject</em></strong> or <strong><em>fail to reject</em></strong> the Null Hypothesis</p><table><thead><tr class="header"><th></th><th>Accept <span class="math inline">\(H_0\)</span></th><th>Reject <span class="math inline">\(H_0\)</span></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(H_0\)</span> is true</td><td>‚òë</td><td>type I error</td></tr><tr class="even"><td><span class="math inline">\(H_0\)</span> is false</td><td>type II error</td><td>‚òë</td></tr></tbody></table><p><strong>type I error:</strong> the probability of rejecting the null hypothesis while the null hypothesis is true, often noted <span class="math inline">\(\alpha\)</span> and also called &quot;false alarm&quot; or significance level . If we note <em>T</em> the <strong>test statistic</strong> and <em>R</em> the <strong>rejection region</strong>, then we have: <span class="math display">\[\alpha=P(T \in R| H_0 \text{ is true})\]</span> <strong>type II error:</strong> the probability of not rejecting the null hypothesis while the null hypothesis is not true, often noted <span class="math inline">\(\beta\)</span> and also called &quot;missed alarm&quot; or <strong>&quot;false positive</strong>&quot;. <span class="math display">\[\beta=P(T\notin R|H_0{\small\textrm{ not true})}\]</span> <img src="type-error.png"></p><p><strong>p-value</strong>: the probability under the <strong>null hypothesis</strong> of having a test statistic <em>T</em> at least as extreme as the one that we observed <span class="math inline">\(T_0\)</span>. We have: <span class="math display">\[{\small\textrm{(left-sided)}}\quad\boxed{p\textrm{-value}=P(T\leqslant T_0|H_0{\small\textrm{ true})}}\quad\quad\quad{\small\textrm{(right-sided)}} \quad \boxed{p\textrm{-value}=P(T\geqslant T_0|H_0{\small\textrm{ true})}}\]</span></p><p><span class="math display">\[{\small\textrm{(two-sided)}}\quad\boxed{p\textrm{-value}=P(|T|\geqslant |T_0||H_0{\small\textrm{ true})}}\]</span></p><p><em>Remark: the example below illustrates the case of a right-sided </em>p*-value.</p><p><img src="p-value.png"></p><p><strong>Testing for the difference in two means</strong>:</p><p>The table below sums up the <strong>test statistic</strong> to compute when performing a hypothesis test where the null hypothesis is: <span class="math display">\[H_0\quad:\quad\mu_X-\mu_Y=\delta\]</span></p><table><colgroup><col style="width: 17%"><col style="width: 17%"><col style="width: 24%"><col style="width: 40%"></colgroup><thead><tr class="header"><th>Distribution of <span class="math inline">\(X_i, Y_i\)</span></th><th><strong>Sample size <span class="math inline">\(n_X, n_Y\)</span></strong></th><th><strong>Variance <span class="math inline">\(\sigma_X^2, \sigma_Y^2\)</span></strong></th><th><strong>Test statistic under <span class="math inline">\(H_0\)</span></strong></th></tr></thead><tbody><tr class="odd"><td>Normal</td><td>Any</td><td>Known</td><td><span class="math inline">\(\frac{(\bar{X}-\bar{Y})-\delta}{\sqrt{\frac{\sigma_X^2}{n_X}+\frac{\sigma_Y^2}{n_Y}}} \sim \mathcal{N}(0,1)\)</span></td></tr><tr class="even"><td>Normal</td><td>Large</td><td>Unknown</td><td><span class="math inline">\(\frac{(\bar{X}-\bar{Y})-\delta}{\sqrt{\frac{S_X^2}{n_X}+\frac{S_Y^2}{n_Y}}} \sim \mathcal{N}(0,1)\)</span></td></tr><tr class="odd"><td>Normal</td><td>Small</td><td>Unknown with <span class="math inline">\(\sigma_X=\sigma_Y\)</span></td><td><span class="math inline">\(\frac{(\bar{X}-\bar{Y})-\delta}{s\sqrt{\frac{1}{n_X}+\frac{1}{n_Y}}} \sim \mathcal{t}_{n_X+n_Y-2}\)</span></td></tr></tbody></table><h2 id="the-wald-test">The Wald Test</h2><p>Let <span class="math inline">\(Œ∏\)</span> be a scalar parameter, let <span class="math inline">\(\hat{Œ∏}\)</span> be an estimate of <span class="math inline">\(Œ∏\)</span> and let <span class="math inline">\(Ùè∞∏\hat{se}\)</span> be the estimated standard error of <span class="math inline">\(Œ∏\)</span>.</p><p>‚Äã Note: <span class="math inline">\(\hat{se} \approx \frac{s}{n}\)</span>, where <span class="math inline">\(s\)</span> is sample standard deviation <span class="math inline">\(s=\sqrt{\frac{\sum{(x-\bar{x})^2}}{n-1}}\)</span></p><p><strong>Definition.</strong> The Wald Test</p><p>‚Äã <em>Consider testing</em> <span class="math display">\[H_0 :Œ∏=Œ∏_0 \text{ versus } H_1 :Œ∏ \neq Œ∏_0.\]</span> ‚Äã <em>Assume that Œ∏ is asymptotically Normal:</em> <span class="math display">\[\frac{(\hat{Œ∏}‚àíŒ∏_0)}{\hat{se}} \sim Ùè±ÇN(0,1)\]</span> ‚Äã <em>The size Œ± of <strong>Wald test</strong> is: reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(|W | &gt; z_{Œ±/2}\)</span> where</em> <span class="math display">\[W=\frac{(\hat{Œ∏}‚àíŒ∏_0)}{\hat{se}} \]</span></p><h2 id="p-value">P-Value</h2><p><strong>p-values</strong> are numbers <span class="math inline">\(\in [0,1]\)</span> that quantify how confident we should be that A is different from B.</p><blockquote><p>How <strong><em>small</em></strong> does a <strong>p-value</strong> have to be before we are sufficiently confident that A is different from B?</p></blockquote><ul><li>A commonly used threshold is <strong>0.05</strong>. It means<ul><li>If there is no difference between A and B, and if we did this exact sample experiment a bunch of times, then only <strong>5%</strong> of those experiments would result in the <strong><em>wrong</em></strong> decision; Or</li><li>If there is no difference between A and B, <strong>5%</strong> time we do the experiment, we will get a <strong>p-value</strong> less than <strong>0.05</strong>, aka a <strong>False Positive</strong>.</li></ul></li></ul><p><strong>False Positive</strong>: getting a small <strong>p-value</strong> where there is no difference</p><p><strong>Another e.g.:</strong></p><ul><li>Using a threshold of <strong>0.00001</strong> means we would only get a <strong>False Positive</strong> once every <strong>100,000</strong> experiments.</li><li>Using a threshold of <strong>0.2</strong> means we would only get a <strong>False Positive</strong> <strong>2</strong> times out of <strong>10</strong>.</li></ul><p><strong>Important</strong>:</p><ul><li><p>If we calculate a <strong>p-value</strong> &lt; 0.05 then we will decide that A is different from B <span class="math inline">\(\rightarrow\)</span> we should reject the <strong>Null Hypothesis</strong></p></li><li><p>While a small <strong>p-value</strong> helps us decide if A is different from B, it does <strong><em>not</em></strong> tell us <strong><em>how different</em></strong> they are or effective size.</p></li></ul><p><strong>Two types of p-value:</strong></p><ol type="1"><li><strong>One-sided</strong>: rarely used and potentially dangerous</li><li><strong>Two-sided</strong></li></ol><h3 id="calculating-p-value">Calculating P-Value</h3><p><strong>p-values</strong> are determined by adding up probabilities. 3 parts:</p><ol type="1"><li>The probability random chance would result in the observation</li><li>The probability of observing something else that <strong>is eqaully rare</strong></li><li>The probability of observing something <strong>rarer</strong> or more extreme</li></ol><p>Why adding the latter 2 parts? - A lot of equally rare or rarer things would make something less special</p><p>e.g. the p-value for getting 4 Heads and 1 Tails</p><blockquote><p>5/32+5/32+2/32=0.375</p></blockquote><h3 id="p-hacking">P-Hacking</h3><p><strong>P-Hacking</strong> refers to the misues and abuse of analysis techniques and results in being fooled by <strong>false positives</strong>.</p><ul><li><strong>Multiple Testing Problem</strong>: doing a lot of tests and ending up with <strong>False Positives</strong>.<ul><li><strong>False Discovery Rate</strong>: don't just collect all the data but only calculate a <strong>p-value</strong> for the one time things look different, instead, calcualte a <strong>p-value</strong> for each test and adjust all of the <strong>p-values</strong> with <strong>FDR</strong>.</li></ul></li><li>When a <strong>p-value</strong> is close to <strong>0.05</strong>, there is a high prob that just adding one new measurement to both groups will result in a <strong>false positive</strong>. -&gt; don't give each group more data points<ul><li><strong>Power Analysis:</strong> performed before doing an experiment and tells us how many replicates we need in order to have a relatively high probability of <strong><em>correctly</em></strong> rejecting the <strong>null hypothesis</strong>.</li></ul></li></ul><h2 id="statistical-power">Statistical Power</h2><p><strong>Power</strong> is the probability that we will <em>correctly</em> reject the <strong>Null Hypothesis</strong>, i.e. get a small <strong>p-value</strong>.</p><ul><li>When we have 2 distributions that have very little overlap, we have a lot of <strong>Power</strong> because there is a high prob that we will <em>correctly</em> reject the <strong><em>null hypothesis</em></strong>.</li><li>When the 2 distributions overlap a lot, and if we have a small sample size, we will have small <strong>Power</strong></li><li>If we want more <strong>Power</strong>, we can increase the sample size</li></ul><p><strong>Power Analysis</strong>: tell us how many measurements we need to collect to have a good amount of <strong>Power</strong>.</p><h2 id="power-analysis">Power Analysis</h2><p><strong>Power Analysis</strong>: determines what sample size will ensure a high probability that we <em>correctly</em> reject the <strong><em>null hypothesis</em></strong> that there is no difference between the 2 groups.</p><p><strong>Power Analysis</strong> is affected by 2 factors:</p><ol type="1"><li>How much overlap there is between the 2 distibutions we want to identify with our study</li></ol><p><img src="1.png"></p><ol start="2" type="1"><li>The <strong>Sample Size</strong></li></ol><p><img src="2.png"></p><blockquote><p>The more overlap between the 2 distributions, the larger then <strong>Sample Size</strong> needs to be in order to have a large <strong>Power</strong>.</p></blockquote><p><img src="3.png"></p><p>When we increase the <strong>Sample Size</strong> , we have more confidence that the <strong>estimated</strong> means are close to the <strong>Population Means</strong> because extreme observations have less effect on the location of the estimated means. And the closer the estimated means are to the <strong>Population Means</strong> , the less the measn from the different distributions will overlap and that increase the prob that we will <strong><em>correctly</em></strong> reject the <strong>Null Hypothesis</strong>.</p><p><strong>3 components to calculate Sample Size:</strong></p><ol type="1"><li><p>Decide how much <strong>Power</strong> we want.</p><ul><li>Common value for <strong>Power</strong>: <strong>0.8</strong> <span class="math inline">\(\rightarrow\)</span> we want an <strong>80%</strong> prob that we will correctly reject the <strong>Null Hypothesis</strong>.</li></ul></li><li><p>Determine the threshold for significance (called <strong>alpha</strong>, <span class="math inline">\(\alpha\)</span>)</p></li></ol><ul><li>Common value for <strong>Alpha</strong>: <strong>0.05</strong></li></ul><ol start="3" type="1"><li><p>Estimate the <strong>Overlap</strong> between the 2 distributions</p><ul><li><strong>Overlap</strong> is effected by both the <strong>distance</strong> between the population means, and the <strong>standard deviations</strong> <span class="math inline">\(\rightarrow\)</span> combine them together <span class="math inline">\(\rightarrow\)</span> <strong>Effective Size</strong></li><li><strong>Effective Size (d)</strong> = <span class="math inline">\(\frac{The\ estimated\ difference\ in\ the\ means}{Pooled\ esitmated\ standard\ deviations} = \sqrt{\frac{s_1^2+s_2^2}{2}}\)</span> where <span class="math inline">\(s_1\)</span> &amp; <span class="math inline">\(s_2\)</span> represent the estimated standard devation for the 2 distributions.</li></ul><p><img src="5.png"></p><p>Then <span class="math inline">\(\rightarrow\)</span> statistics power calculator <span class="math inline">\(\rightarrow\)</span> sample size = N</p><p>It means if I get N measurements per group, I will have an <strong>80%</strong> chance that I will <strong><em>correctly</em></strong> reject the <strong>Null Hypothesis</strong>.</p></li></ol><h1 id="covariance-and-correlation">Covariance and Correlation</h1><p><strong>Covariance:</strong> <span class="math inline">\(Cov(X,Y)=\sigma_{XY}=E[(X-\mu_X)(Y-\mu_Y)]=E(XY)-\mu_X\mu_Y\)</span></p><ul><li><p>If X and Y are discrete random variables with joint support S, then the covariance of X and Y is: <span class="math display">\[Cov(X,Y)=\sum\sum_{(x,y)\in S}(x-\mu_X)(y-\mu_Y)f(x,y)\]</span></p></li><li><p>If X and Y are continuous random variables with supports S1 and S2, respectively, then the covariance of X and Y is: <span class="math display">\[Cov(X,Y)=\int_{S_2}\int_{S_1}(x-\mu_X)(y-\mu_Y)f(x,y)dxdy\]</span></p></li></ul><p><strong>Correlation Coefficient</strong>: <span class="math inline">\(\rho_{XY}=Corr(X,Y)=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}=\frac{\sigma_{XY}}{\sigma_X \sigma_Y} \in [0,1]\)</span></p><p><strong>Independent</strong>: If X and Y are independent random variables (discrete or continuous!), then: <span class="math display">\[Corr(X,Y)=Cov(X,Y)=0\]</span> <strong>Why Coveriance is hard to interpret?</strong> Covariance is sensitive to the scale of the data while correlation is not affected by the scale of the data.</p><p><strong>Correlation - p-value relationship</strong>: For <strong>correlation</strong>, a <strong>p-value</strong> tells us the probability that randomly drawn dots will result in a similarly <em>strong</em> relationship or stronger. Thus, the smaller the <strong>p-value</strong>, the more confidence we have in the predictions we make with the line. We quantify the confidence of correlation with a <strong>p-value</strong>. The more data we have, the more confidence (smaller p-value) we have.</p><p><strong>Why Correlation is still hard to interpret?</strong> It's not obvious that Corr=0.7 is twice as good at making predictions as Corr=0.5, while <span class="math inline">\(R^2=0.7\)</span> is 1.4 times as good as <span class="math inline">\(R^2=0.5\)</span></p><h1 id="anova">ANOVA</h1><h2 id="basic-ideas">Basic Ideas</h2><p><strong>Why ANOVA?</strong>: the previous hypothesis tests are only about a maximum of 2 populations, ANOVA permits comparisons of multiple populations and even subgroups.</p><p><strong>Null Hypothesis:</strong> whether or not these 3 sample means come from the same population. <span class="math display">\[H_0: \mu_1=\mu_2=\mu_3\]</span> <img src="anova1.png"></p><p>In ANOVA, the ideas below are very important.</p><ul><li><strong>Variability AMONG/BETWEEN the sample means</strong>. each sample mean's distance from the mean of the overall population.</li><li><strong>Variability AROUND/WITHIN the sample means</strong>. the variance or SPREAD of each distribution.</li></ul><p><img src="anova5.png"></p><p><strong>Definition</strong>. <strong>ANOVA</strong> is a <em>variability ratio</em> <span class="math display">\[\frac{\text{Variability AMONG/BETWEEN the means}}{\text{Variability AROUND/WITHIN the distributions}}\]</span> <img src="anova3.png"></p><blockquote><p>If the variabitlity BETWEEN the means (distance from overall mean) in the numerator is relatively large compared to the variance WITHIN the samples (internal spread) in the denominator, the ratio will be much larger than 1. The samples then most likely do NOT come from a common population; REJECT Null Hypothesis that mean(s) are equal.</p></blockquote><p><img src="anova4.png"></p><h2 id="one-way-anova">One-Way ANOVA</h2><p><strong>Formulas For One-Way ANOVA</strong>: <span class="math display">\[\text{Sum of Squares Total(SST)}=\text{Sum of Squares Between(SSC)}+\text{Sum of Squares Within(SSE)}\]</span></p><p><span class="math display">\[\begin{align}N=\text{total observations} \quad C=\text{Number of columns/treatments/tests}  \\\end{align}\]</span></p><table><colgroup><col style="width: 26%"><col style="width: 10%"><col style="width: 28%"><col style="width: 34%"></colgroup><thead><tr class="header"><th></th><th>Degree of Freedom</th><th>Average</th><th>Formula</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\text{Sum of squares(columns)(SSC)}\)</span></td><td><span class="math inline">\(C-1\)</span></td><td><span class="math inline">\(\text{MSC}=\frac{SSC}{\text{df}_{columns}}\)</span></td><td><span class="math inline">\(\sum_{j=1}^kn_j(\bar{x}_j-\bar{\bar{x}})^2\)</span></td></tr><tr class="even"><td><span class="math inline">\(\text{Sum of squares(within/error)(SSE)}\)</span></td><td><span class="math inline">\(N-C\)</span></td><td><span class="math inline">\(\text{MSE}=\frac{SSE}{\text{df}_{error}}\)</span></td><td><span class="math inline">\(\sum_{j=1}^k(\sum_{i=1}^{n_1}(x_{ji}-\bar{x}_j)^2)\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\text{Sum of squares(total)(SST)}\)</span></td><td><span class="math inline">\(N-1\)</span></td><td><span class="math inline">\(\text{F}=\frac{MSC}{MSE}\)</span></td><td><span class="math inline">\(\sum_{j=1}^k\sum_{i=1}^{n_j}(x_{ji}-\bar{\bar{x}})^2\)</span></td></tr></tbody></table><p><strong>Example</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df=pd.DataFrame(data=np.array([[<span class="number">82</span>,<span class="number">93</span>,<span class="number">61</span>,<span class="number">74</span>,<span class="number">69</span>,<span class="number">70</span>,<span class="number">53</span>],[<span class="number">71</span>,<span class="number">62</span>,<span class="number">85</span>,<span class="number">94</span>,<span class="number">78</span>,<span class="number">66</span>,<span class="number">71</span>],[<span class="number">64</span>,<span class="number">73</span>,<span class="number">87</span>,<span class="number">91</span>,<span class="number">56</span>,<span class="number">78</span>,<span class="number">87</span>]]).T,</span><br><span class="line">                columns=[<span class="string">'year_1'</span>,<span class="string">'year_2'</span>,<span class="string">'year_3'</span>])</span><br><span class="line">df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }        .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>year_1</th><th>year_2</th><th>year_3</th></tr></thead><tbody><tr><th>0</th><td>82</td><td>71</td><td>64</td></tr><tr><th>1</th><td>93</td><td>62</td><td>73</td></tr><tr><th>2</th><td>61</td><td>85</td><td>87</td></tr><tr><th>3</th><td>74</td><td>94</td><td>91</td></tr><tr><th>4</th><td>69</td><td>78</td><td>56</td></tr><tr><th>5</th><td>70</td><td>66</td><td>78</td></tr><tr><th>6</th><td>53</td><td>71</td><td>87</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">N=len(df)*len(df.columns)</span><br><span class="line">C=len(df.columns)</span><br><span class="line">N,C</span><br></pre></td></tr></table></figure><p>(21, 3)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mean_1=df[<span class="string">'year_1'</span>].mean()</span><br><span class="line">mean_2=df[<span class="string">'year_2'</span>].mean()</span><br><span class="line">mean_3=df[<span class="string">'year_3'</span>].mean()</span><br><span class="line">mean_total=(df[<span class="string">'year_1'</span>].sum()+df[<span class="string">'year_2'</span>].sum()+df[<span class="string">'year_3'</span>].sum())/N</span><br><span class="line">mean_1,mean_2,mean_3,mean_total</span><br></pre></td></tr></table></figure><p>(71.71428571428571, 75.28571428571429, 76.57142857142857, 74.52380952380952)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SSC = ((mean_1-mean_total)**<span class="number">2</span> +</span><br><span class="line">      (mean_2-mean_total)**<span class="number">2</span> +</span><br><span class="line">      (mean_3-mean_total)**<span class="number">2</span>)*<span class="number">7</span></span><br><span class="line"></span><br><span class="line">SSE = (sum((df[<span class="string">'year_1'</span>]-mean_1)**<span class="number">2</span>) +</span><br><span class="line">       sum((df[<span class="string">'year_2'</span>]-mean_2)**<span class="number">2</span>) +</span><br><span class="line">       sum((df[<span class="string">'year_3'</span>]-mean_3)**<span class="number">2</span>))</span><br><span class="line">SST=SSC+SSE</span><br><span class="line">SSC,SSE,SST</span><br></pre></td></tr></table></figure><p>(88.66666666666693, 2812.571428571429, 2901.238095238096)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MSC=SSC/(C<span class="number">-1</span>)</span><br><span class="line">MSE=SSE/(N-C)</span><br><span class="line">F=MSC/MSE</span><br><span class="line">MSC,MSE,F</span><br></pre></td></tr></table></figure><p>(44.333333333333464, 156.25396825396828, 0.28372612759041116)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">F_crit=scipy.stats.f.ppf(q=<span class="number">1</span><span class="number">-0.05</span>, dfn=C<span class="number">-1</span>, dfd=N-C)</span><br><span class="line">print(<span class="string">"Since our F value(&#123;0:.2f&#125;) is smaller than F critical value(&#123;1:.2f&#125;), we fail to reject the null hypothesis. So there is no significant difference between the mean of each column"</span>.format(F,F_crit))</span><br></pre></td></tr></table></figure><p>Since our F value(0.28) is smaller than F critical value(3.55), we fail to reject the null hypothesis. So there is no significant difference between the mean of each column</p><h2 id="two-way-anova">Two-Way ANOVA</h2><p><strong>Two-Way ANOVA &quot;Block&quot; Design</strong>:</p><p>ANOVA is about partitioning the total / overall variance into <strong>different parts</strong>; assigning parts of the overall variance to different sources.</p><p>One of those parts is always ERROR; the unexplained source.</p><p>In a One-Way ANOVA, aside from ERROR, we were only working with one potential source of variance: COLUMNS/GROUPS.</p><p>A Two-Way ANOVA allows us to &quot;account for variation&quot; at the ROW level due to some other <strong>factor</strong> or <strong>grouping</strong> . We introduce a new way to separate the data: BLOCKS</p><ul><li>Blocks allow us to further refine how we &quot;assign&quot; or split apart the overall variance, allowing for more powerfil hypothesis tests.</li><li>By adding blocks or factors to the ROWS, we can &quot;subtract out&quot; that ROW variance from the overall ERROR variance.</li><li>This allows greater focus on COLUMN or GRROUP differences <em>making it easier to <strong>detect</strong> group differences.</em></li></ul><p><img src="anova6.png"></p><p><strong>Eating up original SSE with Blocks</strong>:</p><blockquote><p>SSC wants SSE to be as small as possible. &quot;Hey SSB, eat up original SSE!&quot;</p></blockquote><blockquote><p>In the end, SSC will be compared to SSE. So, the smaller SSE is, SSC can claim a larger part of SST.</p></blockquote><p><strong>Formulas For One-Way ANOVA</strong>: <span class="math display">\[\begin{align}\text{Sum of Squares Total(SST)}=&amp;\text{Sum of Squares Between(SSC)}+ \\                &amp;\text{Sum of Squares Block(SSB)}+ \\                &amp;\text{Sum of Squares Within(SSE)}\end{align}\]</span></p><p><span class="math display">\[\begin{align}N=\text{total observations} \quad C=\text{Number of  columns/treatments/tests} \quad B=\text{Number of  blocks} \\\end{align}\]</span></p><table><colgroup><col style="width: 26%"><col style="width: 10%"><col style="width: 28%"><col style="width: 34%"></colgroup><thead><tr class="header"><th></th><th>Degree of Freedom</th><th>Average</th><th>Formula</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\text{Sum of squares(columns)(SSC)}\)</span></td><td><span class="math inline">\(C-1\)</span></td><td><span class="math inline">\(\text{MSC}=\frac{SSC}{\text{df}_{columns}}\)</span></td><td><span class="math inline">\(\sum_{j=1}^kn_j(\bar{x}_j-\bar{\bar{x}})^2\)</span></td></tr><tr class="even"><td><span class="math inline">\(\text{Sum of squares(block)(SSB)}\)</span></td><td><span class="math inline">\(B-1\)</span></td><td><span class="math inline">\(\text{MSB}=\frac{SSB}{\text{df}_{blocks}}\)</span></td><td><span class="math inline">\(\sum_{b=1}^Bn_b(\bar{x}_b-\bar{\bar{x}})^2\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\text{Sum of squares(within/error)(SSE)}\)</span></td><td><span class="math inline">\((C-1)(B-1)\)</span></td><td><span class="math inline">\(\text{MSE}=\frac{SSE}{\text{df}_{error}}\)</span></td><td><span class="math inline">\(SST-SSB-SSC\)</span></td></tr><tr class="even"><td><span class="math inline">\(\text{Sum of squares(total)(SST)}\)</span></td><td><span class="math inline">\(N-1\)</span></td><td><span class="math inline">\(\text{F}=\frac{MSC}{MSE}\)</span></td><td><span class="math inline">\(\sum_{j=1}^k\sum_{i=1}^{n_j}(x_{ji}-\bar{\bar{x}})^2\)</span></td></tr></tbody></table><p><strong>Example</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df=pd.DataFrame(data=np.array([[<span class="number">75</span>,<span class="number">70</span>,<span class="number">50</span>,<span class="number">65</span>,<span class="number">80</span>,<span class="number">65</span>],[<span class="number">75</span>,<span class="number">70</span>,<span class="number">55</span>,<span class="number">60</span>,<span class="number">65</span>,<span class="number">65</span>],[<span class="number">90</span>,<span class="number">70</span>,<span class="number">75</span>,<span class="number">85</span>,<span class="number">80</span>,<span class="number">65</span>]]).T,</span><br><span class="line">                columns=[<span class="string">"city_&#123;&#125;"</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>)],index=[<span class="string">"shopper_&#123;&#125;"</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">7</span>)])</span><br><span class="line">df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }    .dataframe tbody tr th {        vertical-align: top;    }        .dataframe thead th {        text-align: right;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>city_1</th><th>city_2</th><th>city_3</th></tr></thead><tbody><tr><th>shopper_1</th><td>75</td><td>75</td><td>90</td></tr><tr><th>shopper_2</th><td>70</td><td>70</td><td>70</td></tr><tr><th>shopper_3</th><td>50</td><td>55</td><td>75</td></tr><tr><th>shopper_4</th><td>65</td><td>60</td><td>85</td></tr><tr><th>shopper_5</th><td>80</td><td>65</td><td>80</td></tr><tr><th>shopper_6</th><td>65</td><td>65</td><td>65</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">N=len(df)*len(df.columns)</span><br><span class="line">C=len(df.columns)</span><br><span class="line">B=len(df)</span><br><span class="line">N,C,B</span><br></pre></td></tr></table></figure><p>(18, 3, 6)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">block_means=df.mean(axis=<span class="number">1</span>)</span><br><span class="line">column_means=df.mean(axis=<span class="number">0</span>)</span><br><span class="line">total_mean=df.values.mean()</span><br><span class="line">print(<span class="string">"Means of block"</span>)</span><br><span class="line">print(block_means)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Means of columns"</span>)</span><br><span class="line">print(column_means)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Total mean:"</span>,total_mean)</span><br></pre></td></tr></table></figure><p>Means of block shopper_1 80.0 shopper_2 70.0 shopper_3 60.0 shopper_4 70.0 shopper_5 75.0 shopper_6 65.0 dtype: float64</p><p>Means of columns city_1 67.5 city_2 65.0 city_3 77.5 dtype: float64</p><p>Total mean: 70.0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SST=sum((df.values.reshape(<span class="number">-1</span>)-total_mean)**<span class="number">2</span>)</span><br><span class="line">SSC=sum((column_means-total_mean)**<span class="number">2</span>)*len(df)</span><br><span class="line">SSB=sum((block_means-total_mean)**<span class="number">2</span>)*len(df.columns)</span><br><span class="line">SSE=SST-SSB-SSC</span><br><span class="line">SST,SSC,SSB,SSE</span><br></pre></td></tr></table></figure><p>(1750.0, 525.0, 750.0, 475.0)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MST=SST/(N<span class="number">-1</span>)</span><br><span class="line">MSC=SSC/(C<span class="number">-1</span>)</span><br><span class="line">MSB=SSB/(B<span class="number">-1</span>)</span><br><span class="line">MSE=SSE/(C<span class="number">-1</span>)/(B<span class="number">-1</span>)</span><br><span class="line">MST,MSC,MSB,MSE</span><br></pre></td></tr></table></figure><p>(102.94117647058823, 262.5, 150.0, 47.5)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">F=MSC/MSE</span><br><span class="line">F_=MSB/MSE</span><br><span class="line">F,F_</span><br></pre></td></tr></table></figure><p>(5.526315789473684, 3.1578947368421053)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">F_crit=scipy.stats.f.ppf(q=<span class="number">1</span><span class="number">-0.05</span>, dfn=C<span class="number">-1</span>, dfd=(C<span class="number">-1</span>)*(B<span class="number">-1</span>))</span><br><span class="line">print(<span class="string">"Since our F value(&#123;0:.2f&#125;) is larger than F critical value(&#123;1:.2f&#125;), we can reject the null hypothesis. So significant difference do exist in cities."</span>.format(F,F_crit))</span><br></pre></td></tr></table></figure><p>Since our F value(5.53) is larger than F critical value(4.10), we can reject the null hypothesis. So significant difference do exist in cities.</p><p><strong>Ref</strong></p><p>https://cnx.org/contents/6Znhbn2_<span class="citation" data-cites="1.5:7mUmR30Q">@1.5:7mUmR30Q</span><span class="citation" data-cites="1/Central-Limit-Theorem-Assumptions-and-Conditions">@1/Central-Limit-Theorem-Assumptions-and-Conditions</span></p><p>https://github.com/wzchen/probability_cheatsheet/blob/master/probability_cheatsheet.pdf</p><p><a href="https://en.wikipedia.org/wiki/Student&#39;s_t-distribution" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Student%27s_t-distribution</a></p><p>https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics#hypothesis-testing</p><p>https://www.youtube.com/user/joshstarmer</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Basic Probability &amp;amp; Statistics knowledge.&lt;/p&gt;
    
    </summary>
    
      <category term="Math" scheme="https://nancyyanyu.github.io/categories/Math/"/>
    
    
      <category term="Probability" scheme="https://nancyyanyu.github.io/tags/Probability/"/>
    
      <category term="Statistics" scheme="https://nancyyanyu.github.io/tags/Statistics/"/>
    
      <category term="ANOVA" scheme="https://nancyyanyu.github.io/tags/ANOVA/"/>
    
      <category term="Hypothesis Testing" scheme="https://nancyyanyu.github.io/tags/Hypothesis-Testing/"/>
    
  </entry>
  
  <entry>
    <title>A/B Testing Final Project</title>
    <link href="https://nancyyanyu.github.io/posts/8fdfc10f/"/>
    <id>https://nancyyanyu.github.io/posts/8fdfc10f/</id>
    <published>2020-07-03T19:28:54.000Z</published>
    <updated>2020-07-03T21:47:33.264Z</updated>
    
    <content type="html"><![CDATA[<p>My implementation &amp; answer of Udacity A/B Testing course's final project.</p><a id="more"></a><h1 id="experiment-overview-free-trial-screener">Experiment Overview: Free Trial Screener</h1><p>At the time of this experiment, Udacity courses currently have two options on the course overview page: &quot;start free trial&quot;, and &quot;access course materials&quot;. If the student clicks &quot;start free trial&quot;, they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. If the student clicks &quot;access course materials&quot;, they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.</p><p>In the experiment, Udacity tested a change where if the student clicked &quot;start free trial&quot;, they were asked how much time they had available to devote to the course. If the student indicated 5 or more hours per week, they would be taken through the checkout process as usual. If they indicated fewer than 5 hours per week, a message would appear indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, the student would have the option to continue enrolling in the free trial, or access the course materials for free instead. This screenshot shows what the experiment looks like.</p><p>The <strong>hypothesis</strong> was that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time‚Äîwithout significantly reducing the number of students to continue past the free trial and eventually complete the course. If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.</p><p>The <strong>unit of diversion is a cookie</strong>, although if the student enrolls in the free trial, they are tracked by user-id from that point forward. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.</p><h2 id="metric-choice">Metric Choice</h2><p>Which of the following metrics would you choose to measure for this experiment and why? For each metric you choose, indicate whether you would use it as an invariant metric or an evaluation metric. The practical significance boundary for each metric, that is, the difference that would have to be observed before that was a meaningful change for the business, is given in parentheses. All practical significance boundaries are given as absolute changes.</p><p>Any place &quot;unique cookies&quot; are mentioned, the uniqueness is determined by day. (That is, the same cookie visiting on different days would be counted twice.) User-ids are automatically unique since the site does not allow the same user-id to enroll twice.</p><ul><li><strong><em>Number of cookies</em></strong>: That is, number of unique cookies to view the course overview page. (<span class="math inline">\(d_{min}=3000\)</span>)</li><li><strong><em>Number of user-ids</em></strong>: That is, number of users who enroll in the free trial. (<span class="math inline">\(d_{min}=50\)</span>)</li><li><strong><em>Number of clicks</em></strong>: That is, number of unique cookies to click the &quot;Start free trial&quot; button (which happens before the free trial screener is trigger). (<span class="math inline">\(d_{min}=240\)</span>)</li><li><strong><em>Click-through-probability</em></strong>: That is, number of unique cookies to click the &quot;Start free trial&quot; button divided by number of unique cookies to view the course overview page. (<span class="math inline">\(d_{min}=0.01\)</span>)</li><li><strong><em>Gross conversion</em></strong>: That is, number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the &quot;Start free trial&quot; button. (<span class="math inline">\(d_{min}=0.01\)</span>)</li><li><strong><em>Retention</em></strong>: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. (<span class="math inline">\(d_{min}=0.01\)</span>)</li><li><strong><em>Net conversion</em></strong>: That is, number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the &quot;Start free trial&quot; button. (<span class="math inline">\(d_{min}=0.0075\)</span>)</li></ul><p>You should also decide now what results you will be looking for in order to launch the experiment. Would a change in any one of your evaluation metrics be sufficient? Would you want to see multiple metrics all move or not move at the same time in order to launch? This decision will inform your choices while designing the experiment.</p><p><strong><em>Invariant Metrics</em></strong> : number of cookies, number of clicks, click-through-probability.</p><p><strong><em>Evaluation Metrics</em></strong> : gross conversion, retention, net conversion.</p><h2 id="measure-variability">Measure Variability</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">n_pageviews=<span class="number">40000</span></span><br><span class="line">n_clicks=<span class="number">3200</span></span><br><span class="line">n_enroll=<span class="number">660</span></span><br><span class="line">ctp=<span class="number">0.08</span></span><br><span class="line">n_sample=<span class="number">5000</span></span><br><span class="line"></span><br><span class="line">click_through_probability=<span class="number">0.08</span> <span class="comment">#clicks / pageviews</span></span><br><span class="line">gross_conversion=<span class="number">0.20625</span> <span class="comment"># enroll / click</span></span><br><span class="line">retention=<span class="number">0.53</span> <span class="comment"># payment / enroll</span></span><br><span class="line">net_conversion=<span class="number">0.1093125</span> <span class="comment"># payment / click</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""analytic standard deviation estimate"""</span></span><br><span class="line"><span class="comment"># gross_conversion</span></span><br><span class="line">std_gross_conversion=math.sqrt(gross_conversion*(<span class="number">1</span>-gross_conversion)/(n_clicks/n_pageviews*n_sample))</span><br><span class="line"><span class="comment"># retention</span></span><br><span class="line">std_retention=math.sqrt(retention*(<span class="number">1</span>-retention)/(n_enroll/n_pageviews*n_sample))</span><br><span class="line"><span class="comment"># net_conversion</span></span><br><span class="line">std_net_conversion=math.sqrt(net_conversion*(<span class="number">1</span>-net_conversion)/(n_clicks/n_pageviews*n_sample))</span><br><span class="line">print(<span class="string">"SD of GC: "</span>,round(std_gross_conversion,<span class="number">4</span>))</span><br><span class="line">print(<span class="string">"SD of Retention: "</span>,round(std_retention,<span class="number">4</span>))</span><br><span class="line">print(<span class="string">"SD of NC: "</span>,round(std_net_conversion,<span class="number">4</span>))</span><br></pre></td></tr></table></figure><pre><code>SD of GC:  0.0202SD of Retention:  0.0549SD of NC:  0.0156</code></pre><h2 id="sizing">Sizing</h2><ul><li><p>Will you use Bonferroni Correction? &gt; Evaluation metrics are closely related to each other, so that Bonferroni would be too conservative.</p></li><li><p>Which evaluation metrics did you choose? &gt; gross conversion, retention, net conversion</p></li><li><p>How many pageviews will you need?</p></li></ul><h3 id="using-sample-size-calculator">Using sample size calculator</h3><p>https://www.evanmiller.org/ab-testing/sample-size.html</p><p><strong>gross conversion</strong> - Baseline rate: 20.625% - Minimum Detectable Effect: 0.01 - Sample size: 25,835 clicks/group - Total sample size: 25,835*2=51670 clicks - Pageviews= 51670 / 0.08 (clicks / pageviews)=645875</p><p><strong>retention</strong> - Baseline rate: 53% - Minimum Detectable Effect: 0.01 - Sample size: 39,115 enrolls/group - Total sample size: 39,115*2=78230 enrolls - Pageviews= 78230 / (660/40000) (enrolls / pageviews)=4741212</p><p><strong>net conversion</strong> - Baseline rate: 10.93125% - Minimum Detectable Effect: 0.0075 - Sample size: 27,413 clicks/group - Total sample size: 27,413*2=54826 clicks - Pageviews= 54826 / 0.08 (clicks / pageviews)=685325</p><p>The maximum number of pageviews is 4741212</p><h3 id="using-standard-error">Using Standard Error</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"><span class="comment"># Inputs:</span></span><br><span class="line"><span class="comment">#   The desired alpha for a two-tailed test</span></span><br><span class="line"><span class="comment"># Returns: The z-critical value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_z_star</span><span class="params">(alpha)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> norm.ppf((<span class="number">1</span>-alpha/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inputs:</span></span><br><span class="line"><span class="comment">#   z-star: The z-critical value</span></span><br><span class="line"><span class="comment">#   s: The standard error of the metric at N=1</span></span><br><span class="line"><span class="comment">#   d_min: The practical significance level</span></span><br><span class="line"><span class="comment">#   N: The sample size of each group of the experiment</span></span><br><span class="line"><span class="comment"># Returns: The beta value of the two-tailed test</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_beta</span><span class="params">(z_star,s, d_min, N)</span>:</span></span><br><span class="line">    SE = s /  math.sqrt(N)</span><br><span class="line">    <span class="keyword">return</span> norm.cdf(z_star*SE,d_min,SE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inputs:</span></span><br><span class="line"><span class="comment">#   s: The standard error of the metric with N=1 in each group</span></span><br><span class="line"><span class="comment">#   d_min: The practical significance level</span></span><br><span class="line"><span class="comment">#   Ns: The sample sizes to try</span></span><br><span class="line"><span class="comment">#   alpha: The desired alpha level of the test</span></span><br><span class="line"><span class="comment">#   beta: The desired beta level of the test</span></span><br><span class="line"><span class="comment"># Returns: The smallest N out of the given Ns that will achieve the desired</span></span><br><span class="line"><span class="comment">#          beta. There should be at least N samples in each group of the experiment.</span></span><br><span class="line"><span class="comment">#          If none of the given Ns will work, returns -1. N is the number of</span></span><br><span class="line"><span class="comment">#          samples in each group.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">required_size</span><span class="params">(s, d_min, Ns=<span class="number">200000</span>, alpha=<span class="number">0.05</span>, beta=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    z=get_z_star(alpha)</span><br><span class="line">    <span class="keyword">for</span> N <span class="keyword">in</span> range(<span class="number">1</span>,Ns):        </span><br><span class="line">        <span class="keyword">if</span> get_beta(z, s, d_min, N) &lt;= beta:</span><br><span class="line">            <span class="keyword">return</span> N</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># s is the pooled standard error for N=1 in each group,</span></span><br><span class="line"><span class="string">"""number of clicks needed for gross_conversion"""</span></span><br><span class="line">n=<span class="number">1</span></span><br><span class="line">s=math.sqrt(gross_conversion*(<span class="number">1</span>-gross_conversion)/n)</span><br><span class="line">d_min=<span class="number">0.01</span></span><br><span class="line">req=required_size(s,d_min)</span><br><span class="line">print(<span class="string">"number of clicks needed for gross_conversion:"</span>,int(req))</span><br><span class="line">print(<span class="string">"number of pageviews needed for gross_conversion:"</span>,int(req/click_through_probability))</span><br></pre></td></tr></table></figure><pre><code>number of clicks needed for gross_conversion: 12850number of pageviews needed for gross_conversion: 160625</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""number of enrollment needed for retention"""</span></span><br><span class="line">n=<span class="number">1</span></span><br><span class="line">s=math.sqrt(retention*(<span class="number">1</span>-retention)/n)</span><br><span class="line">d_min=<span class="number">0.01</span></span><br><span class="line">req=required_size(s,d_min)</span><br><span class="line">print(<span class="string">"number of clicks needed for retention:"</span>,int(req))</span><br><span class="line">print(<span class="string">"number of pageviews needed for retentionn:"</span>,int(req/(n_enroll/n_pageviews)))</span><br></pre></td></tr></table></figure><pre><code>number of clicks needed for retention: 19552number of pageviews needed for retentionn: 1184969</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""number of clickes needed for net_conversion"""</span></span><br><span class="line">n=<span class="number">1</span></span><br><span class="line">s=math.sqrt(net_conversion*(<span class="number">1</span>-net_conversion)/n)</span><br><span class="line">d_min=<span class="number">0.0075</span></span><br><span class="line">req=required_size(s,d_min)</span><br><span class="line">print(<span class="string">"number of clicks needed for net_conversion:"</span>,int(req))</span><br><span class="line">print(<span class="string">"number of pageviews needed for net_conversion:"</span>,int(req/click_through_probability))</span><br></pre></td></tr></table></figure><pre><code>number of clicks needed for net_conversion: 13586number of pageviews needed for net_conversion: 169825</code></pre><h2 id="duration-and-exposure">Duration and Exposure</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fraction_of_traffic=<span class="number">1</span></span><br><span class="line">duration=<span class="number">685325</span>/(n_pageviews*fraction_of_traffic)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">duration</span><br></pre></td></tr></table></figure><pre><code>17.133125</code></pre><h2 id="sanity-check">Sanity Check</h2><p>For each invariant metric, compute a 95% confidence interval for the value you expect to observe.</p><p>For invariant metrics we expect equal diversion into the experiment and control group.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">dates=[<span class="string">'Sat, Oct 11'</span>, <span class="string">'Sun, Oct 12'</span>, <span class="string">'Mon, Oct 13'</span>, <span class="string">'Tue, Oct 14'</span>,</span><br><span class="line">       <span class="string">'Wed, Oct 15'</span>, <span class="string">'Thu, Oct 16'</span>, <span class="string">'Fri, Oct 17'</span>, <span class="string">'Sat, Oct 18'</span>,</span><br><span class="line">       <span class="string">'Sun, Oct 19'</span>, <span class="string">'Mon, Oct 20'</span>, <span class="string">'Tue, Oct 21'</span>, <span class="string">'Wed, Oct 22'</span>,</span><br><span class="line">       <span class="string">'Thu, Oct 23'</span>, <span class="string">'Fri, Oct 24'</span>, <span class="string">'Sat, Oct 25'</span>, <span class="string">'Sun, Oct 26'</span>,</span><br><span class="line">       <span class="string">'Mon, Oct 27'</span>, <span class="string">'Tue, Oct 28'</span>, <span class="string">'Wed, Oct 29'</span>, <span class="string">'Thu, Oct 30'</span>,</span><br><span class="line">       <span class="string">'Fri, Oct 31'</span>, <span class="string">'Sat, Nov 1'</span>, <span class="string">'Sun, Nov 2'</span>, <span class="string">'Mon, Nov 3'</span>,</span><br><span class="line">       <span class="string">'Tue, Nov 4'</span>, <span class="string">'Wed, Nov 5'</span>, <span class="string">'Thu, Nov 6'</span>, <span class="string">'Fri, Nov 7'</span>,</span><br><span class="line">       <span class="string">'Sat, Nov 8'</span>, <span class="string">'Sun, Nov 9'</span>, <span class="string">'Mon, Nov 10'</span>, <span class="string">'Tue, Nov 11'</span>,</span><br><span class="line">       <span class="string">'Wed, Nov 12'</span>, <span class="string">'Thu, Nov 13'</span>, <span class="string">'Fri, Nov 14'</span>, <span class="string">'Sat, Nov 15'</span>,</span><br><span class="line">       <span class="string">'Sun, Nov 16'</span>]</span><br><span class="line">pageviews_cont=[ <span class="number">7723</span>,  <span class="number">9102</span>, <span class="number">10511</span>,  <span class="number">9871</span>, <span class="number">10014</span>,  <span class="number">9670</span>,  <span class="number">9008</span>,  <span class="number">7434</span>,  <span class="number">8459</span>,</span><br><span class="line">       <span class="number">10667</span>, <span class="number">10660</span>,  <span class="number">9947</span>,  <span class="number">8324</span>,  <span class="number">9434</span>,  <span class="number">8687</span>,  <span class="number">8896</span>,  <span class="number">9535</span>,  <span class="number">9363</span>,</span><br><span class="line">        <span class="number">9327</span>,  <span class="number">9345</span>,  <span class="number">8890</span>,  <span class="number">8460</span>,  <span class="number">8836</span>,  <span class="number">9437</span>,  <span class="number">9420</span>,  <span class="number">9570</span>,  <span class="number">9921</span>,</span><br><span class="line">        <span class="number">9424</span>,  <span class="number">9010</span>,  <span class="number">9656</span>, <span class="number">10419</span>,  <span class="number">9880</span>, <span class="number">10134</span>,  <span class="number">9717</span>,  <span class="number">9192</span>,  <span class="number">8630</span>,</span><br><span class="line">        <span class="number">8970</span>]</span><br><span class="line">pageviews_exp=[ <span class="number">7716</span>,  <span class="number">9288</span>, <span class="number">10480</span>,  <span class="number">9867</span>,  <span class="number">9793</span>,  <span class="number">9500</span>,  <span class="number">9088</span>,  <span class="number">7664</span>,  <span class="number">8434</span>,</span><br><span class="line">       <span class="number">10496</span>, <span class="number">10551</span>,  <span class="number">9737</span>,  <span class="number">8176</span>,  <span class="number">9402</span>,  <span class="number">8669</span>,  <span class="number">8881</span>,  <span class="number">9655</span>,  <span class="number">9396</span>,</span><br><span class="line">        <span class="number">9262</span>,  <span class="number">9308</span>,  <span class="number">8715</span>,  <span class="number">8448</span>,  <span class="number">8836</span>,  <span class="number">9359</span>,  <span class="number">9427</span>,  <span class="number">9633</span>,  <span class="number">9842</span>,</span><br><span class="line">        <span class="number">9272</span>,  <span class="number">8969</span>,  <span class="number">9697</span>, <span class="number">10445</span>,  <span class="number">9931</span>, <span class="number">10042</span>,  <span class="number">9721</span>,  <span class="number">9304</span>,  <span class="number">8668</span>,</span><br><span class="line">        <span class="number">8988</span>]</span><br><span class="line">clicks_cont=[<span class="number">687</span>, <span class="number">779</span>, <span class="number">909</span>, <span class="number">836</span>, <span class="number">837</span>, <span class="number">823</span>, <span class="number">748</span>, <span class="number">632</span>, <span class="number">691</span>, <span class="number">861</span>, <span class="number">867</span>, <span class="number">838</span>, <span class="number">665</span>,</span><br><span class="line">       <span class="number">673</span>, <span class="number">691</span>, <span class="number">708</span>, <span class="number">759</span>, <span class="number">736</span>, <span class="number">739</span>, <span class="number">734</span>, <span class="number">706</span>, <span class="number">681</span>, <span class="number">693</span>, <span class="number">788</span>, <span class="number">781</span>, <span class="number">805</span>,</span><br><span class="line">       <span class="number">830</span>, <span class="number">781</span>, <span class="number">756</span>, <span class="number">825</span>, <span class="number">874</span>, <span class="number">830</span>, <span class="number">801</span>, <span class="number">814</span>, <span class="number">735</span>, <span class="number">743</span>, <span class="number">722</span>]</span><br><span class="line">clicks_exp=[<span class="number">686</span>, <span class="number">785</span>, <span class="number">884</span>, <span class="number">827</span>, <span class="number">832</span>, <span class="number">788</span>, <span class="number">780</span>, <span class="number">652</span>, <span class="number">697</span>, <span class="number">860</span>, <span class="number">864</span>, <span class="number">801</span>, <span class="number">642</span>,</span><br><span class="line">       <span class="number">697</span>, <span class="number">669</span>, <span class="number">693</span>, <span class="number">771</span>, <span class="number">736</span>, <span class="number">727</span>, <span class="number">728</span>, <span class="number">722</span>, <span class="number">695</span>, <span class="number">724</span>, <span class="number">789</span>, <span class="number">743</span>, <span class="number">808</span>,</span><br><span class="line">       <span class="number">831</span>, <span class="number">767</span>, <span class="number">760</span>, <span class="number">850</span>, <span class="number">851</span>, <span class="number">831</span>, <span class="number">802</span>, <span class="number">829</span>, <span class="number">770</span>, <span class="number">724</span>, <span class="number">710</span>]</span><br><span class="line">enrolls_cont=[<span class="number">134</span>, <span class="number">147</span>, <span class="number">167</span>, <span class="number">156</span>, <span class="number">163</span>, <span class="number">138</span>, <span class="number">146</span>, <span class="number">110</span>, <span class="number">131</span>, <span class="number">165</span>, <span class="number">196</span>, <span class="number">162</span>, <span class="number">127</span>,</span><br><span class="line">       <span class="number">220</span>, <span class="number">176</span>, <span class="number">161</span>, <span class="number">233</span>, <span class="number">154</span>, <span class="number">196</span>, <span class="number">167</span>, <span class="number">174</span>, <span class="number">156</span>, <span class="number">206</span>]</span><br><span class="line">enrolls_exp=[<span class="number">105</span>, <span class="number">116</span>, <span class="number">145</span>, <span class="number">138</span>, <span class="number">140</span>, <span class="number">129</span>, <span class="number">127</span>,  <span class="number">94</span>, <span class="number">120</span>, <span class="number">153</span>, <span class="number">143</span>, <span class="number">128</span>, <span class="number">122</span>,</span><br><span class="line">       <span class="number">194</span>, <span class="number">127</span>, <span class="number">153</span>, <span class="number">213</span>, <span class="number">162</span>, <span class="number">201</span>, <span class="number">207</span>, <span class="number">182</span>, <span class="number">142</span>, <span class="number">182</span>]</span><br><span class="line">payment_cont=[ <span class="number">70</span>,  <span class="number">70</span>,  <span class="number">95</span>, <span class="number">105</span>,  <span class="number">64</span>,  <span class="number">82</span>,  <span class="number">76</span>,  <span class="number">70</span>,  <span class="number">60</span>,  <span class="number">97</span>, <span class="number">105</span>,  <span class="number">92</span>,  <span class="number">56</span>,</span><br><span class="line">       <span class="number">122</span>, <span class="number">128</span>, <span class="number">104</span>, <span class="number">124</span>,  <span class="number">91</span>,  <span class="number">86</span>,  <span class="number">75</span>, <span class="number">101</span>,  <span class="number">93</span>,  <span class="number">67</span>]</span><br><span class="line">payment_exp=[ <span class="number">34</span>,  <span class="number">91</span>,  <span class="number">79</span>,  <span class="number">92</span>,  <span class="number">94</span>,  <span class="number">61</span>,  <span class="number">44</span>,  <span class="number">62</span>,  <span class="number">77</span>,  <span class="number">98</span>,  <span class="number">71</span>,  <span class="number">70</span>,  <span class="number">68</span>,</span><br><span class="line">        <span class="number">94</span>,  <span class="number">81</span>, <span class="number">101</span>, <span class="number">119</span>, <span class="number">120</span>,  <span class="number">96</span>,  <span class="number">67</span>, <span class="number">123</span>, <span class="number">100</span>, <span class="number">103</span>]</span><br><span class="line">ctp_cont=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(clicks_cont,pageviews_cont)]</span><br><span class="line">ctp_exp=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(clicks_exp,pageviews_exp)]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""pageviews"""</span></span><br><span class="line">sum_pv_cont=sum(pageviews_cont)</span><br><span class="line">sum_pv_exp=sum(pageviews_exp)</span><br><span class="line">SD_pageviews=math.sqrt(<span class="number">0.5</span>*<span class="number">0.5</span>/(sum_pv_cont+sum_pv_exp))</span><br><span class="line">m=<span class="number">1.96</span>*SD_pageviews</span><br><span class="line">ci_min,ci_max=<span class="number">0.5</span>-m,<span class="number">0.5</span>+m</span><br><span class="line">print(<span class="string">"Confidence Interval for pageviews: [&#123;&#125;,&#123;&#125;]"</span>.format(round(ci_min,<span class="number">4</span>),round(ci_max,<span class="number">4</span>)))</span><br><span class="line">print(<span class="string">"Observed: "</span>,round(sum_pv_cont/(sum_pv_exp+sum_pv_cont),<span class="number">4</span>))</span><br></pre></td></tr></table></figure><pre><code>Confidence Interval for pageviews: [0.4988,0.5012]Observed:  0.5006</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""clicks"""</span></span><br><span class="line">sum_click_cont=sum(clicks_cont)</span><br><span class="line">sum_click_exp=sum(clicks_exp)</span><br><span class="line">SD_clicks=math.sqrt(<span class="number">0.5</span>*<span class="number">0.5</span>/(sum_click_cont+sum_click_exp))</span><br><span class="line">m=<span class="number">1.96</span>*SD_clicks</span><br><span class="line">ci_min,ci_max=<span class="number">0.5</span>-m,<span class="number">0.5</span>+m</span><br><span class="line">print(<span class="string">"Confidence Interval for clicks: [&#123;&#125;,&#123;&#125;]"</span>.format(round(ci_min,<span class="number">4</span>),round(ci_max,<span class="number">4</span>)))</span><br><span class="line">print(<span class="string">"Observed: "</span>,round(sum_click_cont/(sum_click_exp+sum_click_cont),<span class="number">4</span>))</span><br></pre></td></tr></table></figure><pre><code>Confidence Interval for clicks: [0.4959,0.5041]Observed:  0.5005</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""click_through_probability"""</span></span><br><span class="line">ctp_cont=sum_click_cont/sum_pv_cont</span><br><span class="line">ctp_exp=sum_click_exp/sum_pv_exp</span><br><span class="line">d_hat=ctp_exp-ctp_cont</span><br><span class="line">ctp_pool=(sum_click_cont+sum_click_exp)/(sum_pv_cont+sum_pv_exp)</span><br><span class="line">SE_ctp=math.sqrt(ctp_pool*(<span class="number">1</span>-ctp_pool)*(<span class="number">1</span>/sum_pv_cont+<span class="number">1</span>/sum_pv_exp))</span><br><span class="line">m=<span class="number">1.96</span>*SE_ctp</span><br><span class="line">ci_min,ci_max=-m,m</span><br><span class="line">print(<span class="string">"Confidence Interval for ctp: [&#123;&#125;,&#123;&#125;]"</span>.format(round(ci_min,<span class="number">4</span>),round(ci_max,<span class="number">4</span>)))</span><br><span class="line">print(<span class="string">"Observed: "</span>,round(d_hat,<span class="number">4</span>))</span><br></pre></td></tr></table></figure><pre><code>Confidence Interval for ctp: [-0.0013,0.0013]Observed:  0.0001</code></pre><h2 id="effective-size-test">Effective Size Test</h2><p>For each evaluation metric, compute a 95% confidence interval around the difference between the experiment and control group</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""gross conversion"""</span></span><br><span class="line">n=len(enrolls_exp)</span><br><span class="line">d_min=<span class="number">0.01</span></span><br><span class="line">sum_clicks_cont=sum(clicks_cont[:n])</span><br><span class="line">sum_clicks_exp=sum(clicks_exp[:n])</span><br><span class="line">sum_enroll_cont=sum(enrolls_cont[:n])</span><br><span class="line">sum_enroll_exp=sum(enrolls_exp[:n])</span><br><span class="line">p_pool=(sum_enroll_exp+sum_enroll_cont)/(sum_clicks_exp+sum_clicks_cont)</span><br><span class="line">SE_pool=math.sqrt(p_pool*(<span class="number">1</span>-p_pool)*(<span class="number">1</span>/sum_clicks_cont+<span class="number">1</span>/sum_clicks_exp))</span><br><span class="line">m=SE_pool*<span class="number">1.96</span></span><br><span class="line">d_hat=sum_enroll_exp/sum_clicks_exp-sum_enroll_cont/sum_clicks_cont</span><br><span class="line">print(<span class="string">"Confidence Interval:[&#123;&#125;,&#123;&#125;]"</span>.format(d_hat-m,d_hat+m))</span><br><span class="line">print(<span class="string">"Observed:"</span>,d_hat)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Statistically significant:"</span>, d_hat+m&lt;<span class="number">0</span> <span class="keyword">or</span> d_hat-m&gt;<span class="number">0</span> ,<span class="string">",  CI doesn't include 0"</span>)</span><br><span class="line">print(<span class="string">"Practically significant:"</span>,<span class="literal">True</span>,<span class="string">",  CI doesn't include d_min or -d_min"</span>)</span><br></pre></td></tr></table></figure><pre><code>Confidence Interval:[-0.0291233583354044,-0.01198639082531873]Observed: -0.020554874580361565Statistically significant: True ,  CI doesn&#39;t include 0Practically significant: True ,  CI doesn&#39;t include d_min or -d_min</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""retention"""</span></span><br><span class="line">n=len(payment_exp)</span><br><span class="line">d_min=<span class="number">0.01</span></span><br><span class="line">sum_payment_cont=sum(payment_cont[:n])</span><br><span class="line">sum_payment_exp=sum(payment_exp[:n])</span><br><span class="line">sum_enroll_cont=sum(enrolls_cont[:n])</span><br><span class="line">sum_enroll_exp=sum(enrolls_exp[:n])</span><br><span class="line">p_pool=(sum_payment_cont+sum_payment_exp)/(sum_enroll_cont+sum_enroll_exp)</span><br><span class="line">SE_pool=math.sqrt(p_pool*(<span class="number">1</span>-p_pool)*(<span class="number">1</span>/sum_enroll_cont+<span class="number">1</span>/sum_enroll_exp))</span><br><span class="line">m=SE_pool*<span class="number">1.96</span></span><br><span class="line">d_hat=sum_payment_exp/sum_enroll_exp-sum_payment_cont/sum_enroll_cont</span><br><span class="line">print(<span class="string">"Confidence Interval:[&#123;&#125;,&#123;&#125;]"</span>.format(d_hat-m,d_hat+m))</span><br><span class="line">print(<span class="string">"Observed:"</span>,d_hat)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Statistically significant:"</span>, d_hat+m&lt;<span class="number">0</span> <span class="keyword">or</span> d_hat-m&gt;<span class="number">0</span> ,<span class="string">",  CI doesn't include 0"</span>)</span><br><span class="line">print(<span class="string">"Practically significant:"</span>,<span class="literal">False</span>,<span class="string">",  CI include d_min"</span>)</span><br></pre></td></tr></table></figure><pre><code>Confidence Interval:[0.008104435728019967,0.05408517368626556]Observed: 0.031094804707142765Statistically significant: True ,  CI doesn&#39;t include 0Practically significant: False ,  CI include d_min</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""net conversion"""</span></span><br><span class="line">n=len(enrolls_exp)</span><br><span class="line">d_min=<span class="number">0.0075</span></span><br><span class="line">sum_clicks_cont=sum(clicks_cont[:n])</span><br><span class="line">sum_clicks_exp=sum(clicks_exp[:n])</span><br><span class="line">sum_payment_cont=sum(payment_cont[:n])</span><br><span class="line">sum_payment_exp=sum(payment_exp[:n])</span><br><span class="line">p_pool=(sum_payment_exp+sum_payment_cont)/(sum_clicks_exp+sum_clicks_cont)</span><br><span class="line">SE_pool=math.sqrt(p_pool*(<span class="number">1</span>-p_pool)*(<span class="number">1</span>/sum_clicks_cont+<span class="number">1</span>/sum_clicks_exp))</span><br><span class="line">m=SE_pool*<span class="number">1.96</span></span><br><span class="line">d_hat=sum_payment_exp/sum_clicks_exp-sum_payment_cont/sum_clicks_cont</span><br><span class="line">print(<span class="string">"Confidence Interval:[&#123;&#125;,&#123;&#125;]"</span>.format(d_hat-m,d_hat+m))</span><br><span class="line">print(<span class="string">"Observed:"</span>,d_hat)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Statistically significant:"</span>, d_hat+m&lt;<span class="number">0</span> <span class="keyword">or</span> d_hat-m&gt;<span class="number">0</span> ,<span class="string">",  CI doesn't include 0"</span>)</span><br><span class="line">print(<span class="string">"Practically significant:"</span>,<span class="literal">False</span>,<span class="string">",  CI include d_min"</span>)</span><br></pre></td></tr></table></figure><pre><code>Confidence Interval:[-0.011604624359891718,0.001857179010803383]Observed: -0.0048737226745441675Statistically significant: False ,  CI doesn&#39;t include 0Practically significant: False ,  CI include d_min</code></pre><h2 id="sign-tests">Sign Tests</h2><p>Run a sign test on each evaluation metric using the day-by-day data</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> binom_test </span><br><span class="line"><span class="string">"""gross conversion"""</span></span><br><span class="line">alpha=<span class="number">0.05</span></span><br><span class="line">beta=<span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">gc_exp=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(enrolls_exp,clicks_exp)]</span><br><span class="line">gc_cont=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(enrolls_cont,clicks_cont)]</span><br><span class="line">gc_diff=sum([i&gt;j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(gc_exp,gc_cont)])</span><br><span class="line">days=len(gc_exp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The prob of gross conversion of experiment group &gt; gross conversion of control group is 0.5</span></span><br><span class="line">p_value=binom_test(gc_diff, n=days, p=<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">"p-value:"</span>,p_value,<span class="string">", Statistically Significant:"</span>,p_value&lt;alpha)</span><br></pre></td></tr></table></figure><pre><code>p-value: 0.0025994777679443364 , Statistically Significant: True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""retention"""</span></span><br><span class="line">rt_exp=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(payment_exp,enrolls_exp)]</span><br><span class="line">rt_cont=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(payment_cont,enrolls_cont)]</span><br><span class="line">rt_diff=sum([i&gt;j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(rt_exp,rt_cont)])</span><br><span class="line">days=len(rt_exp)</span><br><span class="line">p_value=binom_test(rt_diff, n=days, p=<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">"p-value:"</span>,p_value,<span class="string">", Statistically Significant:"</span>,p_value&lt;alpha)</span><br></pre></td></tr></table></figure><pre><code>p-value: 0.6776394844055175 , Statistically Significant: False</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(gc_exp)</span><br></pre></td></tr></table></figure><pre><code>23</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""net conversion"""</span></span><br><span class="line">nc_exp=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(payment_exp,clicks_exp)]</span><br><span class="line">nc_cont=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(payment_cont,clicks_cont)]</span><br><span class="line">nc_diff=sum([i&gt;j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(nc_exp,nc_cont)])</span><br><span class="line">days=len(nc_exp)</span><br><span class="line">p_value=binom_test(nc_diff, n=days, p=<span class="number">0.5</span>)</span><br><span class="line">print(<span class="string">"p-value:"</span>,p_value,<span class="string">", Statistically Significant:"</span>,p_value&lt;alpha)</span><br></pre></td></tr></table></figure><pre><code>p-value: 0.6776394844055175 , Statistically Significant: False</code></pre><blockquote><p>Recomendation: not to launch</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;My implementation &amp;amp; answer of Udacity A/B Testing course&#39;s final project.&lt;/p&gt;
    
    </summary>
    
      <category term="Product Sense" scheme="https://nancyyanyu.github.io/categories/Product-Sense/"/>
    
    
      <category term="ab-testing" scheme="https://nancyyanyu.github.io/tags/ab-testing/"/>
    
  </entry>
  
  <entry>
    <title>Study Notes of Udacity A/B Testing</title>
    <link href="https://nancyyanyu.github.io/posts/17c5bb19/"/>
    <id>https://nancyyanyu.github.io/posts/17c5bb19/</id>
    <published>2020-07-03T19:24:21.000Z</published>
    <updated>2020-07-03T21:54:59.829Z</updated>
    
    <content type="html"><![CDATA[<p>My study note of Udacity <a href="https://classroom.udacity.com/courses/ud257" target="_blank" rel="noopener">A/B Testing course</a>.</p><a id="more"></a><h1 id="overview">Overview</h1><p>A/B testing is a methodology for testing product changes. You split your users to two groups - the control group which sees the default feature, and an experimental group that sees the new features.</p><h2 id="what-ab-testing-isnt-good-for">What A/B testing <strong><em>isn‚Äôt</em></strong> good for</h2><p>A/B testing is not good for <strong>testing new experiences</strong>. It may result in <em>change aversion</em> (where users don‚Äôt like changes to the norm), or a <em>novelty effect</em> (where users see something new and test out everything).</p><p>The two things with new experiences is</p><ul><li>having a baseline and</li><li>how much time needs to be allowed for the users to adapt to the new experience, so you can say what is going to be the plateaud experience and make a robust decision.</li></ul><p>Finally, A/B testing cannot tell you <strong>if you are missing something</strong>.</p><p>In these cases, user logs can be used to develop hypothesis that can then be used in an A/B test. A/B testing gives broad quantitiative data, while other techniques such as user research, focus groups, human evaluation give you deep qualitative data</p><h2 id="metric-choice">Metric Choice</h2><p><strong>Click-Through Rate</strong>: <span class="math inline">\(\frac{Number \ of \ clicks }{Number \ of \ page views}\)</span></p><ul><li>Use a rate when you want to measure the <strong>usability</strong> (how often do they actually find that button.)</li></ul><p><strong>Click-Through Probability</strong> <span class="math inline">\(\frac{Unique \ visitors \ who \ click }{Unique \ visitors \ to \ page}\)</span></p><ul><li>Use a probability when you want to measure the <strong>total impact</strong> (how often users went to the second level page on your site)</li></ul><blockquote><p>We're interested in whether users are progressing to the second level of the funnel, which is why we picked a probability.</p></blockquote><h3 id="how-to-compute-rate-probability">How to compute rate &amp; probability ?</h3><p>Rate: on every page view you capture the event, and then whenever a user clicks you also capture that click event</p><ul><li>Sum the page views, you sum the clicks and you divide.</li></ul><p>Probability: match each page view with all of the child clicks, so that you count, at most, one child click per page view.</p><h2 id="review-distribution">Review Distribution</h2><h3 id="binomial-distribution">Binomial Distribution</h3><p>For a binomial distribution with probability <span class="math inline">\(p\)</span> , the mean is given by <span class="math inline">\(p\)</span> and the standard deviation is <span class="math inline">\(\sqrt{p \times (1‚àíp)/N}\)</span> where <span class="math inline">\(N\)</span> is the number of trials.</p><p>A binomial distribution can be used when</p><ol type="1"><li>The outcomes are of 2 types</li><li>Each event is independent of the other</li><li>Each event has an identical distribution (i.e. <span class="math inline">\(p\)</span> is the same for all)</li></ol><blockquote><p>We expect click-through probability to follow a binomial distribution</p></blockquote><h3 id="confidence-interval">Confidence Interval</h3><p>Click or non-click</p><p><span class="math inline">\(x\)</span>: # of users who clicked</p><p><span class="math inline">\(N\)</span>: # of users</p><p><span class="math inline">\(\hat{p}=\frac{x}{N}\)</span> : estimate of probability</p><ul><li><span class="math inline">\(\hat{p}=\frac{100}{1000}=0.1\)</span></li></ul><p>To use normal: check <span class="math inline">\(N \cdot \hat{p}&gt;5\)</span> and <span class="math inline">\(N \cdot (1-\hat{p})&gt;5\)</span></p><p><span class="math inline">\(z\)</span>- distribution: standard normal distribution with <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span></p><ul><li>With 95% confidence, the true value would be within $1.96 $ and <span class="math inline">\(-1.96\)</span></li></ul><p><span class="math inline">\(m=z \cdot SE=z \cdot \sqrt{\frac{\hat{p}(1-\hat{p})}{N}}\)</span>: margin of error</p><ul><li><span class="math inline">\(m=0.019\)</span>,</li><li>Confidence interval: <span class="math inline">\([0.081,0.119]\)</span></li></ul><p>This means, if you'd run the experiment again with another 1000 page views, you'd maybe expect to see between 80 and 120 clicks, but more or less than that would be pretty surprising.</p><h4 id="standard-deviation-of-binomial">Standard deviation of binomial</h4><p>If you look up a binomial distribution elsewhere, you may find that it has a mean of <span class="math inline">\(np\)</span> and a standard deviation of <span class="math inline">\(\sqrt{np(1-p)}\)</span>. This is for a binomial distribution defined as <strong>the total number of successes</strong>, whereas we will use <strong>the fraction or proportion of successes</strong> throughout this class. In this cas, the mean is <span class="math inline">\(p\)</span> and standard deviation is <span class="math inline">\(\sqrt{\frac{p(1-p)}{n}}\)</span>.</p><h4 id="useful-equations">Useful equations</h4><p>You may find these equations helpful in solving the quiz: <img src="https://lh3.googleusercontent.com/7G4sR5EnVaKfqmc98cJorT0F9TtKIYEql7WDIYuemeWSGcGf6fW_MqrxR8fAs39n6gdmZ2ubbg28ttH_O9c=s0#w=134&amp;h=128" alt="p_hat = X/N SE = sqrt(p_hat (1-p_hat) / N) m = z* SE"></p><h2 id="hypothesis-testing">Hypothesis Testing</h2><p><strong>null hypothesis:</strong> there's no difference in click-through probability between our control, and our experiment.</p><p><strong>alternative hypothesis:</strong> whether the click-through rate is different? Or it's higher, or lower? Or are we interested in any kind of difference at all?</p><h3 id="two-tailed-vs.-one-tailed-tests">Two-tailed vs. one-tailed tests</h3><p>Two-tailed: The null hypothesis and alternative hypothesis allows you to distinguish between three cases:</p><ol type="1"><li>A statistically significant positive result</li><li>A statistically significant negative result</li><li>No statistically significant difference.</li></ol><p>one-tailed: allows you to distinguish between two cases:</p><ol type="1"><li>A statistically significant positive result</li><li>No statistically significant result</li></ol><p>Which one you should use depends on what action you will take based on the results. If you're going to launch the experiment for a statistically significant positive change, and otherwise not, then you don't need to distinguish between a negative result and no result, so a one-tailed test is good enough. If you want to learn the direction of the difference, then a two-tailed test is necessary.</p><h2 id="comparing-two-samples---pooled-standard-error">Comparing two samples - Pooled Standard Error</h2><p>For comparing two samples, we calculate the <strong>pooled standard error</strong>. For e.g., suppose <span class="math inline">\(Xcont\)</span> and <span class="math inline">\(Ncont\)</span> are the control number of users that click, and the total number of users in the control group. Let <span class="math inline">\(X_{exp}\)</span> and <span class="math inline">\(N_{exp}\)</span> be the values for the experiment.</p><p>The pooled probability is given by</p><p><span class="math inline">\(\hat{p}_{pool}=\frac{X_{cont}+X_{exp}}{N_{cont}+N_{exp}}\)</span></p><p><span class="math display">\[SE_{pool}=\sqrt{\hat{p}_{pool}‚àó(1‚àí\hat{p}_{pool})‚àó(\frac{1}{N_{cont}}+\frac{1}{N_{exp}})}\]</span></p><p><span class="math display">\[\hat{d}=\hat{p}_{exp}‚àí\hat{p}_{cont}\]</span></p><p><span class="math display">\[H_0:d=0 \ where \ \hat{d} ‚àºN(0,SE_{pool})\]</span></p><p><span class="math inline">\(d\)</span> is practical significance boundary. If <span class="math inline">\(\hat{d} &gt;1.96‚àóSE_{pool}\)</span> or $ &lt;‚àí1.96‚àóSE_{pool}$ then we can reject the null hypothesis and state that our difference represents a statistically significant difference</p><h3 id="practical-or-substantive-significance">Practical or Substantive Significance</h3><p>Practical significance is the level of change that you would expect to see from a business standpoint for the change to be valuable.</p><p>The differences in the magnitudes for what's consider practically significant can be quite different. What you really want to observe is repeatability.</p><p>Statistical significance is about <strong>repeatability</strong>. And you want to make sure when you setup your experiment that you get that guarantee that yes, these results are repeatable so it's statistically significant.</p><p>The statistical significance bar is often lower than the practical significance bar, so that if the outcome is practically significance, it is also statistically significant.</p><h2 id="size-vs-power-trade-off">Size vs Power trade-off</h2><p><strong>Statistical power:</strong> given that we have control over how many page views go into our control and our experiment, we have to decide how many page views we need in order to get a statistically significant result. -&gt; determine the number of data points needed to get a statistically significant result.</p><p><strong>Power</strong> has an inverse trade-off with <strong>size</strong>. The smaller the change you want to detect or the increased confidence you want to have in the result, means you have to run a larger experiment.</p><p>As you increase the number of samples, the confidence interval moves closer to the mean.</p><p><span class="math display">\[Œ±=P(reject\ null | null\ true)\]</span> - Falsely concluding there is a difference.</p><p><span class="math display">\[Œ≤=P(fail\ to\ reject\ null | null\ false)\]</span> - Falsely concluding there is no difference</p><p><span class="math inline">\(1‚àíŒ≤\)</span> is referred to as the <strong>sensitivity</strong> of the experiment, or <strong>statistical power</strong>. In general, you want your experiment to have a high level of sensitivity at the practical significance boundary. People often choose high sensitivity, typically around <em>80%</em>.</p><p>For a small sample,</p><ul><li><span class="math inline">\(Œ±\)</span> is low (you are unlikely to launch a bad experiment)</li><li><span class="math inline">\(Œ≤\)</span> is high (you are likely to fail to launch an experiment that actually did have a difference you care about).</li></ul><p>For a large sample,</p><ul><li><span class="math inline">\(Œ±\)</span> remains the same</li><li><span class="math inline">\(Œ≤\)</span> is lower (i.e. sensitivity increases).</li></ul><p>As you change one of the parameters, your sample size will change as well.</p><ul><li>If you increase the baseline click through probability (under 0.5) then this increases the standard error, and therefore, you need a higher number of samples</li><li>If you increase the practical significance level (<span class="math inline">\(d_{min}\)</span>), you require a fewer number of samples since larger changes are easier to detect</li><li>If you increase the confidence level (<span class="math inline">\(1-\alpha\)</span>), you want to be more certain that you are rejecting the null. At the same sensivitiy, this would require increasing the number of samples</li><li>If you want to increase the sensitivity (<span class="math inline">\(1-\beta\)</span>), you need to collect more samples</li></ul><h3 id="analyze-results">Analyze Results</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">N_cont = <span class="number">10072</span>  <span class="comment"># Control samples (pageviews)</span></span><br><span class="line">N_exp = <span class="number">9886</span>  <span class="comment"># Test samples (pageviews)</span></span><br><span class="line">X_cont = <span class="number">974</span>  <span class="comment"># Control clicks</span></span><br><span class="line">X_exp = <span class="number">1242</span>  <span class="comment"># Exp. clicks</span></span><br><span class="line"></span><br><span class="line">p_pool = (X_cont + X_exp)/(N_cont+N_exp)</span><br><span class="line">se_pool = sqrt(p_pool*(<span class="number">1</span>-p_pool)*(<span class="number">1</span>/N_cont + <span class="number">1</span>/N_exp))</span><br><span class="line"></span><br><span class="line">p_cont = X_cont/N_cont</span><br><span class="line">p_exp = X_exp/N_exp</span><br><span class="line">d_hat = p_exp - p_cont</span><br><span class="line"><span class="comment"># d_hat = 0.02892847</span></span><br><span class="line"></span><br><span class="line">m = <span class="number">1.96</span>*se_pool</span><br><span class="line">cf_min = d_hat-m</span><br><span class="line">cf_max = d_hat+m</span><br><span class="line">d_min = <span class="number">0.02</span> <span class="comment"># Minimum practical significance value for difference</span></span><br><span class="line"><span class="comment"># cf_min = 0.0202105</span></span><br><span class="line"><span class="comment"># cf_max = 0.03764645</span></span><br></pre></td></tr></table></figure><p>Since the minimum confidence limit is greater than 0 and the practical significance level of 0.02, we conclude that it is highly probable that click through probability is higher than 0.02 and is significant. Based on this, one would launch the new version.</p><h1 id="choosing-and-charaterizing-metrics">Choosing and Charaterizing Metrics</h1><p>:)</p><h2 id="metric-definition">Metric Definition</h2><p><strong>Two use cases of metrics:</strong></p><ul><li><strong><em>Invariant checking</em></strong> (sanity checking): Metrics that shouldn‚Äôt change between your test and control<ul><li>Do you have the same number of users across the two?</li><li>Is the distribution the same?</li></ul></li><li><strong><em>Evaluation</em></strong>:<ul><li>High level business metrics: how much revenue you make, what your market share is, how many users you have</li><li>Detailed metrics: user experience with the product</li></ul></li></ul><p><strong>How to make a definition</strong>:</p><ol type="1"><li>A high level concept for a metric/ one sentence summary: &quot;active users&quot;, &quot;click-through probability&quot;</li><li>Nitty gritty details: How do you define what active is? Which events count towards activity?</li><li>Summarize individual data measurements into a single metric: a sum or a count, an average</li></ol><p>For evaluation, you can choose either one metric or a whole suite of metrics.</p><p><strong>Overall Evaluation Criterion (OEC):</strong> a term that Microsoft uses for when they come up with a weighted function that combines all of these different metrics.</p><ul><li>Hard to define and get everyone to agree</li><li>Over-optimize</li><li>Hard to explain why is it moving</li></ul><p><strong>How generally applicable the metric is</strong>:</p><p>If you're running a whole suite of AB tests, then ideally you'd have one or more metrics that you can use across the entire suite. If you are running a suite of A/B tests, it is preferable to have a metric that works across the entire suite. It's much better to use a metric that's <strong>less optimal</strong> than it is to come up with the perfect metric for your test.</p><h3 id="refining-the-customer-funnel">Refining the Customer Funnel</h3><p>User funnel indicates a series of steps taken by users through the site. It is called a funnel because every subsequent stage has fewer users than the stage above.</p><p>Each stage is a metric:</p><ul><li><p>Count: No. of users who reach that point (keep in certain stages)</p></li><li>Rate: better for usability test. You want to increase rate of progression</li><li><p>Probability: progression. A unique user progressed down the funnel</p></li></ul><h2 id="defining-metrics-other-techniques">Defining Metrics: Other Techniques</h2><ol type="1"><li><strong>External Data:</strong> great for brainstorming, think of new metrics idea; good for validating metrics; benchmark your own metrics against the industry; help you develop validation techniques<ul><li>Companies that collect data (e.g. Comscore, Nielsen)</li><li>Companies that conduct surveys (e.g. Pew)</li><li>Academic papers</li></ul></li><li>Internal Data:<ul><li><strong>Retrospective analysis</strong>:<ul><li>Look at historic data to look at changes and see the evaluation</li><li>Good to get a baseline and help you develop theories</li></ul></li><li><strong>Surveys and User experience research</strong>:<ul><li>help you develop ideas on what you want to research</li></ul></li></ul></li></ol><p>The problem of these studies: show you correlation, not causation</p><h3 id="gathering-additional-data"><strong>Gathering Additional Data</strong></h3><ol type="1"><li><p><strong>User Experience Research (UER)</strong>:</p><ul><li>high depth on a few users.</li><li>good for brainstorming.</li><li><p>can use special equipment in a UER (e.g. eye movement camera)</p><p>X validate the results (retrospective analysis)</p></li></ul></li><li><p><strong>Focus groups</strong>:</p><ul><li>Medium depth and medium # of participants.</li><li><p>Get feedback on hypotheticals</p><p>X may run into the issue of groupthink</p></li></ul></li><li><p><strong>Surveys</strong>:</p><ul><li>low depth but high # of participants</li><li><p>Useful for metrics you cannot directly measure.</p><p>X Can‚Äôt directly compare with other metrics since population for survey and internal metrics may be different.</p></li></ul></li></ol><h3 id="applying-other-techniques-on-difficult-metrics">Applying Other Techniques on Difficult Metrics</h3><p><strong>Difficult Metrics:</strong></p><ol type="1"><li>Don't have access to data</li><li>Takes too long</li></ol><p><strong>Rate of returning for 2nd course</strong></p><ul><li>Takes too long</li><li>Survey -&gt; proxy</li></ul><p><strong>Average happiness of shoppers</strong></p><ul><li>Don't have access to data</li><li>Survey; UER - brainstorm</li></ul><p><strong>Probability of finding info via search</strong></p><ul><li>Don't have access to data</li><li>External data; UER; Human raters</li><li>Possible proxies: time spent; clickes on result; follow up queries</li></ul><h2 id="metric-definitions-data-capture">Metric definitions &amp; Data Capture</h2><p>Def #1 (Cookie probability): For each <time interval>, number of cookies that click divided by number of cookies</time></p><p>Def #2 (Pageview probability): Number of pageviews with a click within <time interval> divided by number of pageviews</time></p><p>Def #3 (Rate): Number of clicks divided by number of pageviews</p><h3 id="filtering-and-segmenting">Filtering and Segmenting</h3><blockquote><p>Good for evaluating definitions and building intuitions</p></blockquote><p>You may have to filter out spam and fraud to de-bias the data. You do want to be careful you don't introduce bias into your data by doing the filtering.</p><p>One way to figure out if you are biasing or de-biasing the data by filtering, is to <strong>slice your data</strong> by country, or by language, or by platform, and then calculate the metric for each slice after filterig. If you are affecting any slide disproportionately, then you may be biasing your data with filtering</p><p>To remove any weekly effects when looking say at total active cookies over time, use week-over-week i.e. divide current data by data from a week ago. Alternately, one can use year-over-year.</p><h2 id="summary-metrics">Summary Metrics</h2><blockquote><p>Summarize all of these individual data measurements into a single summary metric.</p></blockquote><p><strong>Characteristics for your metric</strong>:</p><ol type="1"><li>the <strong>sensitivity</strong> and <strong>robustness</strong>: You want your metric to be sensitive enough, in order to actually detect a change when you, when you're testing your possible future options,</li><li>the <strong>distribution</strong>: The most ideal way of doing this is to do a retrospective analysis to compute a histogram.</li></ol><p><strong>4 categories of summary metrics:</strong></p><ol type="1"><li>Sums and counts: # of users who visited</li><li><p>Distributional metrics: the means, the medians, the 25th, the 75th and 90th percentiles.</p></li><li>Probabilities and rates.</li><li><p>Ratios: can compute a whole range of different business models, and various different things that you may care about, but they can be very difficult to characterize.</p></li></ol><h2 id="sensitivity-and-robustness"><strong>Sensitivity</strong> and <strong>Robustness</strong></h2><p>Whether the metric is sensitive to changes you care about, and is robust to changes you don‚Äôt care about</p><ul><li><strong>mean</strong> is sensitive to outliers - NOT robust</li><li><strong>median</strong> is robust but not sensitive to changes to small group of users</li></ul><p>How to measure <strong>Sensitivity</strong> and <strong>Robustness</strong>:</p><ol type="1"><li><strong>A/A tests</strong> to see if the metric picks up any spurious differences</li><li>Using <em>prior experiments</em> to see if the metric moves in a way that intuitively make sense.</li><li><em>Retrospective analysis</em> of log data: look back at changes you made to your website and see if the metrics you're interested in actually moved in conjunction with chose changes.</li></ol><h2 id="absolute-vs.-relative-difference">Absolute vs. Relative difference</h2><p>Suppose you run an experiment where you measure the number of visits to your homepage, and you measure 5000 visits in the control and 7000 in the experiment. Then the absolute difference is the result of subtracting one from the other, that is, 2000. The <strong>relative difference</strong> is the absolute difference divided by the control metric, that is, 40%.</p><p>If you are running a lot of experiments you want to use the relative difference i.e the percentage change.</p><ul><li>The main advantage : you only have to choose one <em>practical significance boundary</em> to get stability over time rather than change it as the system changes.</li><li>The main disadvantage : <strong>variability</strong>, relative differences such as <em>ratios</em> are not as well behaved as absolute differences</li></ul><h6 id="relative-differences-in-probabilities">Relative differences in probabilities</h6><p>For probability metrics, people often use percentage points to refer to absolute differences and percentages to refer to relative differences. For example, if your control click-through-probability were 5%, and your experiment click-through-probability were 7%, the absolute difference would be 2 percentage points, and the relative difference would be 40 percent. However, sometimes people will refer to the absolute difference as a 2 percent change, so if someone gives you a percentage, it's important to clarify whether they mean a relative or absolute difference!</p><h2 id="variability">Variability</h2><p>We want to check the variability of a metric to later determine the sizing of the experiment and to analyze confidence intervals and draw conclusions. If we have a metric that varies a lot, then the practical significance level that we are looking for may not be feasible.</p><p>To calculate the confidence interval, you need</p><ul><li>Variance (or standard deviation)</li><li>Distribution</li></ul><p>Binomial distribution:</p><p><span class="math display">\[SE=\sqrt{\frac{\hat{p}(1-\hat{p})}{N}}\]</span></p><p><span class="math display">\[m=z^*SE\]</span></p><p>We use the fact that this was a binomial distribution in two ways.</p><ol type="1"><li><p>we use the fact that this was a binomial distribution to get this formula for the standard error.</p></li><li><p>this formula for the margin of error depends on the assumption that this is a normal distribution, as the binomial approaches a normal distribution as N gets larger.</p></li></ol><table><colgroup><col style="width: 36%"><col style="width: 27%"><col style="width: 36%"></colgroup><thead><tr class="header"><th>Type of metric</th><th>Distribution</th><th>Estimated variance</th></tr></thead><tbody><tr class="odd"><td>Probability</td><td>Binomial (normal)</td><td><span class="math inline">\(\frac{\hat{p}(1-\hat{p})}{N}\)</span></td></tr><tr class="even"><td>Mean</td><td>Normal</td><td><span class="math inline">\(\frac{\hat{\sigma}^2}{n}\)</span> (<span class="math inline">\(\hat{\sigma}\)</span> : variance of the sample)</td></tr><tr class="odd"><td>Medium/percentile</td><td>Depends on the assumption of data distribution</td><td>Depends</td></tr><tr class="even"><td>Count/difference</td><td>Normal (maybe)</td><td><span class="math inline">\(Var(X)+Var(Y)\)</span></td></tr><tr class="odd"><td>Rates</td><td>Poisson</td><td><span class="math inline">\(\bar{X}\)</span> (mean)</td></tr><tr class="even"><td>Ratios (<span class="math inline">\(\frac{\hat{p}_{exp}}{\hat{p}_{cont}}\)</span> instead of <span class="math inline">\(\hat{p}_{exp}-\hat{p}_{cont}\)</span>)</td><td>Depends</td><td>Depends</td></tr></tbody></table><p>The variance of the actual metric: if you were to collect a new sample, how would you expect this metric to vary?</p><p>The variance of the sample: take each of your data points and then collect the variance of them.</p><p><strong>Calculating CI for a Mean</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">N= [<span class="number">87029</span>, <span class="number">113407</span>, <span class="number">84843</span>, <span class="number">104994</span>, <span class="number">99327</span>, <span class="number">92052</span>, <span class="number">60684</span>]</span><br><span class="line">N_mean=sum(N)/len(N)</span><br><span class="line">N_std=math.sqrt(sum([(n-N_mean)**<span class="number">2</span> <span class="keyword">for</span> n <span class="keyword">in</span> N])/(len(N)<span class="number">-1</span>))</span><br><span class="line">SE=N_std/math.sqrt(len(N))</span><br><span class="line"></span><br><span class="line">cf_min,cf_max=N_mean-SE*<span class="number">1.96</span>,N_mean+SE*<span class="number">1.96</span></span><br></pre></td></tr></table></figure><h3 id="non-parametric-methods">Non-parametric methods</h3><p>A way to analyze the data without making an assumption about what the distribution Is.</p><ul><li><strong>sign test</strong></li><li>Compute variance empirically</li></ul><p>Reason of using Non-parametric methods:</p><ul><li>for more complicated metrics, you might need to estimate the variance empirically instead of computing it analytically.</li><li>At Google, it was observed that the analytical estimates of variance was often under-estimated, and therefore they have resorted to use empirical measurements based on A/A test to evaluate variance.</li></ul><p>-&gt; using <strong>A versus A experiments</strong> across the board to estimate the empirical variability of all of our metrics.</p><p><strong>What are A versus A experiments</strong>: in an A versus A test, what you have is a control, A against another control A, and so there's actually no change in what the users are seeing. What that means that any differences that you measure are due to the underlying variability, maybe of your system, of the user population, what users are doing, all of those types of things.</p><p>If you see a lot of variability in a metric in an A versus A test, it's probably too sensitive to be useful in, in evaluating a real experiment.</p><p><strong>How many A/A tests are needed to get a good sense?</strong>: The key rule of thumb to keep in mind is that the standard deviation is going to be proportional to the square root of the number of sample.</p><p><strong>What if you can't run many A/A tests for some reason?</strong>: another option is to run one really big A versus A experiment. And then using bootstrap, where what you do is you take that big sample, and you randomly divvy it up into a bunch of small samples and you do the comparison within those random subsets.</p><p>One advantage of running the lots of different A/A tests is because if your experiment system is itself complicated, it's actually a very good test of your system.</p><p><strong>Uses of A/A tests:</strong></p><ol type="1"><li>Compare result to what you expect (sanity check)</li><li>Estimate variance empirically and use your assumption about the distribution to calculate confidence</li><li>Directly estimate confidence interval without making any assumption of the data</li></ol><p><strong>Calculating a Confidence Interval Empirically</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># click-through-probability of 40 A/A tests or bootstrap samples</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">group1=[<span class="number">0.02</span>, <span class="number">0.11</span>, <span class="number">0.14</span>, <span class="number">0.05</span>, <span class="number">0.09</span>, <span class="number">0.11</span>, <span class="number">0.09</span>, <span class="number">0.1</span> , <span class="number">0.14</span>, <span class="number">0.08</span>, <span class="number">0.09</span>,</span><br><span class="line">       <span class="number">0.08</span>, <span class="number">0.09</span>, <span class="number">0.08</span>, <span class="number">0.12</span>, <span class="number">0.09</span>, <span class="number">0.16</span>, <span class="number">0.11</span>, <span class="number">0.12</span>, <span class="number">0.11</span>, <span class="number">0.06</span>, <span class="number">0.11</span>,</span><br><span class="line">       <span class="number">0.13</span>, <span class="number">0.1</span> , <span class="number">0.08</span>, <span class="number">0.14</span>, <span class="number">0.1</span> , <span class="number">0.08</span>, <span class="number">0.12</span>, <span class="number">0.09</span>, <span class="number">0.14</span>, <span class="number">0.1</span> , <span class="number">0.08</span>,</span><br><span class="line">       <span class="number">0.08</span>, <span class="number">0.07</span>, <span class="number">0.13</span>, <span class="number">0.11</span>, <span class="number">0.08</span>, <span class="number">0.1</span> , <span class="number">0.11</span>]</span><br><span class="line">group2=[<span class="number">0.07</span>, <span class="number">0.11</span>, <span class="number">0.05</span>, <span class="number">0.07</span>, <span class="number">0.1</span> , <span class="number">0.07</span>, <span class="number">0.1</span> , <span class="number">0.1</span> , <span class="number">0.12</span>, <span class="number">0.14</span>, <span class="number">0.04</span>,</span><br><span class="line">       <span class="number">0.07</span>, <span class="number">0.07</span>, <span class="number">0.06</span>, <span class="number">0.15</span>, <span class="number">0.09</span>, <span class="number">0.12</span>, <span class="number">0.1</span> , <span class="number">0.08</span>, <span class="number">0.09</span>, <span class="number">0.08</span>, <span class="number">0.08</span>,</span><br><span class="line">       <span class="number">0.14</span>, <span class="number">0.09</span>, <span class="number">0.1</span> , <span class="number">0.08</span>, <span class="number">0.08</span>, <span class="number">0.09</span>, <span class="number">0.08</span>, <span class="number">0.11</span>, <span class="number">0.11</span>, <span class="number">0.1</span> , <span class="number">0.14</span>,</span><br><span class="line">       <span class="number">0.1</span> , <span class="number">0.08</span>, <span class="number">0.05</span>, <span class="number">0.19</span>, <span class="number">0.11</span>, <span class="number">0.08</span>, <span class="number">0.13</span>]</span><br><span class="line">confidence_level=<span class="number">0.95</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># assume metric is normally distributed</span></span><br><span class="line">difference=[i-j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(group1,group2)]</span><br><span class="line">mean=sum(difference)/len(difference)</span><br><span class="line">SD=math.sqrt(sum([(i-mean)**<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> difference ])/(len(difference)<span class="number">-1</span>))</span><br><span class="line"><span class="comment"># m=SD*z-score</span></span><br><span class="line">m=SD*<span class="number">1.96</span></span><br><span class="line">ci_max=mean+m</span><br><span class="line">ci_min=mean-m</span><br><span class="line"></span><br><span class="line">&gt; ci_min,ci_max</span><br><span class="line">&gt; (<span class="number">-0.06702773846019527</span>, <span class="number">0.07552773846019528</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># no assumption of metric distribution</span></span><br><span class="line">difference=sorted(difference)</span><br><span class="line">ci_min,ci_max=difference[<span class="number">1</span>],difference[<span class="number">-2</span>]</span><br><span class="line"></span><br><span class="line">&gt; ci_min,ci_max</span><br><span class="line">&gt; (<span class="number">-0.06000000000000001</span>, <span class="number">0.08</span>)</span><br></pre></td></tr></table></figure><p>In summary, different metrics have different variability. The variability may be high for certain metrics which makes them useless even if they make business or product sense. Computing the variability of a metric is tricky and one needs to take a lot of care.</p><p>For a lot of analysts, a majority of the time is spent is validating and choosing a metric compared to actually running the experiment. Being able to standardize the definitions was critical in the test. When measuring latency, are you talking about when the first byte loads and when a last byte loads. Also, for latency, the mean may not change at all. The signals (e.g. slow/fast connections or browsers) causes lumps in the distribution, and no central measure works. One needs to look at the right percentile metric. The key thing is that you are build</p><h1 id="design-an-experiment"><strong>Design an Experiment</strong></h1><ol type="1"><li>Choose <strong>subject</strong>: What are the units in the population you are going to run the test on? (unit of diversion)</li><li>Choose <strong>population</strong>: What population are you going to use (US only?)</li><li>Size</li><li>Duration</li></ol><h2 id="unit-of-diversion">Unit of Diversion</h2><p>Commonly used units of diversion are:</p><ol type="1"><li><strong>User identifier (id)</strong>: Typically the username or email address used on the website. It is typically stable and unchanging. If user id is used as a unit of diversion, then it is either in the test group or the control group. User ID is personally identifiable</li><li><strong>Anonymous id</strong>: This is usually an anonymous identifier such as a <strong>cookie</strong>. It changes with browser or device. People may often refresh their cookies every time they log in. It is difficult to refresh a cookie on an app or a phone compared to the computer.</li><li><strong>Event</strong>: An event is a page load that can change for each user. This is used typically for non-user-visible changes.</li></ol><p>Less common:</p><ol type="1"><li><strong>Device id</strong>: Typically available for mobile devices. It is tied to a specific device and cannot be changed by the user. Personally identifiable.</li><li><strong>IP address</strong>: location specific, but may change as the user changes location (e.g. testing on infrastructure change to test impact on latency)</li></ol><p>3 main considerations in selecting an appropriate unit of diversion:</p><ol type="1"><li><strong>Consistency</strong></li><li>Ethical - Informed consent: a issue when using user id.</li><li>Variability</li></ol><h3 id="consistency-of-diversion">Consistency of Diversion</h3><p>If you're testing a change that crosses the sign in, sign out border, then a <strong>user ID</strong> doesn't work as well, use cookie instead.</p><p><strong>For user visible changes</strong>, you would definitely use a <strong>cookie</strong> or a <strong>user ID</strong>. But there are lots of non-user-visible changes (latency changes, back-end infra changes or ranking changes):</p><p>Examples:</p><ul><li>Cookie: change button color and size (distracting if changes on reload; different look on difference devices is ok)</li><li>User id: Add instructors note before quizzes (cross device consistency important)</li><li>Event: Reducing video load time; change order of search results (users won't probably notice)</li></ul><p>IP based diversion not very useful:</p><ul><li>not as consistent as cookie or user id as ip randomly change depending on the provider; no clean randomization like event based diversion.</li><li>only choice: testing out one hosting provider versus a different hosting provider to understand the impact of latency.</li><li>May not get a clean comparison between your experiment and your control:some providers aggregate all of those modem dialup users into a single IP address, so it's hard to find comparable population of users in the control group.</li></ul><h3 id="unit-of-analysis-v.s.-unit-of-diversion">Unit of Analysis v.s. Unit of Diversion</h3><p><strong>Variability</strong> is higher when it is calculated empirically than when calculated analytically. This is because the <strong>unit of analysis</strong> (i.e. the denominator in the metric. e.g. page view in click through rate) is different from the unit of variability.</p><p>When your <em>unit of diversion</em> is also a page view, so as would be the case in an <strong>event</strong> base diversion, then the analytically computed variability is likely to be very close to the empirically computed variability.</p><p>If, however, your unit of diversion is a <strong>cookie</strong> or a user id then the variability of your same metric click through rate is actually going to be much higher.</p><p>This is because when you're actually computing your variability analytically, you're fundamentally making an assumption about the distribution of the data and what's considered to be independent.</p><ul><li><p>When you're doing event-based diversion every single event is a different randomdraw, and so your independence assumption is actually valid.</p></li><li><p>When you're doing cookie or user ID based diversion, that independence assumption is no longer valid because you're actually diverting groups of events. And so they're actually correlated together.</p></li></ul><p><strong>Measure variablity of a metric</strong>:</p><p>Unit of diversion: queries (event-based) v.s. cookies</p><p>Metric: coverage (the percentage of queries for which an ad is shown) = <span class="math inline">\(\frac{\# queries\ with\ ads}{\#\ queries}\)</span></p><p>Unit of analysis: query</p><p>Binomial: <span class="math inline">\(SE=\sqrt{\frac{p(1-p)}{N}}\)</span></p><p>The SE of coverage under cookies diversion is much larger then that of queries.</p><blockquote><p>When unit of analysis = unit of diversion, variability tends to be lower and closer to the anlytical estimate.</p></blockquote><h2 id="choose-population">Choose Population</h2><p><strong>intra-user experiment:</strong> expose the same user to this feature being on and of over time, and you actually analyze how they behave in different time windows.</p><ul><li>pitfalls:<ul><li>be really careful that you choose a comparable time window.(not 2 weeks before Christmas)</li><li>different behaviors as a result of a frustration or a learning problem, where people learn to use the particular feature in the first two weeks</li></ul></li></ul><p><strong>interleaved experiment</strong>: expose the same user to the A and the B side at the same time. And typically this only works in cases where you're looking at reordering a list.</p><p><strong>inter-user experiments</strong>: different people on the A side and on the B side.</p><h3 id="target-population">Target Population</h3><p>Some easy divisions of your user space: what browser they're on, what geo location they come from, what country, what language they're using.</p><p>Reasons why you might make that decision in advance:</p><ul><li>For a feature not sure if you're going to release it and it's a pretty high profile launch, you might want to restrict how many of your users have actually seen it.</li><li>If you're running a couple of different experiments at your company at the same time, you might not want to overlap.</li><li>You may not want to dilute the effect of your experiment across a global population. So if you're analyzing an experiment for the first time, and it only affects English, you may want to actually do your analysis specific on English,</li></ul><p><strong>Targeting an Experiment</strong>:</p><blockquote><p>Filtering could also affect variability</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># New Zealand</span></span><br><span class="line">N_cont = <span class="number">6021</span></span><br><span class="line">X_cont = <span class="number">302</span></span><br><span class="line">N_exp = <span class="number">5979</span></span><br><span class="line">X_exp = <span class="number">374</span></span><br><span class="line"></span><br><span class="line">p_cont=X_cont/N_cont</span><br><span class="line">p_exp=X_exp/N_exp</span><br><span class="line">p_pool=(X_cont+X_exp)/(N_cont+N_exp)</span><br><span class="line">SE=math.sqrt((p_pool*(<span class="number">1</span>-p_pool)*(<span class="number">1</span>/N_cont+<span class="number">1</span>/N_exp)))</span><br><span class="line">d_hat=p_exp-p_cont</span><br><span class="line">m=<span class="number">1.96</span>*SE</span><br><span class="line">&gt; m,d_hat</span><br><span class="line">&gt; (<span class="number">0.00825068746366646</span>, <span class="number">0.012394485165776618</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Global</span></span><br><span class="line">N_cont = <span class="number">50000</span> + <span class="number">6021</span></span><br><span class="line">X_cont = <span class="number">2500</span> + <span class="number">302</span></span><br><span class="line">N_exp = <span class="number">50000</span> + <span class="number">5979</span></span><br><span class="line">X_exp = <span class="number">2500</span> + <span class="number">374</span></span><br><span class="line"></span><br><span class="line">p_cont=X_cont/N_cont</span><br><span class="line">p_exp=X_exp/N_exp</span><br><span class="line">p_pool=(X_cont+X_exp)/(N_cont+N_exp)</span><br><span class="line">SE=math.sqrt((p_pool*(<span class="number">1</span>-p_pool)*(<span class="number">1</span>/N_cont+<span class="number">1</span>/N_exp)))</span><br><span class="line">d_hat=p_exp-p_cont</span><br><span class="line">m=<span class="number">1.96</span>*SE</span><br><span class="line">&gt; m,d_hat</span><br><span class="line">&gt; (<span class="number">0.0025691881506085417</span>, <span class="number">0.0013237234004343165</span>)</span><br></pre></td></tr></table></figure><h3 id="population-v.s.-cohort">Population v.s. Cohort</h3><p>Cohort: people who enter the experiment at the same time. You define an entering class and you only look at users who entered your experiment on both sides around the same time, and you go forward from there.</p><p><strong>Cohorts</strong> are harder to analyze, limit your experiment to a subset of population, can affect variability .</p><p>Want to use cohort instead of population:<strong>user stability</strong></p><ul><li>Looking for learning effects</li><li>Examining user retention</li><li>Want to increate user activity</li><li>Anything that require users to be established</li></ul><h2 id="experiment-design-and-sizing">Experiment Design and Sizing</h2><p><strong>How to reduce the size of experiments:</strong></p><p>Experiment: change order of courses in the course list</p><p>Metric: click-through-rate</p><p>Unit-of-diversion: cookie</p><p><span class="math inline">\(d_{min}=0.05\)</span>, <span class="math inline">\(\alpha=0.05\)</span>, <span class="math inline">\(\beta=0.2\)</span>, <span class="math inline">\(SE=0.0628\)</span> for 1000 page views</p><p>Result: need 300,000 pageviews per group!</p><p>Which strategies could reduce the number of page views?</p><ol type="1"><li>Increase <span class="math inline">\(d_{min}\)</span>, <span class="math inline">\(\alpha\)</span>, or <span class="math inline">\(\beta\)</span></li><li>Change unit of diversion to page view<ul><li>Makes unit of diversion same as unit of analysis</li><li>The variability of the metric might decrease and thus decrease the number of page views you need to be confident in your results</li></ul></li><li>Target experiment to specific traffic<ul><li>Non-english traffic will dilute the results</li><li>Could impact choice of practical significance boundrey<ul><li>since you're only looking at a subset of the traffic, you might need a bigger change before it matters to the business;</li><li>Or since your variablity is probably lower, you might want to detect smaller changes rather than decreasing the number of page views.</li></ul></li></ul></li></ol><p>How are you actually going to detect impact? What effect does that have on the size of the experiment when you really don't know what fraction of your population is going to be affected?</p><ul><li>run a pilot where you turn on the experiment for a little while and see who's affected, or you can even just use the first day or the first week of data to try to get a better guess at what fraction of your population you're really looking at.</li></ul><h2 id="duration-v.s.-exposure">Duration v.s. Exposure</h2><p><strong>Practical considerations in experimental design</strong>:</p><ol type="1"><li>Duration</li><li>When to run the experiment</li><li>Fraction of the traffic to send to the experiment</li></ol><p>The duration of your experiment, is related to the proportion of traffic that you're sending through your experiment.</p><p><strong>Why not run on all of the traffic to get results quicker?</strong></p><ul><li>Safety: not sure if it's working on all browser or how the users will react</li><li>Press: people blogging about features you might not keep</li><li>Other sources of variablity : holiday, weekly variation</li><li>Running multiple tasks at your company: to be comparable, run them at the same time on smaller percentages of traffic.</li></ul><p><strong>Note:</strong> There is weekly variation in traffic and metric, so it's better to run on mix of weekend and weekday days. For risky change, run longer with less traffic.</p><h2 id="learning-effects">Learning Effects</h2><p><strong>learning effects</strong> is basically when you want to measure user learning. Or effectively whether a user is adapting to a change or not.</p><p>Two different types of learning effects</p><ol type="1"><li>Change aversion &quot;I don't like anything.&quot;</li><li>Knowledge effect &quot;Let me try everything around.&quot;</li></ol><p>When users first encounter a change they will react in one of these two ways, but will eventually plateau to a very different behavior.</p><p><strong>Considerations when measure a learning effect:</strong></p><ol type="1"><li><p>The key issue with trying to is <strong>time</strong>Ôºö It takes time for you just to actually adapt to a change and often times you don't have the luxury of taking that much time to make a decision.</p></li><li>Choosing the unit of diversion correctly. - a cookie or a user ID</li><li>Dosage: because a lot of the learning is based on not just a slight time but how often they see the change. Then you probably want to be using a cohort as opposed to just a population. - choose a cohort in both the experiment and the control based on either how long they've been exposed to the change or how many times they've seen it.</li><li><p>risk and duration: both of those mean to run it through a small proportion of your users for a longer period of time.</p></li></ol><p><strong>pre-periods and post-periods</strong>:</p><p>Before run your <strong>A/B test</strong>, you're on a <em>pre-period</em> on the exact same populations but they're receiving the exact same frequence. It's an <strong>A/A test</strong> on the same set of users.</p><p>In the <strong>pre-period</strong>, if you measure any difference between your experiment and your control populations that difference is due to something else (system variability, user variability).<strong>Pre-period</strong> helps you know that any difference that you measure in your experiment and control is due to the experiment, and not due to any preexisting and inherent differences in your population.</p><p>A <strong>post-period</strong> is saying, after I run my experiment, my control, I'm going to run another <em>A versus A test</em>. And then, what, what we can say is that if there are any differences in the experiment and the control populations after I've run my experiment, then I can attribute those differences to <strong>user learning</strong> that happened in the experiment period.</p><h1 id="analyze-results-1">Analyze Results</h1><h2 id="sanity-checks">Sanity Checks</h2><p><strong>2 main types of checks:</strong></p><ol type="1"><li><strong>Population sizing metrics</strong> based on your unit of diversion<ul><li>Check your experiment population and your control populations are actually comparable.</li></ul></li><li><strong>Actual invariants:</strong> metrics that are same in the control and experiment groups and shouldn't change when you run your experiment</li></ol><p><strong>Step 1: Choosing invariant metircs</strong></p><p><strong>Step 2: Checking invariants</strong></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">Run experiment for 2 weeks</span></span><br><span class="line"></span><br><span class="line">‚ÄãUnit of diversion: cookie</span><br><span class="line"></span><br><span class="line">‚ÄãTotal control: 64454</span><br><span class="line"></span><br><span class="line">‚ÄãTotal experiment: 61818</span><br><span class="line"></span><br><span class="line">‚ÄãHow would you figure out if the difference is within expectations?</span><br><span class="line"></span><br><span class="line">‚ÄãGiven: each cookie is randomly assigned to the control or experiment group with a prob of 0.5.</span><br></pre></td></tr></table></figure><pre><code>1. Compute SD of binomial with $p=0.5$ of success. $SD=\sqrt{0.5 \cdot0.5/(64454+61818)}=0.0014$2. Multiple by z-score to get margin of error. $m=1.96*SD=0.0027$3. Compute confidence interval around 0.5. $ [0.4973, 0.5027]$4. Check whether observed fraction is within interval. $64454/(64454+61818)=0.5104$ Not within interval</code></pre><p><strong>Step 3: what to do</strong></p><ul><li>Talk to engineers</li><li>Try <em>slicing</em> to see if one particular slice is weird</li><li>Check age of cookies - does one group has more new cookies?</li></ul><p><strong>What happens if one of your sanity checks fails</strong>: analyzing why your sanity checks fail</p><ol type="1"><li>work with your engineers to understand is there something going on with the <em>experiment infrastructure</em>, <em>experiment set up</em>, <em>experiment diversion</em>.</li><li><p><strong>retrospective analysis</strong>: Try and recreate experiment diversion from the data capture,</p></li><li>use pre &amp; post periods<ul><li>If you're in a pre-period,did I see the same changes in those invariance in my pre-period?<ul><li>If I saw them in the pre-period and the experiment, that points to a problem with the experiment infrastructure, the set up, something along those lines.</li><li>if you see the change only in your experiment but not in the pre-period, that points to something with the experiment itself (data capture or something along those lines).</li></ul></li></ul></li></ol><p><strong>The most common reasons for data not matching up:</strong></p><ul><li>data capture: Maybe the change triggers very rarely, and you capture it correctly in the experiment, but you don't capture quickly in the control.</li><li>experiment's set up: Maybe you set up the filter for the experiment, but not the control.</li><li>experiment system</li></ul><blockquote><p>The key thing : if there really is a learning effect, then not very much change in the beginning, but it's increasing over time.</p></blockquote><blockquote><p>Not a learning effect: a big change from the beginning</p></blockquote><h2 id="single-metric">Single Metric</h2><p>Goal: make a business decision about whether your experiment has favorably impacted your metrics. Analytically, decide if you've observed a statistically significant result of your experiment.</p><p><strong>How do you decide if the change was statistically significant?</strong>:</p><ul><li>Characterizing the metric, understanding how it behaves</li><li>Use variability to estimate <strong>how long</strong> we needed to run the experiment for and <strong>size</strong> our experiment appropriately</li><li>Use the results of last 2 steps to estimate the <strong>variability</strong> we need to analyze the A/B experiment</li></ul><p><strong>What if our results are not statistically significant?</strong>:</p><ul><li>Break down the result into different platforms, different days of the week -&gt; find bugs in your experiment setup &amp; give you a new hypothesis about how people are reacting to the experiment.</li><li>Cross checking your results with other methods, like non parametric sign tests to compare the results to what you got from your parametric hypothesis test.</li></ul><h3 id="analysis-with-single-metric">Analysis with Single Metric</h3><p><strong>Experiment I :</strong> change color and place of 'start now' button.</p><p>Metric: click-through-rate</p><p>Unit of diversion: cookie</p><p><span class="math inline">\(X_{cont}=352\)</span>, <span class="math inline">\(N_{cont}=7370\)</span></p><p><span class="math inline">\(X_{exp}=565\)</span>, <span class="math inline">\(N_{exp}=7270\)</span></p><p><span class="math inline">\(d_{min}=0.01\)</span>, <span class="math inline">\(\alpha=0.05\)</span>, <span class="math inline">\(\beta=0.2\)</span></p><p><em>Empirical SE</em>: <span class="math inline">\(0.0035\)</span> with 10000 page views in each group</p><p><span class="math display">\[\begin{align}&amp;SE \sim \sqrt{\frac{1}{N_1}+\frac{1}{N_2}} \\&amp;\frac{0.035}{\sqrt{\frac{1}{10000}+\frac{1}{10000}}}=\frac{SE}{\sqrt{\frac{1}{7370}+\frac{1}{7270}}} \\&amp;SE=0.0041 \\&amp;\hat{d}=\hat{r}_{exp}-\hat{r}_{cont}=\frac{565}{7270}-\frac{352}{7370}=0.03  \\&amp;m=1.96*0.0041=0.0080 \\&amp;Confidence\ Interval:[0.022,0.038]\end{align}\]</span> Recommandation: <strong>launch</strong></p><p><strong>Sign Test:</strong></p><p>‚Äã Number of days: 7</p><p>‚Äã Number of days with positive change: 7</p><p>‚Äã If no difference, 50% chance of positive change each day</p><p>‚Äã (7 days too short, cannot assume normal by binomial)</p><p>‚Äã Two-tailed <strong>p-value</strong>: <em>0.0156</em> (the prob of observing a result at least this extreme by chance)</p><p>‚Äã - Since this is less than the chosen alpha of .05, the sign test agrees with the hypothesis test that this result is unlikely to happen by chance.</p><p>‚Äã Recommandation: <strong>launch</strong></p><p><strong>Experiment II :</strong></p><p>Metric: click-through-rate</p><p><span class="math inline">\(d_{min}=0.01\)</span>, <span class="math inline">\(\alpha=0.05\)</span>,</p><p>Empirical SE: <span class="math inline">\(0.0062\)</span> with 5000 page views in each group</p><ul><li><strong>Effect size:</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Xs_cont = [<span class="number">196</span>, <span class="number">200</span>, <span class="number">200</span>, <span class="number">216</span>, <span class="number">212</span>, <span class="number">185</span>, <span class="number">225</span>, <span class="number">187</span>, <span class="number">205</span>, <span class="number">211</span>, <span class="number">192</span>, <span class="number">196</span>, <span class="number">223</span>, <span class="number">192</span>]</span><br><span class="line">Ns_cont = [<span class="number">2029</span>, <span class="number">1991</span>, <span class="number">1951</span>, <span class="number">1985</span>, <span class="number">1973</span>, <span class="number">2021</span>, <span class="number">2041</span>, <span class="number">1980</span>, <span class="number">1951</span>, <span class="number">1988</span>, <span class="number">1977</span>, <span class="number">2019</span>, <span class="number">2035</span>, <span class="number">2007</span>]</span><br><span class="line">Xs_exp = [<span class="number">179</span>, <span class="number">208</span>, <span class="number">205</span>, <span class="number">175</span>, <span class="number">191</span>, <span class="number">291</span>, <span class="number">278</span>, <span class="number">216</span>, <span class="number">225</span>, <span class="number">207</span>, <span class="number">205</span>, <span class="number">200</span>, <span class="number">297</span>, <span class="number">299</span>]</span><br><span class="line">Ns_exp = [<span class="number">1971</span>, <span class="number">2009</span>, <span class="number">2049</span>, <span class="number">2015</span>, <span class="number">2027</span>, <span class="number">1979</span>, <span class="number">1959</span>, <span class="number">2020</span>, <span class="number">2049</span>, <span class="number">2012</span>, <span class="number">2023</span>, <span class="number">1981</span>, <span class="number">1965</span>, <span class="number">1993</span>]</span><br><span class="line">SE_emp=<span class="number">0.0062</span></span><br><span class="line">Ns_emp=<span class="number">5000</span></span><br><span class="line">SE=SE_emp/math.sqrt(<span class="number">1</span>/Ns_emp+<span class="number">1</span>/Ns_emp)*math.sqrt(<span class="number">1</span>/sum(Ns_cont)+<span class="number">1</span>/sum(Ns_exp))</span><br><span class="line">d_hat=sum(Xs_exp)/sum(Ns_exp)-sum(Xs_cont)/sum(Ns_cont)</span><br><span class="line">m=<span class="number">1.96</span>*SE</span><br><span class="line">ci_min,ci_max=d_hat-m,d_hat+m</span><br><span class="line">&gt; ci_min,ci_max</span><br><span class="line">&gt; (<span class="number">0.006465853496236934</span>, <span class="number">0.016736185710796242</span>)</span><br></pre></td></tr></table></figure><p>Since the confidence level is larger than 0, it's statistically significant.</p><ul><li><strong>Sign test:</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ctr_cont=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(Xs_cont,Ns_cont)]</span><br><span class="line">ctr_exp=[i/j <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(Xs_exp,Ns_exp)]</span><br><span class="line"></span><br><span class="line">days=len(Xs_cont)</span><br><span class="line">positive_days=sum([exp&gt;cont <span class="keyword">for</span> exp,cont <span class="keyword">in</span> zip(ctr_exp,ctr_cont)])</span><br><span class="line">&gt; days,positive_days</span><br><span class="line">&gt; (<span class="number">14</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure><p>The two-tail P value is <span class="math inline">\(0.4240\)</span>. Since p-value is larger than alpha, it's not significant.</p><h3 id="single-metric-gochas">Single Metric: Gochas</h3><p><strong>What if the sign test and the hypothesis test don't agree?</strong></p><p>Statistical reasons for counter-intuitive results: <strong>Simpson's paradox</strong></p><blockquote><p>Different subgroups in the data like user populations. Within each subgroup, the results are stable but when aggregated together, it's the mix of subgroups that actually drives your result.</p></blockquote><ul><li>New users might be correlated with weekend use, experienced users who react differently are correlated with weekday use. Within each group, their behavior is stable. What drives the results of your experiment are how many people from each group.</li></ul><h2 id="multiple-metrics">Multiple Metrics</h2><p><strong>What changes when you have multiple evaluation metrics instead of just one?</strong></p><blockquote><p>As you test more metrics, it becomes more likely that one of them will show a statistically significant result by chance.</p></blockquote><p>But the significant result shouldn't be repeatable. If you did the same experiment on another day or you slices or do some bootstrap analysis, you wouldn't see the same metric showing up as significant differences every time, it should occur randomly.</p><p><strong>Multiple comparisons</strong>: adjusts your significance level, so that it accounts for how many metrics or how many different tests you're doing.</p><ul><li>Do it when you want automatic alerting that tells you one of my metrics was significantly different on this experiment.</li></ul><h3 id="tracking-multiple-metrics">Tracking Multiple Metrics</h3><p>For 3 <em>independent</em> metrics, <span class="math inline">\(\alpha=0.05\)</span>, the chance of at least 1 false positive is <span class="math inline">\(P(FP‚â•1)=1-0.95^3=0.143\)</span></p><p><span class="math inline">\(Overall\ \alpha=1-(1-\alpha)^{\#\ of\ metrics}\)</span></p><p><strong>Problem:</strong> Probability of any False Positive increases as you increase number of metrics</p><p><strong>Solution</strong>: Use higher confidence level for each metric</p><ul><li><strong>Method1:</strong> Assume <em>independence</em> of metrics</li></ul><p>‚Äã <span class="math inline">\(\alpha_{overall}=1-(1-\alpha_{individual})^n\)</span>, calculate <span class="math inline">\(\alpha_{individual}\)</span> with specified <span class="math inline">\(\alpha_{overall}\)</span></p><ul><li><strong>Method2: Bonferroni Correction</strong><ul><li>Simple</li><li>No assumption</li><li><p>Conservative - guarantee to give <span class="math inline">\(\alpha_{overall}\)</span> at lease as small as specified</p></li><li><span class="math inline">\(\alpha_{individual}=\frac{ \alpha_{overall}}{n}\)</span></li><li><u>Problem:</u> Given metrics are correlated, and tend to move at the same time, this method is too conservative.</li><li>Recommendation:<ul><li>Rigorous answer: use a more sophisticated method like <a href="http://en.wikipedia.org/wiki/Closed_testing_procedure" target="_blank" rel="noopener">closed testing procedure</a>, the <a href="http://en.wikipedia.org/wiki/Bonferroni_bound" target="_blank" rel="noopener">Boole-Bonferroni bound</a>, and the <a href="http://en.wikipedia.org/wiki/Holm‚ÄìBonferroni_method" target="_blank" rel="noopener">Holm-Bonferroni method</a></li><li>In practice: <strong>Judgment Call</strong>, possibly based on business strategy</li></ul></li></ul></li></ul><p><strong>Different Strategies:</strong></p><ul><li><p>Control probability that <em>any</em> metric shows a false positive <span class="math inline">\(\alpha_{overall}\)</span> -&gt; <strong>family wise error rate (FWER)</strong> (Overal alpha)</p></li><li><p>Control false discovery rate (FDR):</p><p><span class="math display">\[FDR=E[\frac{\#\ false\ positives }{\#\ rejections}]\]</span></p><ul><li>Out of all of the rejections of the null, that is, all of the metrics that you declare to have a statistically significant difference. How many of them had a real difference as opposed to how many were false positives?</li><li>This really only makes sense if you have a huge number of metrics, say hundreds.</li></ul></li></ul><p>e.g.: Suppose that you have 200 metrics that you're measuring and you capped the false discovery rate at 0.05. What this means is that you're okay with having 5 false positives and 95 true positives in <u>every experiment</u>. The family wise error rate, or the overall alpha in this case, would be one, since you have at least one false positive every time. But the false discovery rate is 0.05.</p><h3 id="analyzing-multiple-metrics">Analyzing Multiple Metrics</h3><p><strong>How do I actually make a recommendation?</strong></p><ul><li><p>You really have to understand how people are reacting to the change because you can't quantitatively evaluate which one is better.</p></li><li><p>An alternative to using multiple metrics is to use an ‚ÄòOverall Evaluation Criterion‚Äô (OEC).</p></li></ul><p><strong>How to choose a good OEC?</strong></p><ul><li>Start with some kind of business analysis (Our company, as a whole, wants to look at 25% revenue plus 75% increase usage of the site.)</li><li>Run a whole bunch of different experiments and validate how they steer you.</li></ul><blockquote><p>Having an OIC doesn't have to be a formal number. It's really just trying to encapsulate what your company cares about. And how much you're going to be balancing something like stay time and clicks.</p></blockquote><h2 id="drawing-conclusion">Drawing Conclusion</h2><p>If you have statistically significant results, now the questions come down to</p><ul><li><p>Do you understand the change?</p></li><li><p>Do you want to launch the change?</p></li></ul><p>What if your change has a positive impact on one slice of your users, but for another slice, there's no impact or there's a negative impact?</p><ul><li>Is it a question about having different users, and how much they like or don't like the change? Have you seen that effect in other experiments? Do you have a bug?</li></ul><p><strong>How do you decide whether to launch your change or not?</strong></p><ul><li><p>Do I have statistically significant and practically significant results in order to justify the change?</p></li><li>Do I understand what that change has actually done with regards to user experience?</li><li><p>Is it worth it?</p></li></ul><blockquote><p><strong>The end goal is actually making that recommendation that shows your judgment.</strong></p></blockquote><h2 id="gochas-changes-over-time">Gochas: Changes Over Time</h2><p>It's good practice to always do a ramp-up when you actually want to launch a change.</p><p>The other thing is to remove all of the filters. So if you're only testing your change on English, for example, you want to test your change during your ramp-up on all users to check if there's been any incidental impact to unaffected users that you didn't test in the original experiment.</p><p><strong>Complication:</strong> the effect may actually flatten out as you ramp up the change.</p><p>Reasons of the effects not being repeatable:</p><ol type="1"><li>seasonality effects, holidays<ul><li><p>Capture seasonal / event-driven impacts: <strong>holdback</strong></p><ul><li>You launch your change to everyone except for a small holdback (a set of users), that don't get the change, and you continue comparing their behavior to the control. Now, in that case you should see the reverse of the impact that you saw in your experiment. And what you can do is you can track that over time until you're confident that your results are actually repeatable.</li></ul></li></ul></li><li><strong>novelty effect</strong> or <strong>change aversion</strong>: as users discover or adopt your change, then their behavior can change and therefore the measured effects can change.<ul><li><strong>Cohort analysis</strong></li></ul></li><li>If you don't control for the budgets properly, the effect can change as you ramp up.</li></ol><h2 id="summary">Summary</h2><ol type="1"><li>Check that your experiment was set up properly, Look at your end variance.Check that your experiment metrics are actually looking sane.</li><li>Remember you aren't just looking for statistical significance, you're really making a business decision.</li><li>Can't actually forget the overall business analysis<ul><li>Judgment call with regards to the user experience and the business:<ul><li>What's the engineering cost of maintaining the change?</li><li>Are there customer support or sales issues?</li><li>What's the opportunity cost if you actually choose to launch the change relative to the rewards you're going to get from the change or potentially not launching the change?</li></ul></li></ul></li></ol><h1 id="policy-and-ethics-for-experiments">Policy and Ethics for Experiments</h1><h2 id="four-principles">Four Principles</h2><h3 id="risk">1. Risk</h3><p>In the study, <em>what risk is the participant undertaking</em>? The main threshold is whether the risk exceeds that of ‚Äúminimal risk‚Äù. Minimal risk is defined as the probability and magnitude of harm that a participant would encounter in <u>normal daily life</u>.</p><h3 id="benefits">2. Benefits</h3><p>It is important to be able to state what the benefit would be from completing the study.</p><h3 id="alternatives">3. Alternatives</h3><p>In online experiments, the issues to consider are what the other alternative services that a user might have, and what the switching costs might be, in terms of time, money, information, etc.</p><h3 id="data-sensitivity">4. Data Sensitivity</h3><p><em>What data is being collected, and what is the expectation of privacy and confidentiality</em>?</p><ul><li>For new data being collected and stored, how sensitive is the data and what are the internal safeguards for handling that data? E.g., what access controls are there, how are breaches to that security caught and managed, etc.?</li><li>Then, for that data, how will it be used and how will participants‚Äô data be protected? How are participants guaranteed that their data, which was collected for use in the study, will not be used for some other purpose? This becomes more important as the sensitivity of the data increases.</li><li>Finally, what data may be published more broadly, and does that introduce any additional risk to the participants?</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;My study note of Udacity &lt;a href=&quot;https://classroom.udacity.com/courses/ud257&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;A/B Testing course&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Product Sense" scheme="https://nancyyanyu.github.io/categories/Product-Sense/"/>
    
    
      <category term="ab-testing" scheme="https://nancyyanyu.github.io/tags/ab-testing/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Exploratory Data Analysis </title>
    <link href="https://nancyyanyu.github.io/posts/c8b80037/"/>
    <id>https://nancyyanyu.github.io/posts/c8b80037/</id>
    <published>2020-04-29T09:58:33.000Z</published>
    <updated>2020-04-29T12:20:08.982Z</updated>
    
    <content type="html"><![CDATA[<p>My study note of the book <em>Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python - Chapter 1 Exploratory Data Analysis</em> .</p><a id="more"></a><h1 id="estimates-of-location">Estimates of Location</h1><p><strong>Trimmed mean</strong>: calculate by dropping a fixed number of sorted values at each end and then taking an average of the remaining values</p><ul><li><span class="math inline">\(\bar{x}= \frac{\sum_{i=p+1}^{n-p}x_{(i)}}{n-2p}\)</span></li><li>Eliminates the influence of extreme values</li></ul><p><strong>Weighted mean</strong>: calculate by multiplying each data value <span class="math inline">\(x_i\)</span> by a user-specified weight <span class="math inline">\(w_i\)</span> and dividing their sum by the sum of the weights.</p><ul><li><p><span class="math inline">\(\bar{x}_w=\frac{\sum_{i=1}^nx_iw_i}{\sum_{i=1}^nw_i}\)</span></p></li><li>Motivations for using a weighted mean:<ul><li>Some values are intrinsically more variable than others, and highly variable observations are given a lower weight.</li><li>The data collected does not equally represent the different groups that we are interested in measuring.</li></ul></li></ul><p><strong>Weighted median</strong>: first sort the data, although each data value has an associated weight. Instead of the middle number, the weighted median is a value such that the sum of the weights is equal for the lower and upper halves of the sorted list.</p><h2 id="robust-estimate-of-location"><em>Robust</em> estimate of location</h2><p><strong>median</strong>: a <em>robust</em> estimate of location since it is not influenced by <em>outliers</em> (extreme cases) that could skew the results.</p><p><strong>trimmed mean</strong>: widely used to avoid the influence of outliers.</p><ul><li>A compromise between the median and the mean: it is robust to extreme values in the data, but uses more data to calculate the estimate for location.</li></ul><h1 id="estimates-of-variability">Estimates of Variability</h1><p><strong><em>Variability</em></strong>, also referred to as <em>dispersion</em>, measures whether the data values are tightly clustered or spread out.</p><p><strong>Mean absolute deviation</strong>: <span class="math inline">\(\frac{\sum_{i=1}^n|x_i-\bar{x}|}{n}\)</span>, where <span class="math inline">\(x_i\)</span> is the sample mean.</p><p><strong>Variance</strong>: <span class="math inline">\(s^2=\frac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}\)</span></p><blockquote><p>If you use the intuitive denominator of <em>n</em> in the variance formula, you will underestimate the true value of the variance and the standard deviation in the population. This is referred to as a <em>biased</em> estimate. However, if you divide by <em>n</em> ‚Äì 1 instead of <em>n</em>, the variance becomes an <em>unbiased</em> estimate.</p><p>To fully explain why using <em>n</em> leads to a biased estimate involves the notion of <em>degrees of freedom</em>, which takes into account the number of constraints in computing an estimate. In this case, there are <em>n</em> ‚Äì 1 degrees of freedom since there is one constraint: the standard deviation depends on calculating the sample mean.</p></blockquote><p><strong>Standard deviation</strong>: <span class="math inline">\(s=\sqrt{variance}\)</span></p><ul><li>SD is much easier to interpret than the variance since it is on the same scale as the original data.</li><li>SD is preferred in statistics over the mean absolute deviation b.o. working with squared values is much more convenient than absolute values, especially for statistical models.</li><li>The variance and SD are especially sensitive to outliers since they are based on the squared deviations.</li></ul><h2 id="robust-estimate-of-variability"><em>Robust</em> estimate of variability</h2><p><strong><em>Median absolute deviation</em></strong> from the median (MAD): <span class="math inline">\(Median(|x_1-m|,|x_2-m|,...,|x_N-m|)\)</span>, where <em>m</em> is the median.</p><ul><li>Sometimes, the <em>median absolute deviation</em> is multiplied by a constant scaling factor to put the MAD on the same scale as the SD in the case of a <em>normal distribution</em>. The commonly used factor of 1.4826 means that 50% of the normal distribution fall within the range <span class="math inline">\(\pm MAD\)</span></li></ul><h3 id="estimates-based-on-percentiles"><strong>Estimates Based on Percentiles</strong></h3><p>A different approach to estimating <em>dispersion</em> is based on looking at the spread of the <em>sorted data</em>.</p><blockquote><p><em>order statistics</em></p></blockquote><p><strong>Range</strong>: (the most basic) the difference between the largest and smallest numbers</p><ul><li>Extremely sensitive to outliers and not very useful as a general measure of dispersion in the data.</li></ul><p><strong>Percentiles</strong>: the <em>P</em>th percentile is a value such that at least <em>P</em> percent of the values take on this value or less and at least (100 ‚Äì <em>P</em>) percent of the values take on this value or more.</p><ul><li>e.g. - to find the 80th percentile, sort the data. Then, starting with the smallest value, proceed 80 percent of the way to the largest value. Note that the median is the same thing as the 50th percentile.</li><li>Essentially the same as a <strong><em>quantile</em></strong>, with quantiles indexed by fractions (so the .8 quantile is the same as the 80th percentile).</li></ul><p><strong>Interquartile range</strong>(IQR): the difference between the 25th percentile and the 75th percentile</p><h1 id="exploring-the-data-distribution">Exploring the Data Distribution</h1><h2 id="percentiles-and-boxplots"><strong>Percentiles and Boxplots</strong></h2><p><strong><em>Quartiles</em></strong> (25th, 50th, and 75th percentiles) and the <strong><em>deciles</em></strong> (the 10th, 20th, ..., 90th percentiles). Percentiles are especially valuable for summarizing the <strong><em>tails</em></strong> (the outer range) of the distribution</p><center><img src="./1.EDA_1.png" width="500"></center><p><strong><em>Boxplots</em></strong>: based on percentiles and give a quick way to visualize the distribution of data.</p><center><img src="./1.EDA_2.png" width="300"></center><ul><li>From this boxplot we can immediately see that the median state population is about 5 million, half the states fall between about 2 million and about 7 million, and there are some high population outliers.</li><li>The top and bottom of the box are the 75th and 25th percentiles, respectively. The median is shown by the horizontal line in the box. The dashed lines, referred to as <em>whiskers</em>, extend from the top and bottom of the box to indicate the range for the bulk of the data.</li><li>Any data outside of the whiskers is plotted as single points or circles (often considered outliers).</li></ul><h2 id="frequency-tables-and-histograms">Frequency Tables and Histograms</h2><p><strong>Frequency table</strong> of a variable divides up the variable range into <em>equally spaced</em> segments and tells us how many values fall within each segment.</p><p>The function <code>pandas.cut</code> creates a series that maps the values into the segments. Using the method <code>value_counts</code>, we get the frequency table:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">binnedPopulation = pd.cut(state[<span class="string">'Population'</span>], <span class="number">10</span>)</span><br><span class="line">binnedPopulation.value_counts()</span><br></pre></td></tr></table></figure><p><strong>Frequency tables v.s. Percentiles</strong></p><p>Both frequency tables and percentiles summarize the data by creating bins. In general,</p><ul><li>quartiles and deciles will have the same count in each bin (equal-count bins), but the bin sizes will be different.</li><li>The frequency table will have different counts in the bins (equal-size bins), and the bin sizes will be the same.</li></ul><p><strong>Histogram</strong>: visualize a frequency table, with bins on the x- axis and the data count on the y-axis.</p><ul><li>Empty bins are included in the graph.</li><li>Bins are of equal width.</li><li>The number of bins (or, equivalently, bin size) is up to the user.</li><li>Bars are contiguous‚Äîno empty space shows between bars, unless there is an empty bin.</li></ul><center><img src="./1.EDA_3.png" width="300"></center><p><strong>STATISTICAL MOMENTS</strong></p><p>In statistical theory, <strong><em>location</em></strong> and <strong><em>variability</em></strong> are referred to as the <em>first</em> and <em>second</em> <strong><em>moments</em></strong> of a distribution. The <em>third</em> and <em>fourth</em> moments are called <strong><em>skewness</em></strong> and <strong><em>kurtosis</em></strong>.</p><ul><li><strong>Skewness</strong> refers to whether the data is skewed to larger or smaller values</li><li><strong>Kurtosis</strong> indicates the propensity of the data to have extreme values.</li></ul><p>Generally, metrics are not used to measure skewness and kurtosis; instead, these are discovered through visual displays.</p><h2 id="density-plots-and-estimates">Density Plots and Estimates</h2><p><strong>Density plot</strong> can be thought of as a smoothed <em>histogram</em>, although it is typically computed directly from the data through a <em>kernel density estimate</em>.</p><ul><li>A key distinction from the histogram plotted in Figure 1-3 is the <em>scale of the y-axis</em>: a density plot corresponds to plotting the histogram as a <strong><em>proportion</em></strong> rather than <strong><em>counts</em></strong>. Note that the total area under the density curve = 1, and instead of counts in bins you calculate areas under the curve between any two points on the x-axis, which correspond to the proportion of the distribution lying between those two points.</li></ul><center><img src="./1.EDA_4.png" width="300"></center><h1 id="exploring-binary-and-categorical-data">Exploring Binary and Categorical Data</h1><p><strong><em>Categorical data</em></strong> is typically summed up in proportions and can be visualized in a bar chart or pie chart.</p><ul><li>Categories might represent distinct things (apples and oranges, male and female), levels of a factor variable (low, medium, and high), or numeric data that has been binned.</li></ul><p><strong>Mode</strong>: the value‚Äîor values in case of a tie‚Äîthat appears most often in the data - simple summary statistic for categorical data.</p><p><strong>Expected Value</strong>: A special type of categorical data is data in which the categories represent or can be mapped to discrete values on the same scale. This data can be summed up in a single ‚Äúexpected value,‚Äù which is a form of <em>weighted mean</em>, in which the weights are probabilities.</p><p>The expected value is calculated as follows:</p><ol type="1"><li>Multiply each outcome by its probability of occurrence.</li><li>Sum these values.</li></ol><h1 id="correlation"><strong>Correlation</strong></h1><p><strong>Correlation coefficient</strong>: gives an estimate of the correlation between two variables that always lies on the same scale.</p><ul><li><strong><em>Pearson‚Äôs correlation coefficient</em></strong>: <span class="math inline">\(r=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{(n-1)s_xs_y}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum(x_i-\bar{x})^2}\sqrt{\sum(y_i-\bar{y})^2}} \in [-1,+1]\)</span></li><li>Variables can have an association that is <strong><em>nonlinear</em></strong>, in which case the correlation coefficient may not be a useful metric.</li></ul><p><strong>Correlation matrix</strong>: A table where the variables are shown on both rows and columns, and the cell values are the <strong>correlations</strong> between the variables.</p><center><img src="./1.EDA_5.png" width="500"></center><p><strong>Scatterplots</strong>: The standard way to visualize the relationship between two measured data variables. The x-axis represents one variable and the y-axis another, and each point on the graph is a record.</p><center><img src="./1.EDA_6.png" width="300"></center><h1 id="exploring-two-or-more-variables">Exploring Two or More Variables</h1><h2 id="two-numeric-variables">Two Numeric Variables</h2><p><strong>Hexagonal binning</strong>: A plot of two numeric variables with the records binned into hexagons.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ax = kc_tax0.plot.hexbin(x=<span class="string">'SqFtTotLiving'</span>,</span><br><span class="line">y=<span class="string">'TaxAssessedValue'</span>,</span><br><span class="line">                         gridsize=<span class="number">30</span>, sharex=<span class="literal">False</span>, figsize=</span><br><span class="line">(<span class="number">5</span>, <span class="number">4</span>))</span><br><span class="line">ax.set_xlabel(<span class="string">'Finished Square Feet'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Tax-Assessed Value'</span>)</span><br></pre></td></tr></table></figure><center><img src="./1.EDA_7.png" width="300"></center><p><strong>Contour plot</strong>: A plot showing the density of two numeric variables like a topographical map. The contours are essentially a topographical map to two variables; each contour band represents a specific density of points, increasing as one nears a ‚Äúpeak.‚Äù</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = sns.kdeplot(kc_tax0.SqFtTotLiving,</span><br><span class="line">kc_tax0.TaxAssessedValue, ax=ax)</span><br><span class="line">ax.set_xlabel(<span class="string">'Finished Square Feet'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Tax-Assessed Value'</span>)</span><br></pre></td></tr></table></figure><center><img src="./1.EDA_8.png" width="300"></center><h2 id="two-categorical-variables">Two Categorical Variables</h2><p><strong>Contingency table</strong>: A tally of counts between two or more categorical variables.</p><center><img src="./1.EDA_9.png" width="400"></center><h2 id="categorical-and-numeric-data">Categorical and Numeric Data</h2><p><strong>Boxplots</strong> : a simple way to visually compare the distributions of a numeric variable grouped according to a categorical variable.</p><center><img src="./1.EDA_10.png" width="300"></center><p><strong>Violin plot</strong>: Similar to a boxplot but plots the density estimate with the density on the y- axis.</p><ul><li>The advantage of a violin plot is that it can show nuances in the distribution that aren‚Äôt perceptible in a boxplot: the violin plot shows a concentration in the distribution near zero for Alaska and, to a lesser extent, Delta. This phenomenon is not as obvious in the boxplot.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ax = sns.violinplot(airline_stats.airline,</span><br><span class="line">airline_stats.pct_carrier_delay,</span><br><span class="line">                    inner=<span class="string">'quartile'</span>, color=<span class="string">'white'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Daily % of Delayed Flights'</span>)</span><br></pre></td></tr></table></figure><center><img src="./1.EDA_11.png" width="300"></center><h1 id="visualizing-multiple-variables">Visualizing Multiple Variables</h1><blockquote><p>The concept of <em>conditioning</em> variables (in this case, zip code)</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">zip_codes = [<span class="number">98188</span>, <span class="number">98105</span>, <span class="number">98108</span>, <span class="number">98126</span>]</span><br><span class="line">kc_tax_zip = kc_tax0.loc[kc_tax0.ZipCode.isin(zip_codes),:]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hexbin</span><span class="params">(x, y, color, **kwargs)</span>:</span></span><br><span class="line">cmap = sns.light_palette(color, as_cmap=<span class="literal">True</span>) </span><br><span class="line">  plt.hexbin(x, y, gridsize=<span class="number">25</span>, cmap=cmap, **kwargs)</span><br><span class="line"></span><br><span class="line">g = sns.FacetGrid(kc_tax_zip, col=<span class="string">'ZipCode'</span>, col_wrap=<span class="number">2</span>)</span><br><span class="line">g.map(hexbin, <span class="string">'SqFtTotLiving'</span>, <span class="string">'TaxAssessedValue'</span>,</span><br><span class="line">      extent=[<span class="number">0</span>, <span class="number">3500</span>, <span class="number">0</span>, <span class="number">700000</span>])</span><br><span class="line">g.set_axis_labels(<span class="string">'Finished Square Feet'</span>, <span class="string">'Tax-Assessed</span></span><br><span class="line"><span class="string">Value'</span>)</span><br><span class="line">g.set_titles(<span class="string">'Zip code &#123;col_name:.0f&#125;'</span>)</span><br></pre></td></tr></table></figure><center><img src="./1.EDA_12.png" width="300"></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;My study note of the book &lt;em&gt;Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python - Chapter 1 Exploratory Data Analysis&lt;/em&gt; .&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
      <category term="Practical Statistics for DS" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/Practical-Statistics-for-DS/"/>
    
    
      <category term="EDA" scheme="https://nancyyanyu.github.io/tags/EDA/"/>
    
      <category term="Statistics" scheme="https://nancyyanyu.github.io/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>October 2019 | ÂçÅÊúàÊó•Âøó</title>
    <link href="https://nancyyanyu.github.io/posts/781a431b/"/>
    <id>https://nancyyanyu.github.io/posts/781a431b/</id>
    <published>2019-11-10T20:47:45.000Z</published>
    <updated>2019-11-10T23:49:42.016Z</updated>
    
    <content type="html"><![CDATA[<p>¬†</p><a id="more"></a><p>¬†</p><center><b>‚ù§ 10ÊúàÊâãÂ∏≥ ‚ù§</b></center><table><thead><tr class="header"><th style="text-align: left;">Date</th><th>Study</th><th>Workout</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">10/06</td><td>TDS Podcast</td><td>20 mins cardio</td></tr><tr class="even"><td style="text-align: left;">10/07</td><td>TDS Podcast; read Yoav's Primer</td><td>20 mins cardioÔºå 10 mins strength</td></tr><tr class="odd"><td style="text-align: left;">10/08</td><td>TDS Podcast</td><td>20 mins cardio</td></tr><tr class="even"><td style="text-align: left;">10/09</td><td>/</td><td>20 mins cardio</td></tr><tr class="odd"><td style="text-align: left;">10/10</td><td>/</td><td>20 mins cardio</td></tr><tr class="even"><td style="text-align: left;">10/11</td><td>/</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">10/12</td><td>CS224N</td><td>/</td></tr><tr class="even"><td style="text-align: left;">10/13</td><td>CS224N</td><td>20 mins cardio, 30 mins strength</td></tr><tr class="odd"><td style="text-align: left;">10/14</td><td>/</td><td>/</td></tr><tr class="even"><td style="text-align: left;">10/15</td><td>/</td><td>10 mins cardio</td></tr><tr class="odd"><td style="text-align: left;">10/16</td><td>Study for Azure Data Scientist Associate</td><td>/</td></tr><tr class="even"><td style="text-align: left;">10/17</td><td>Study for Azure Data Scientist Associate</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">10/18</td><td>Study for Azure Data Scientist Associate</td><td>10 mins cardio</td></tr><tr class="even"><td style="text-align: left;">10/19</td><td>Study for Azure Data Scientist Associate</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">10/20</td><td>CS229 Linear</td><td>/</td></tr><tr class="even"><td style="text-align: left;">10/21</td><td>CS229 Logistic</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">10/22</td><td>CS229 GLM</td><td>/</td></tr><tr class="even"><td style="text-align: left;">10/23</td><td>CS229 Generative Learning</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">10/24</td><td>/</td><td>/</td></tr><tr class="even"><td style="text-align: left;">10/25</td><td>Study for Azure Data Scientist Associate</td><td>/</td></tr><tr class="odd"><td style="text-align: left;">10/26</td><td>/</td><td>/</td></tr><tr class="even"><td style="text-align: left;">10/27</td><td></td><td></td></tr></tbody></table><p>¬†</p><p>¬†</p><center><b>‚ù§ Plan ‚ù§</b></center><table><thead><tr class="header"><th style="text-align: center;">Categories</th><th style="text-align: center;">Content</th><th style="text-align: center;">Progress</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Machine Learning</td><td style="text-align: center;"><a href="http://cs229.stanford.edu/syllabus.html" target="_blank" rel="noopener">CS229: Machine Learning</a></td><td style="text-align: center;">‚ñ†‚ñ†‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°</td></tr><tr class="even"><td style="text-align: center;">NLP</td><td style="text-align: center;"><a href="https://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">CS224N: NLP with Deep Learning</a></td><td style="text-align: center;">‚ñ†‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°</td></tr><tr class="odd"><td style="text-align: center;">NLP</td><td style="text-align: center;">Yoav's Primer</td><td style="text-align: center;">‚ñ†‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°</td></tr><tr class="even"><td style="text-align: center;">Deep Learning</td><td style="text-align: center;">Replicate Boris' <a href="https://github.com/borisbanushev/stockpredictionai" target="_blank" rel="noopener">work</a></td><td style="text-align: center;">‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°</td></tr></tbody></table><p>¬†</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;¬†&lt;/p&gt;
    
    </summary>
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/categories/Journal/"/>
    
    
      <category term="Journal" scheme="https://nancyyanyu.github.io/tags/Journal/"/>
    
  </entry>
  
  <entry>
    <title>CS229 Note: Generative Learning</title>
    <link href="https://nancyyanyu.github.io/posts/5576e748/"/>
    <id>https://nancyyanyu.github.io/posts/5576e748/</id>
    <published>2019-10-23T03:50:34.000Z</published>
    <updated>2020-09-11T22:50:46.742Z</updated>
    
    <content type="html"><![CDATA[<h1 id="generative-learning-algorithms">Generative Learning algorithms</h1><p><strong>Discriminative learning algorithms</strong>: Algorithms that try to learn <span class="math inline">\(p(y|x)\)</span> directly (such as logistic regression), or algorithms that try to learn mappings directly from the space of inputs <span class="math inline">\(X\)</span> to the labels {0, 1}, (such as the perceptron algorithm)</p><a id="more"></a><p><strong>Generative learning algorithms</strong>: After modeling <span class="math inline">\(p(y)\)</span> (called the <strong>class priors</strong>) and <span class="math inline">\(p(x|y)\)</span>, our algorithm can then use <strong>Bayes rule</strong> to derive the <strong>posterior</strong> distribution on <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>: <span class="math display">\[p(y|x)=\frac{p(x|y)p(y)}{p(x)}\]</span> If were calculating <span class="math inline">\(p(y|x)\)</span> in order to make a prediction, then we don‚Äôt actually need to calculate the denominator, since <span class="math display">\[\arg{\max{_y}p(y|x)}=\arg{\max{_y}\frac{p(x|y)p(y)}{p(x)}}=\arg{\max{_y}p(x|y)p(y)}\]</span></p><h2 id="gaussian-discriminant-analysis">Gaussian discriminant analysis</h2><p><strong>Assumption</strong>: that <span class="math inline">\(p(x|y)\)</span> is distributed according to a <strong>multivariate normal</strong> distribution.</p><h3 id="the-multivariate-normal-distribution">The multivariate normal distribution</h3><p>The <strong>multivariate normal(Gaussian)</strong> distribution in d-dimensions is parameterized by a <strong>mean vector</strong> <span class="math inline">\(Œº ‚àà R^d\)</span> and a <strong>covariance matrix</strong> <span class="math inline">\(\sum‚àà R^{d\times d}\)</span>, where <span class="math inline">\(\sum‚â• 0\)</span> is <u>symmetric and positive semi-definite</u>. Also written ‚Äú<span class="math inline">\(\mathcal{N}(\mu, \Sigma)\)</span>‚Äù, its density is given by: <span class="math display">\[p(x; Œº,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\exp{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}\]</span> If <span class="math inline">\(X \sim \mathcal{N}(\mu , \Sigma)\)</span>, <span class="math display">\[E(X)=\int_{x}xp(x;\mu,\Sigma)dx=\mu\\\mathrm{Cov}(X)=\Sigma\]</span> <img src="./mn_1.png" width="600"></p><p>covariance matrix <span class="math inline">\(\Sigma= I\)</span>, <span class="math inline">\(\Sigma= 0.6I\)</span>, <span class="math inline">\(\Sigma= 2I\)</span></p><p><img src="./mn_1.png" width="600"> <span class="math display">\[\Sigma=\begin{bmatrix}1 &amp; 0 \\0 &amp; 1 \end{bmatrix} , \Sigma=\begin{bmatrix}1 &amp; 0.5 \\0.5 &amp; 1 \end{bmatrix} , \Sigma=\begin{bmatrix}1 &amp; 0.8 \\0.8 &amp; 1 \end{bmatrix} \]</span></p><h3 id="the-gaussian-discriminant-analysis-model">The Gaussian Discriminant Analysis model</h3><h4 id="section"></h4>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;generative-learning-algorithms&quot;&gt;Generative Learning algorithms&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Discriminative learning algorithms&lt;/strong&gt;: Algorithms that try to learn &lt;span class=&quot;math inline&quot;&gt;\(p(y|x)\)&lt;/span&gt; directly (such as logistic regression), or algorithms that try to learn mappings directly from the space of inputs &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; to the labels {0, 1}, (such as the perceptron algorithm)&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
      <category term="CS229" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/CS229/"/>
    
    
  </entry>
  
  <entry>
    <title>CS229 Note: Linear Regression, Logistic regression, Generalized Linear Models</title>
    <link href="https://nancyyanyu.github.io/posts/9b3e7e9e/"/>
    <id>https://nancyyanyu.github.io/posts/9b3e7e9e/</id>
    <published>2019-10-20T21:50:20.000Z</published>
    <updated>2019-10-23T06:07:40.457Z</updated>
    
    <content type="html"><![CDATA[<p><strong>linear function</strong>: <span class="math inline">\(h_Œ∏(x) = Œ∏_0 + Œ∏_1x_1 + Œ∏_2x_2+...+Œ∏_dx_d=Œ∏^Tx\)</span></p><p><strong>cost function</strong>: <span class="math inline">\(J(Œ∏)=\frac{1}{2}\sum_{i=1}^n(h_Œ∏(x^{(i)})-y^{(i)})^2\)</span></p><a id="more"></a><h1 id="linear-regression">Linear Regression</h1><h2 id="least-mean-square-algorithm">Least Mean Square Algorithm</h2><p>To minimizes <span class="math inline">\(J(Œ∏)\)</span>, consider the <strong><em>gradient descent</em></strong> algorithm, which starts with some initial <span class="math inline">\(Œ∏\)</span>, and repeatedly performs the update: <span class="math display">\[Œ∏_j:= Œ∏_j ‚àí Œ±\frac{‚àÇJ(Œ∏)}{‚àÇŒ∏_j}\]</span> (This update is simultaneously performed for all values of <span class="math inline">\(j = 0, . . . , d\)</span>.)</p><p>Gradient descent algorithm repeatedly takes a step in the direction of <em>steepest decrease</em> of <span class="math inline">\(J\)</span>.</p><p>Work out partial derivative term with only one training example (x, y): <span class="math display">\[\frac{‚àÇJ(Œ∏)}{‚àÇŒ∏_j}=\frac{‚àÇ}{‚àÇŒ∏_j}\frac{1}{2}\sum_{i=1}^n(h_Œ∏(x)-y)^2 \\=(h_Œ∏(x)-y)\frac{‚àÇ}{‚àÇŒ∏_j}(h_Œ∏(x)-y) \\=(h_Œ∏(x)-y)\frac{‚àÇ}{‚àÇŒ∏_j}(\sum_{i=1}^d Œ∏_ix_i-y) \\=(h_Œ∏(x)-y)x_j\]</span> For a single training example, this gives the update rule - <strong><em>LMS update rule</em></strong>: <span class="math display">\[Œ∏_j:= Œ∏_j + Œ±(y^{(i)}-h_Œ∏(x^{(i)}))x_j^{(i)}\]</span> <strong>Properties</strong>: the magnitude of the update is proportional to the error term <span class="math inline">\((y^{(i)}-h_Œ∏(x^{(i)}))\)</span></p><p><strong>Batch gradient descent</strong>: looks at every example in the entire training set on every step.</p><ul><li>susceptible to local minima</li></ul><p><span class="math display">\[Œ∏:= Œ∏ + Œ±\sum_{i=1}^n(y^{(i)}-h_Œ∏(x^{(i)}))x^{(i)}\]</span></p><p><strong>Stochastic gradient descent</strong>: repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only. <span class="math display">\[Œ∏:= Œ∏ + Œ±(y^{(i)}-h_Œ∏(x^{(i)}))x^{(i)}\]</span> <strong>Stochastic gradient descent V.S. Batch gradient descent</strong>:</p><p>Whereas <em>batch gradient descent</em> has to scan through the entire training set before taking a single step‚Äîa costly operation if <span class="math inline">\(n\)</span> is large‚Äî<em>stochastic gradient descent</em> can start making progress right away, and continues to make progress with each example it looks at.</p><h2 id="the-normal-equations">The normal equations</h2><h3 id="matrix-derivatives">Matrix derivatives</h3><p>For a function <span class="math inline">\(f : \real^{n\times d} ‚Üí \real\)</span> mapping from <span class="math inline">\(n\)</span>-by-<span class="math inline">\(d\)</span> matrices to the real numbers, we define the derivative of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(A\)</span> to be: <span class="math display">\[\triangledown _A f(A)=\begin{bmatrix}\frac{\partial f}{\partial A_{11}}&amp; ... &amp; \frac{\partial f}{\partial A_{1d}} \\... &amp; ... &amp; ... \\\frac{\partial f}{\partial A_{n1}} &amp;... &amp; \frac{\partial f}{\partial A_{nd}} \end{bmatrix}\]</span></p><h3 id="least-squares-revisited">Least squares revisited</h3><p>To find in closed-form the value of <span class="math inline">\(Œ∏\)</span> that minimizes <span class="math inline">\(J(Œ∏)\)</span>.</p><p>Given a training set, define the <strong>design matrix</strong> <span class="math inline">\(X\)</span> to be the <span class="math inline">\(n\)</span>-by-<span class="math inline">\(d\)</span> matrix (actually <span class="math inline">\(n\)</span>-by-<span class="math inline">\(d + 1\)</span>, if we include the intercept term) that contains the training examples‚Äô input values in its rows: <span class="math display">\[X=\begin{bmatrix}-(x^{(1)})^T- \\-(x^{(2)})^T-\\...\\-(x^{(n)})^T- \end{bmatrix}\]</span> Also, let <span class="math inline">\(\overrightarrow{y}\)</span> be the n-dimensional vector containing all the target values from the training set: <span class="math display">\[\overrightarrow{y}=\begin{bmatrix} y^{(1)} \\y^{(2)}\\...\\y^{(n)} \end{bmatrix}\]</span> Since <span class="math inline">\(h_Œ∏(x^{(i)}) = (x^{(i)})^TŒ∏\)</span>, we can easily verify that <span class="math display">\[XŒ∏-\overrightarrow{y} = \begin{bmatrix}(x^{(1)})^TŒ∏ \\(x^{(2)})^TŒ∏\\...\\(x^{(n)})^TŒ∏\end{bmatrix}-\begin{bmatrix} y^{(1)} \\y^{(2)}\\...\\y^{(n)} \end{bmatrix} \\ = \begin{bmatrix}h_Œ∏(x^{(1)})-y^{(1)} \\h_Œ∏(x^{(2)})-y^{(2)} \\...\\h_Œ∏(x^{(n)})-y^{(n)} \end{bmatrix}\]</span> For a vector <span class="math inline">\(z\)</span>, we have that <span class="math inline">\(z^Tz=\sum_i z_i^2\)</span> <span class="math display">\[\frac{1}{2}(XŒ∏-\overrightarrow{y})^T(XŒ∏-\overrightarrow{y})=\frac{1}{2}\sum_{i=1}^n (h_Œ∏(x^{(i)})-y^{(i)} )^2 =J(Œ∏)\]</span> To minimize <span class="math inline">\(J\)</span>, let‚Äôs find its derivatives with respect to <span class="math inline">\(Œ∏\)</span>. <span class="math display">\[\begin{align}\triangledown_Œ∏ J(Œ∏)&amp;=\triangledown_Œ∏\frac{1}{2}(XŒ∏-\overrightarrow{y})^T(XŒ∏-\overrightarrow{y}) \\&amp;=\frac{1}{2}\triangledown_Œ∏ (Œ∏^TX^TXŒ∏-Œ∏^TX^T\overrightarrow{y}-\overrightarrow{y}^TXŒ∏+\overrightarrow{y}^T\overrightarrow{y}) \\&amp;=\frac{1}{2}\triangledown_Œ∏ (Œ∏^T(X^TX)Œ∏-2Œ∏^TX^T\overrightarrow{y}) \\&amp;=\frac{1}{2}(2(X^TX)Œ∏-2X^T\overrightarrow{y}) \\&amp;=X^TXŒ∏-X^T\overrightarrow{y}\end{align}\]</span> Note: <span class="math display">\[a^T b = b^T a \\\triangledown_x b^Tx=b \\\triangledown_xx^TAx=2Ax \]</span> for symmetric matrix <span class="math inline">\(A\)</span>.</p><p>To minimize <span class="math inline">\(J\)</span>, we set its derivatives to zero, and obtain the normal equations: <span class="math display">\[Œ∏=(X^TX)^{-1}X^T\overrightarrow{y}\]</span></p><h2 id="probabilistic-interpretation">Probabilistic interpretation</h2><p><strong>Assumptions:</strong></p><ol type="1"><li>Assume that the target variables and the inputs are related via the equation:</li></ol><p><span class="math display">\[y^{(i)}=Œ∏^Tx^{(i)}+\epsilon^{(i)}\]</span></p><p>‚Äã where <span class="math inline">\(\epsilon^{(i)}\)</span> is an error term that captures either unmodeled effects or random noise.</p><ol start="2" type="1"><li><p>Assume $^{(i)} (0,œÉ^2) $ ;</p></li><li><p>Assume <span class="math inline">\(\epsilon^{(i)}\)</span> are IID.</p></li></ol><p>Thus, the density of <span class="math inline">\(\epsilon^{(i)}\)</span> is given by: <span class="math display">\[p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})\]</span> This implies that <span class="math display">\[p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{(y^{(i)}-Œ∏^Tx^{(i)})^2}{2\sigma^2})\]</span> The notation ‚Äú<span class="math inline">\(p(y^{(i)}|x^{(i)};\theta)\)</span>‚Äù indicates that this is the distribution of <span class="math inline">\(y^{(i)}\)</span> given <span class="math inline">\(x^{(i)}\)</span> and parameterized by <span class="math inline">\(Œ∏\)</span>.</p><p>When we wish to explicitly view <span class="math inline">\(p(\overrightarrow{y}|X;\theta)\)</span> as a function of <span class="math inline">\(Œ∏\)</span>, we will instead call it the <strong>likelihood</strong> function: <span class="math display">\[L(\theta)=L(\theta;X,\overrightarrow{y})=p(\overrightarrow{y}|X;\theta)\]</span> Note that by the independence assumption on the <span class="math inline">\(\epsilon^{(i)}\)</span> (and hence also the <span class="math inline">\(y^{(i)}\)</span>'s given the <span class="math inline">\(x^{(i)}\)</span>‚Äôs), this can also be written: <span class="math display">\[\begin{align}L(\theta)&amp;=\prod_{i=1}^np(y^{(i)}|x^{(i)};\theta) \\&amp;=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{(y^{(i)}-Œ∏^Tx^{(i)})^2}{2\sigma^2})\end{align}\]</span> <u>What is a reasonable way of choosing our best guess of the parameters <span class="math inline">\(Œ∏\)</span>?</u></p><p>The principal of <strong>maximum likelihood</strong> says that we should choose <span class="math inline">\(Œ∏\)</span> so as to make the data as high probability as possible. I.e., we should choose <span class="math inline">\(Œ∏\)</span> to maximize <span class="math inline">\(L(Œ∏)\)</span>, or <strong>log likelihood</strong> <span class="math inline">\(‚Ñì(Œ∏)\)</span> : <span class="math display">\[\begin{align}‚Ñì(Œ∏)&amp;=\log{L(\theta)} \\&amp;=\log{\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{(y^{(i)}-Œ∏^Tx^{(i)})^2}{2\sigma^2})} \\&amp;=\sum_{i=1}^n \log{\frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{(y^{(i)}-Œ∏^Tx^{(i)})^2}{2\sigma^2})} \\&amp;=n\log{\frac{1}{\sqrt{2\pi}}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^n(y^{(i)}-Œ∏^Tx^{(i)})^2\end{align}\]</span> Hence, maximizing <span class="math inline">\(‚Ñì(Œ∏)\)</span> gives the same answer as minimizing: <span class="math display">\[\frac{1}{2}\sum_{i=1}^n(y^{(i)}-Œ∏^Tx^{(i)})^2\]</span> which we recognize to be <span class="math inline">\(J(Œ∏)\)</span>, our original least-squares cost function.</p><p><strong>To summarize:</strong> Under the previous probabilistic assumptions on the data, <em>least-squares regression</em> corresponds to finding the <em>maximum likelihood</em> estimate of <span class="math inline">\(Œ∏\)</span>. This is thus one set of assumptions under which least-squares regression can be justified as a very natural method that‚Äôs just doing maximum likelihood estimation.</p><p>Note also that, our final choice of <span class="math inline">\(Œ∏\)</span> did not depend on what was <span class="math inline">\(œÉ^2\)</span>.</p><h2 id="locally-weighted-linear-regression">Locally weighted linear regression</h2><p><strong>Locally weighted linear regression (LWR)</strong> algorithm assumes there is sufficient training data, makes the choice of features less critical.</p><p>In the <em>original linear regression algorithm</em>, to make a prediction at a query point x (i.e., to evaluate h(x)), we would:</p><ol type="1"><li>Fit <span class="math inline">\(Œ∏\)</span> to minimize <span class="math inline">\(\sum_{i=1}^n(y^{(i)}-Œ∏^Tx^{(i)})^2\)</span>.</li><li>Output <span class="math inline">\(Œ∏^T x\)</span>.</li></ol><p>In the <em>locally weighted linear regression</em> algorithm:</p><ol type="1"><li>Fit <span class="math inline">\(Œ∏\)</span> to minimize <span class="math inline">\(\sum_{i=1}^nw^{(i)}(y^{(i)}-Œ∏^Tx^{(i)})^2\)</span>.</li><li>Output <span class="math inline">\(Œ∏^T x\)</span>.</li></ol><p>A fairly standard choice for the weights <span class="math inline">\(w^{(i)}\)</span> is: <span class="math display">\[w^{(i)}=\exp{-\frac{(x^{(i)}-x)^2}{2\tau^2}}\]</span> Note that the weights depend on the particular point x at which we‚Äôre trying to evaluate x. Moreover, if <span class="math inline">\(|x^{(i)} ‚àí x|\)</span> is small, then <span class="math inline">\(w^{(i)}\)</span> is close to 1; and if <span class="math inline">\(|x^{(i)} ‚àí x|\)</span> is large, then <span class="math inline">\(w^{(i)}\)</span> is small. Hence, <span class="math inline">\(Œ∏\)</span> is chosen giving a much higher ‚Äúweight‚Äù to the (errors on) training examples close to the query point <span class="math inline">\(x\)</span>.</p><p><span class="math inline">\(œÑ\)</span> is called the <strong>bandwidth</strong> parameter, which controls how quickly the weight of a training example falls off with distance of its <span class="math inline">\(x^{(i)}\)</span> from the query point <span class="math inline">\(x\)</span>.</p><h3 id="non-parametric-v.s.-parametric"><strong>Non-parametric</strong> V.S. Parametric</h3><p><strong>Non-parametric algorithm</strong>: Locally weighted linear regression</p><ul><li>The amount of stuff we need to keep in order to represent the hypothesis <span class="math inline">\(h\)</span> grows linearly with the size of the training set.</li></ul><p><strong>Parametric algorithm</strong>: linear regression algorithm</p><ul><li>A fixed, finite number of parameters (the <span class="math inline">\(Œ∏_i\)</span>‚Äôs)</li></ul><h1 id="logistic-regression">Logistic regression</h1><p>Change the form for our <strong>hypotheses <span class="math inline">\(h(x)\)</span>:</strong> <span class="math display">\[h_\theta(x)=g(\theta^Tx)=\frac{1}{1+\exp{(-\theta^Tx)}}\]</span> <strong>Logistic function(sigmoid function):</strong> <span class="math display">\[g(z)=\frac{1}{1+\exp{(-z)}}\]</span></p><p><img src="./sigmoid.png" width="600"></p><p><span class="math display">\[\begin{align}g^{&#39;}(z)&amp;=\frac{d}{dz}\frac{1}{1+\exp{(-z)}} \\&amp;=\frac{1}{(1+\exp{(-z)})^2} \exp{(-z)} \\&amp;=\frac{1}{1+\exp{(-z)}} \cdot (1-\frac{1}{1+\exp{(-z)}}) \\&amp;=g(z) \cdot (1-g(z))\end{align}\]</span></p><h2 id="how-do-we-fit-Œ∏-for-it">How do we fit <span class="math inline">\(Œ∏\)</span> for it?**</h2><p><strong>Probabilistic assumptions</strong>: <span class="math display">\[P(y=1|x;\theta)=h_\theta(x) \\P(y=0|x;\theta)=1-h_\theta(x)\]</span> Note that this can be written more compactly as <span class="math display">\[p(y|x;\theta)=(h_\theta(x) )^y(1-h_\theta(x) )^{1-y}\]</span> Assuming that the <span class="math inline">\(n\)</span> training examples were generated independently, we can then write down the likelihood of the parameters as <span class="math display">\[L(\theta)=p(\overrightarrow{y}|X;\theta)=\prod_{i=1}^np(y^{(i)}|x^{(i)};\theta) \\=\prod_{i=1}^n(h_\theta(x^{(i)}) )^{y^{(i)}}(1-h_\theta(x^{(i)}) )^{1-y^{(i)}}\]</span> As before, it will be easier to maximize the log likelihood: <span class="math display">\[‚Ñì(Œ∏) = \log{L(Œ∏)}=\sum_{i=1}^ny^{(i)}\log{h_\theta(x^{(i)})}+(1-y^{(i)})\log{(1-h_\theta(x^{(i)}) )}\]</span> Use gradient ascent to maximize the likelihood: <span class="math inline">\(Œ∏ := Œ∏ + Œ±‚àá‚Ñì(Œ∏)\)</span></p><p>Let‚Äôs start by working with just one training example <span class="math inline">\((x, y)\)</span>: <span class="math display">\[\begin{align}\frac{\partial ‚Ñì(Œ∏)}{\partial Œ∏_j}&amp;=(y\frac{1}{h_\theta(x)}-(1-y)\frac{1}{1-h_\theta(x)})\frac{\partial h_\theta(x)}{\partial Œ∏_j} \\&amp;=(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)})\frac{\partial g(\theta^Tx)}{\partial Œ∏_j} \\&amp;=(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}) g(\theta^Tx)(1- g(\theta^Tx))\frac{\partial Œ∏^Tx}{\partial Œ∏_j} \\&amp;=(y(1- g(\theta^Tx))-(1-y)g(\theta^Tx))x_j \\&amp;=(y-g(\theta^Tx))x_j \\&amp;=(y-h_\theta(x))x_j\end{align}\]</span> <strong>Stochastic gradient ascent rule</strong>: <span class="math display">\[Œ∏_j := Œ∏_j + Œ±(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}_j\]</span> It looks identical to the LMS update rule; but this is not the same algorithm, because <span class="math inline">\(h_\theta(x^{(i)})\)</span> is now defined as a non-linear function of <span class="math inline">\(\theta^Tx^{(i)}\)</span>.</p><h2 id="digression-the-perceptron-learning-algorithm">Digression: The perceptron learning algorithm</h2><p>Consider modifying the logistic regression method to ‚Äúforce‚Äù it to output values that are either 0 or 1 or exactlyÔºö <span class="math display">\[g(z)=\begin{cases}1 &amp; z \geq 0\\0 &amp; x &lt; 0\end{cases}\]</span> Let <span class="math inline">\(h(x) = g(Œ∏^T x)\)</span> as before but using this modified definition of <span class="math inline">\(g\)</span>, and if we use the update rule <span class="math display">\[Œ∏_j := Œ∏_j + Œ±(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}_j\]</span> then we have the <strong>perceptron learning algorithn</strong>.</p><p>Note:</p><ul><li>Even though the perceptron may be cosmetically similar to the other algorithms we talked about, it is actually a very different type of algorithm than logistic regression and least squares linear regression;</li><li>Difficult to endow the perceptron‚Äôs predictions with meaningful probabilistic interpretations, or derive the perceptron as a maximum likelihood estimation algorithm.</li></ul><h2 id="fisher-scoring-algorithm-for-maximizing-‚ÑìŒ∏">Fisher scoring algorithm for maximizing ‚Ñì(Œ∏)</h2><p><strong>Newton‚Äôs method</strong> (finding a zero of a function) performs the following update: <span class="math display">\[\theta:=\theta-\frac{f(\theta)}{f^{&#39;}(\theta)}\]</span> <strong>Interpretation:</strong> Approximating the function <span class="math inline">\(f\)</span> via a linear function that is tangent to <span class="math inline">\(f\)</span> at the current guess <span class="math inline">\(Œ∏\)</span>, solving for where that linear function equals to zero, and letting the next guess for <span class="math inline">\(Œ∏\)</span> be where that linear function is zero.</p><p><img src="./newton.png" width="600"></p><p>The maxima of <span class="math inline">\(‚Ñì\)</span> correspond to points where its first derivative <span class="math inline">\(‚Ñì‚Ä≤(Œ∏)\)</span> is zero. So, by letting <span class="math inline">\(f(Œ∏) = ‚Ñì‚Ä≤(Œ∏)\)</span>, we can use the same algorithm to maximize <span class="math inline">\(‚Ñì\)</span>, and we obtain update rule: <span class="math display">\[\theta:=\theta-\frac{‚Ñì‚Ä≤(Œ∏)}{‚Ñì‚Ä≤‚Ä≤(Œ∏)}\]</span> <u>The generalization of Newton‚Äôs method:</u> <span class="math display">\[\theta:=\theta-H^{-1}‚àá_Œ∏‚Ñì(Œ∏)\]</span> <span class="math inline">\(H\)</span> is <strong>Hessian</strong> matrix, whose entries are given by <span class="math inline">\(H_{ij}=\frac{\partial^2 ‚Ñì(Œ∏)}{\partial Œ∏_i \partial Œ∏_j}\)</span></p><ul><li>Faster convergence than (batch) gradient descent, and requires many fewer iterations to get very close to the minimum.</li><li>One iteration of Newton‚Äôs can, however, be more expensive than one iteration of gradient descent, since it requires finding and inverting an d-by-d Hessian</li></ul><p>When <strong><em>Newton‚Äôs method</em></strong> is applied to maximize the <strong><em>logistic regression log likelihood function ‚Ñì(Œ∏)</em></strong>, the resulting method is also called <strong>Fisher scoring</strong>.</p><h1 id="generalized-linear-models">Generalized Linear Models</h1><h2 id="the-exponential-family">The exponential family</h2><p>We say that a class of distributions is in the exponential family if it can be written in the form <span class="math display">\[p(y;Œ∑) = b(y) \exp{(Œ∑^TT(y) ‚àí a(Œ∑))}\]</span></p><ul><li><span class="math inline">\(Œ∑\)</span> : <strong>natural parameter</strong>/ <strong>canonical parameter</strong> of the distribution</li><li><span class="math inline">\(T(y)\)</span>: <strong>sufficient statistic</strong> Ôºàoften be the case that <span class="math inline">\(T(y) = y\)</span>Ôºâ</li><li><span class="math inline">\(a(Œ∑)\)</span>: <strong>log partition function</strong></li><li><span class="math inline">\(e^{‚àía(Œ∑)}\)</span>: plays the role of a normalization constant, that makes sure the distribution <span class="math inline">\(p(y; Œ∑)\)</span> sums/integrates over <span class="math inline">\(y\)</span> to 1.</li></ul><p>A fixed choice of <span class="math inline">\(T, a\)</span> and <span class="math inline">\(b\)</span> defines a family (or set) of distributions that is parameterized by <span class="math inline">\(Œ∑\)</span>; as we vary <span class="math inline">\(Œ∑\)</span>, we then get different distributions within this family.</p><h3 id="bernoulliphi">Bernoulli(<span class="math inline">\(\phi\)</span>)</h3><p>Bernoulli distribution: <span class="math display">\[p(y = 1; \phi) = \phi; p(y = 0; \phi) = 1 ‚àí \phi.\]</span> We write the Bernoulli distribution as: <span class="math display">\[\begin{align}p(y;\phi)&amp;=\phi^y(1-\phi)^{1-y} \\&amp;=\exp{(y\log{\phi})+(1-y)\log{(1-\phi)}}  \\&amp;=\exp((\log{(\frac{\phi}{1-\phi}}))y+\log{(1-\phi)})\end{align}\]</span></p><ul><li><strong>natural parameter <span class="math inline">\(Œ∑\)</span></strong>: <span class="math inline">\(\log{(\frac{\phi}{1-\phi}})\)</span></li><li><strong>sufficient statistic <span class="math inline">\(T(y)\)</span></strong>: <span class="math inline">\(y\)</span></li><li><strong>log partition function <span class="math inline">\(a(Œ∑)\)</span></strong>: <span class="math inline">\(-\log{(1-\phi)}=\log{1+e^{Œ∑}}\)</span></li></ul><h3 id="gaussian-distribution-mathcalnmusigma2">Gaussian distribution <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span></h3><p>Recall that, when deriving linear regression, the value of <span class="math inline">\(œÉ^2\)</span> had no effect on our final choice of <span class="math inline">\(Œ∏\)</span> and <span class="math inline">\(h(x)\)</span>. To simplify the derivation below, let‚Äôs set <span class="math inline">\(œÉ^2 = 1.\)</span> We then have: <span class="math display">\[p(y;\mu)=\frac{1}{\sqrt{2\pi}}\exp{(-\frac{1}{2}(y-\mu)^2)} \\=\frac{1}{\sqrt{2\pi}}\exp{(-\frac{1}{2}y^2)}\exp{(y\mu-\frac{1}{2}\mu^2)}\]</span> Thus Gaussian is in the exponential family, with:</p><ul><li><span class="math inline">\(Œ∑ = Œº\)</span></li><li><span class="math inline">\(T=1\)</span></li><li><span class="math inline">\(b(y)=\frac{1}{\sqrt{2\pi}}\exp{(-\frac{1}{2}y^2)}\)</span></li><li><span class="math inline">\(a(Œ∑)=\frac{1}{2}\mu^2=\frac{1}{2}Œ∑^2\)</span></li><li><span class="math inline">\(T(y)=y\)</span></li></ul><h3 id="multinomial-distribution">Multinomial distribution</h3><p>For <em>n</em> independent trials each of which leads to a success for exactly one of <em>k</em> categories, with each category having a given fixed success probability, the <strong>multinomial distribution</strong> gives the probability of any particular combination of numbers of successes for the various categories.</p><p>When <em>k</em> is 2 and <em>n</em> is 1, the multinomial distribution is the <strong>Bernoulli distribution</strong>. When <em>k</em> is 2 and <em>n</em> is bigger than 1, it is the <strong>binomial distribution</strong>. When k is bigger than 2 and <em>n</em> is 1, it is the <strong>Categorical distribution</strong>.</p><p>Mathematically, we have <em>k</em> possible mutually exclusive outcomes, with corresponding probabilities <span class="math inline">\(\phi_1, ..., \phi_k\)</span><em>, and </em>n* independent trials. Since the <em>k</em> outcomes are mutually exclusive and one must occur we have <span class="math inline">\(\phi_i \geq 0\)</span> for $¬†¬† i = 1, ..., k$ and <span class="math inline">\(\sum_{i=1}^k \phi_i = 1\)</span>.</p><p>To parameterize a <strong>multinomial</strong> over <span class="math inline">\(k\)</span> possible outcomes, one could use <span class="math inline">\(k\)</span> parameters <span class="math inline">\(\phi_1, . . . , \phi_{k-1}\)</span>, as they must satisfy <span class="math inline">\(\sum_{i=1}^k \phi_i = 1)\)</span>, we can let <span class="math inline">\(\phi_{k}=1-\sum_{i=1}^{k-1} \phi_i\)</span>.</p><p>To express the multinomial as an <strong>exponential family distribution</strong>, we will define <span class="math inline">\(T(y) ‚àà R^{k‚àí1}\)</span> as follows: <span class="math display">\[T(1)=\begin{bmatrix}1\\0\\0\\...\\0\end{bmatrix} , T(2)=\begin{bmatrix}0\\1\\0\\...\\0\end{bmatrix} , T(3)=\begin{bmatrix}0\\0\\1\\...\\0\end{bmatrix} ,T(k-1)=\begin{bmatrix}0\\0\\0\\...\\1\end{bmatrix} ,T(k)=\begin{bmatrix}0\\0\\0\\...\\0\end{bmatrix}\]</span> Unlike our previous examples, here we do not have <span class="math inline">\(T(y) = y\)</span>; also, <span class="math inline">\(T(y)\)</span> is now a <span class="math inline">\(k ‚àí 1\)</span> dimensional vector.</p><p>We will write <span class="math inline">\((T(y))_i\)</span> to denote the i-th element of the vector <span class="math inline">\(T(y)\)</span>. We can also write the relationship between <span class="math inline">\(T(y)\)</span> and <span class="math inline">\(y\)</span> as <span class="math inline">\((T(y))_i = 1\{y = i\}\)</span>. Further, we have that <span class="math inline">\(E[(T(y))_i] = P(y = i) = \phi_i\)</span>. <span class="math display">\[\begin{align}p(y; \phi) &amp;=  \phi_1^{1\{y=1\}} \phi_2^{1\{y=2\}} ... \phi_k^{1\{y=k\}} \\&amp;=  \phi_1^{1\{y=1\}} \phi_2^{1\{y=2\}} ... \phi_k^{1-\sum_{i=1}^{k-1}1\{y=i\}} \\&amp;= \phi_1^{(T(y))_1} \phi_2^{(T(y))_2} ... \phi_k^{1-\sum_{i=1}^{k-1}(T(y))_i} \\&amp;= \exp{\left[(T(y))_1 \log{\phi_1}+(T(y))_2 \log{\phi_2}+...+(1-\sum_{i=1}^{k-1}(T(y))_i) \log{\phi_k}\right]} \\&amp;=\exp{\left[(T(y))_1 \log{\frac{\phi_1}{\phi_k}}+ (T(y))_2 \log{\frac{\phi_2}{\phi_k}} +...+(T(y))_{k-1} \log{\frac{\phi_{k-1}}{\phi_k}}+\log{\phi_k} \right]} \\&amp;=b(y) \exp{(Œ∑^TT(y) ‚àí a(Œ∑))}\end{align}\]</span> where <span class="math display">\[b(y)=1 \\a(Œ∑)=-\log{\phi_k} \\Œ∑=\begin{bmatrix}\log{\frac{\phi_1}{\phi_k}} \\ ... \\ \log{\frac{\phi_{k-1}}{\phi_k}} \\\log{\frac{\phi_1}{\phi_k}}\end{bmatrix}\]</span> To invert the link function and derive the response function, we therefore have that <span class="math display">\[e^{Œ∑_i}=\frac{\phi_i}{\phi_k}  \\\phi_ke^{Œ∑_i}=\phi_i \\\phi_k \sum_{i=1}^ke^{Œ∑_i}=\sum_{i=1}^k\phi_i=1 \\\]</span> This implies that <span class="math display">\[\phi_k =\frac{1}{\sum_{i=1}^ke^{Œ∑_i}}\]</span> Which means: <span class="math display">\[\phi_i =\frac{e^{Œ∑_i}}{\sum_{i=1}^ke^{Œ∑_i}}\]</span> This function mapping from the <span class="math inline">\(Œ∑\)</span>‚Äôs to the <span class="math inline">\(\phi\)</span> ‚Äôs is called the <strong>softmax function</strong>.</p><h3 id="other-members-of-the-exponential-family">Other Members of the Exponential Family</h3><ul><li>Multinomial</li><li>Poisson</li><li>Gamma and Exponential (for modelling continuous, non-negative random variables, such as time intervals);</li><li>Beta and the Dirichlet (for distributions over probabilities);</li></ul><h2 id="constructing-glms">Constructing GLMs</h2><p>Consider a classification or regression problem where we would like to predict the value of some random variable <span class="math inline">\(y\)</span> as a function of <span class="math inline">\(x\)</span>. To derive a GLM for this problem, we will make the following three assumptions about the conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> and about our model:</p><ol type="1"><li><span class="math inline">\(y | x; Œ∏ ‚àº ExponentialFamily(Œ∑)\)</span>. I.e., given <span class="math inline">\(x\)</span> and <span class="math inline">\(Œ∏\)</span>, the distribution of <span class="math inline">\(y\)</span> follows some exponential family distribution, with parameter <span class="math inline">\(Œ∑\)</span>.</li><li>Given <span class="math inline">\(x\)</span>, our goal is to predict the expected value of <span class="math inline">\(T(y)\)</span> given <span class="math inline">\(x\)</span>. In most of our examples, we will have <span class="math inline">\(T(y) = y\)</span>, so this means <span class="math inline">\(w\)</span> would like the prediction <span class="math inline">\(h(x)\)</span> output by our learned hypothesis <span class="math inline">\(h\)</span> to satisfy <span class="math inline">\(h(x) = E[y|x]\)</span>. For instance, in logistic regression, we had <span class="math inline">\(h(x) = p(y = 1|x; Œ∏) = 0 \cdot p(y = 0|x; Œ∏) + 1 \cdot p(y = 1|x; Œ∏) = E[y|x; Œ∏].)\)</span></li><li>The natural parameter <span class="math inline">\(Œ∑\)</span> and the inputs x are related linearly: <span class="math inline">\(Œ∑ = Œ∏^T x\)</span>. (Or, if <span class="math inline">\(Œ∑\)</span> is vector-valued, then <span class="math inline">\(Œ∑_i = Œ∏^T_i x\)</span>.)</li></ol><h3 id="ordinary-least-squares">Ordinary Least Squares</h3><p><u>Ordinary least squares is a special case of the GLM family of models</u></p><p>We let the <span class="math inline">\(ExponentialFamily(Œ∑)\)</span> distribution above be the <strong>Gaussian distribution</strong>. In the formulation of the Gaussian as an exponential family distribution, we had <span class="math inline">\(Œº = Œ∑\)</span>. <span class="math display">\[h_\theta(x)=E[y|x;\theta] \\=\mu \\=Œ∑ \\=\theta^Tx\]</span> The first equality follows from <em>Assumption 2</em>, above; the second equality follows from the fact that <span class="math inline">\(y|x; Œ∏ ‚àº \mathcal{N}(Œº, œÉ^2)\)</span>, and so its expected value is given by <span class="math inline">\(Œº\)</span>; the third equality follows from <em>Assumption 1</em> (and our earlier derivation showing that Œº = Œ∑ in the formulation of the Gaussian as an exponential family distribution); and the last equality follows from Assumption 3.</p><h3 id="logistic-regression-1">Logistic Regression</h3><p>Given that <span class="math inline">\(y\)</span> is binary-valued, it therefore seems natural to choose the <strong>Bernoulli</strong> family of distributions to model the conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>.</p><ul><li>If <span class="math inline">\(y|x; Œ∏ ‚àº Bernoulli(œÜ)\)</span>, then <span class="math inline">\(E[y|x; Œ∏] = œÜ\)</span>.</li><li>In our formulation of the Bernoulli distribution as an exponential family distribution, we had <span class="math inline">\(œÜ = 1/(1 + e^{‚àíŒ∑})\)</span></li></ul><p><span class="math display">\[h_\theta(x)=E[y|x;\theta] \\=œÜ \\=1/(1 + e^{‚àíŒ∑}) \\=1/(1 + e^{‚àí\theta^Tx})\]</span></p><p><strong>canonical response function</strong>: the function <span class="math inline">\(g\)</span> giving the distribution‚Äôs mean as a function of the natural parameter <span class="math inline">\(g(Œ∑) = E[T(y); Œ∑]\)</span></p><p><strong>canonical link function</strong>: g's inverse <span class="math inline">\(g^{-1}\)</span></p><p>Thus, the <u>canonical response function</u> for the Gaussian family is just the <strong>identify function</strong>; and the <u>canonical response function</u> for the <strong>Bernoulli</strong> is the <strong>logistic function</strong></p><h3 id="softmax-regression">Softmax Regression</h3><p>Consider a classification problem in which the response variable <span class="math inline">\(y\)</span> can take on any one of <span class="math inline">\(k\)</span> values, so <span class="math inline">\(y ‚àà\{1, 2, . . . , k\}\)</span>.</p><p>We will thus model it as distributed according to a multinomial distribution.</p><p>Remember the <strong>softmax function</strong>: <span class="math display">\[\phi_i =\frac{e^{Œ∑_i}}{\sum_{i=1}^ke^{Œ∑_i}}\]</span> Assumption 3 <span class="math inline">\(Œ∑_i\)</span>‚Äôs are linearly related to the <span class="math inline">\(x\)</span>‚Äôs: <span class="math inline">\(Œ∑_i=\theta^Tx_i\)</span>, (for <span class="math inline">\(i = 1, . . . , k ‚àí 1\)</span>)</p><p>For notational convenience, we can also define <span class="math inline">\(\theta_k=0\)</span>, so that <span class="math inline">\(Œ∑_k = Œ∏^T_k x = 0\)</span></p><p>Hence, our model assumes that the conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> is given by <span class="math display">\[p(y = i|x; Œ∏) = \phi_i \\=\frac{e^{Œ∑_i}}{\sum_{j=1}^ke^{Œ∑_j}} \\=\frac{e^{Œ∏^T_i x}}{\sum_{j=1}^ke^{Œ∏^T_j x}}\]</span> This model, which applies to classification problems where <span class="math inline">\(y ‚àà \{1, . . . , k\}\)</span>, is called <strong>softmax regression</strong>.It is a generalization of logistic regression.</p><p>Our hypothesis will output <span class="math display">\[h_\theta(x) = E[T(y)|x; Œ∏]\\=\begin{bmatrix}1\{y=1\} \\1\{y=2\} \\...\\1\{y=k\}  \end{bmatrix}|x;\theta \\=\begin{bmatrix}\phi_1 \\\phi_2 \\...\\\phi_{k-1} \end{bmatrix}  \\=\begin{bmatrix}\frac{e^{Œ∏^T_1 x}}{\sum_{j=1}^ke^{Œ∏^T_j x}}\\ \frac{e^{Œ∏^T_2 x}}{\sum_{j=1}^ke^{Œ∏^T_j x}} \\...\\\frac{e^{Œ∏^T_{k-1} x}}{\sum_{j=1}^ke^{Œ∏^T_j x}}\end{bmatrix}\]</span> <strong>Parameter fitting:</strong></p><p>Training data:<span class="math inline">\({(x^{(i)}, y^{(i)}); i = 1, . . . , n}\)</span> . Log-likelihood: <span class="math display">\[‚Ñì(Œ∏) = \log{L(Œ∏)} \\=\sum_{i=1}^n \log{p(y^{(i)}|x^{(i)};\theta)} \\=\sum_{i=1}^n \log{\prod_{l=1}^k(\frac{e^{Œ∏^T_l x^{(i)}}}{\sum_{j=1}^ke^{Œ∏^T_j x^{(i)}}})^{1\{y^{(i)}=l\}}}\]</span> Maximize <span class="math inline">\(‚Ñì(Œ∏)\)</span> in terms of <span class="math inline">\(Œ∏\)</span>, using a method such as <strong>gradient ascent</strong> or <strong>Newton‚Äôs method</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;linear function&lt;/strong&gt;: &lt;span class=&quot;math inline&quot;&gt;\(h_Œ∏(x) = Œ∏_0 + Œ∏_1x_1 + Œ∏_2x_2+...+Œ∏_dx_d=Œ∏^Tx\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;cost function&lt;/strong&gt;: &lt;span class=&quot;math inline&quot;&gt;\(J(Œ∏)=\frac{1}{2}\sum_{i=1}^n(h_Œ∏(x^{(i)})-y^{(i)})^2\)&lt;/span&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
      <category term="CS229" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/CS229/"/>
    
    
      <category term="Linear Regression" scheme="https://nancyyanyu.github.io/tags/Linear-Regression/"/>
    
      <category term="Logistic Regression" scheme="https://nancyyanyu.github.io/tags/Logistic-Regression/"/>
    
      <category term="Regression" scheme="https://nancyyanyu.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>Realtime Financial Market Data Visualization and Analysis</title>
    <link href="https://nancyyanyu.github.io/posts/109fc1d1/"/>
    <id>https://nancyyanyu.github.io/posts/109fc1d1/</id>
    <published>2019-10-20T01:51:45.595Z</published>
    <updated>2019-10-20T01:56:49.312Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>In this project, I developed a financial data processing and visualization platform using <strong><u>Apache Kafka</u></strong>, <u><strong>Apache Cassandra</strong></u>, and <strong><u>Bokeh</u></strong>. I used Kafka for realtime stock price and market news streaming, Cassandra for historical and realtime stock data warehousing, and Bokeh for visualization on web browsers. I also wrote a web crawler to scrape companys' financial statements and basic information from Yahoo Finance, and played with various economy data APIs.</p><p><em>Please check the <a href="https://github.com/nancyyanyu/kafka_stock" target="_blank" rel="noopener">GitHub repo</a> of this project, and <strong>most importantly, please check this platform's website and play with each plot</strong> <span class="math inline">\(\rightarrow\)</span> <a href="http://magiconch.me/" target="_blank" rel="noopener">magiconch.me</a></em></p><a id="more"></a><p>¬†</p><h1 id="architecture">Architecture</h1><p>There are currently 3 tabs in the webpage:</p><ul><li><strong><em>Stock: Streaming &amp; Fundamental</em></strong><ul><li>Single stock's candlestick, basic company &amp; financial information;</li><li>Realtime S&amp;P500 price during trading hours (<em>fake date</em> during non-trading hours)</li></ul></li><li><strong><em>Stock: Comparison</em></strong><ul><li>2 user-selected stocks' price, and their statstical summay &amp; correlation</li><li>5,10,30-day moving average of adjusted close price</li></ul></li><li><strong><em>Economy</em></strong><ul><li>Geomap of various economy data by state</li><li>Economy indicators's plot</li><li>Latest market news</li></ul></li></ul><p>¬†</p><p>Here is the architecture of the platform.</p><p><img src="./kafka_stock.png" width="900"></p><p>Please check each tab's screenshot:</p><p><strong>Tab 1:</strong></p><p><img src="./tab1.gif" width="800"></p><p><strong>Tab 2:</strong></p><p><img src="./tab2.gif" width="800"></p><p><strong>Tab 3:</strong></p><p><img src="./tab3.gif" width="800"></p><h2 id="stock-streaming-fundamental">1 Stock: Streaming &amp; Fundamental</h2><h3 id="data-source">1.1 Data Source</h3><ul><li><a href="https://www.alphavantage.co/" target="_blank" rel="noopener">Alpha Vantage</a> : provide free APIs for realtime and historical data on stocks.<ul><li>Problems: rate limiting of 5 calls per minute, 500 calls per day for free account .</li></ul></li><li><a href="https://finance.yahoo.com/" target="_blank" rel="noopener">Yahoo Finance</a>: write a web crawler to get company's summary profile and fundamental information such as financial statements from Yahoo Finance.</li></ul><h3 id="etl">1.2 ETL</h3><h4 id="historical-data">1. Historical data</h4><p>In the first tab, both historical data and streaming data are presented. Before each trading day, I got every stocks' historical data from <strong><em>Alpha Vantage</em></strong> to update the newest daily price and volume.</p><p>The raw data is in <em>string</em> type, the transformation of the data type needs to be performed. I also standardized 'time' column to &quot;%Y-%m-%d %H:%M:%S&quot; because that's the datetime format Cassandra could recognize.</p><p>Each stock has three tables that stored its daily, 1-minute, and 18-second frequency's OHLCV in <strong><em>Apache Cassandra</em></strong>. Cassandra has its own query language <strong><em>Cassandra Query Language (CQL)</em></strong> which is quite similar to SQL.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS SYMBOL_historical ( </span><br><span class="line">       TIME timestamp,           </span><br><span class="line">       SYMBOL text,              </span><br><span class="line">       OPEN float,               </span><br><span class="line">       HIGH float,               </span><br><span class="line">       LOW float,                </span><br><span class="line">       CLOSE float,              </span><br><span class="line">       ADJUSTED_CLOSE float,     </span><br><span class="line">       VOLUME float,             </span><br><span class="line">       dividend_amount float,    </span><br><span class="line">       split_coefficient float,  </span><br><span class="line">       PRIMARY KEY (SYMBOL,TIME));</span><br></pre></td></tr></table></figure><h4 id="realtime-data">2. Realtime data</h4><p>I used Apache Kafka to handle streaming data, the stream plot would combine both historical as well as the streaming data for visualization. The <em>topic</em> for this streaming task is <em>'stock_streaming1'</em>.</p><p>This <em>topic</em> has two producers, which recevied fake data generated by random number during non-trading hours and realtime S&amp;P500 price and volume from <strong><em>Alpha Vantage</em></strong> during trading hours. It also has two consumers, which send data to Cassandra database and to a local json file.</p><p><img src="./stream1.png" width="600"></p><p><strong>NOTE:</strong></p><p>Since <strong><em>Alpha Vantage</em></strong> could only be called 5 times per minute, so I could either get one minute frequency data by calling <em>TIME_SERIES_INTRADAY</em> function provided by <strong><em>Alpha Vantage</em></strong> every minute , or to get data faster, call <em>GLOBAL_QUOTE</em> function every 18 seconds to get the latest price and volume information, while avoiding rate limiting error.</p><p>One other thing to note is <strong>timezone</strong>. I am in Pacific Time. To align with Eastern Time trading hours, I need to record the time I called <em>GLOBAL_QUOTE</em>, adjust to Eastern Time, and store accordingly to the database.</p><p>Interestingly, Cassandra stores datetime type data in UTC timezone. When querying data from Cassandra, we need to double check whether the time changed. Otherwise, the plot would not seamlessly combine historical data and streaming data.</p><h4 id="fundatmental-data">3. Fundatmental data</h4><p>I wrote a script to crawl the financial statements and summary profile given the company's symbol, and loaded each company's fundamental data to a json file.</p><p><img src="./fundamental.png" width="700"></p><p>¬†</p><h2 id="stock-comparison">2 Stock: Comparison</h2><p>The second tab only uses historical daily frequency data, extracted from Cassandra database. It aims to compare two stocks. So I plot 2 selected stocks' adjusted close and volume, along with 3 technical indicators: 5MA, 10MA, 30MA; as well as the statistical summay and correlation of these 2 stocks.</p><p>You can also click on the legend entries to hide technical indicators.</p><p><img src="./click.png" width="700"></p><h2 id="economy">3 Economy</h2><h3 id="data-source-1">3.1 Data Source</h3><ul><li><a href="https://www.bea.gov/" target="_blank" rel="noopener">The U.S. Bureau of Economic Analysis</a>: provide API to access to BEA published economic statistics.</li><li><a href="https://fred.stlouisfed.org/" target="_blank" rel="noopener">The Federal Reserve Bank of St. Louis</a>: provide API that has been integrated into <em>pandas_datareader</em></li><li><a href="https://newsapi.org/" target="_blank" rel="noopener">NEWS API</a>: provide breaking news headlines by categories or keywords</li></ul><h3 id="etl-1">3.2 ETL</h3><h4 id="economy-data">1. Economy data</h4><p>Firstly I went through websites of <strong><em>BEA</em></strong> and <strong><em>FRED</em></strong>, and identified the key economy data to present, such as real GDP, CPI, unemployment rate, and etc. <em>BEA</em> provides economy data by state, <em>FRED</em> provides nationwide economy indicators. Then I decided to combine geomap using data from <em>BEA</em> to present the difference across the states, and time series plots using data from <em>FRED</em> to present the change of key economy indicators. The economy data are all stored in json files.</p><p>To plot geomap, I downloaded geographical boundaries of U.S. states <a href="http://econym.org.uk/gmap/states.xml" target="_blank" rel="noopener">here</a>, and fed into <strong>Bokeh</strong>.</p><h4 id="business-news">2. Business news</h4><p><strong><em>NEWS API</em></strong> is a really interesting and useful tool to get up-to-date news headlines given categories or keywords.</p><p><img src="./news.png" width="600"></p><p>Here I used <strong><em>Kafka</em></strong> to stream news data using <em>topic</em> <strong><em>NEWS</em></strong> and stored in <strong><em>Cassandra</em></strong>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS NEWS ( </span><br><span class="line">       DATE date, </span><br><span class="line">       publishedAt timestamp,           </span><br><span class="line">       TITLE text,              </span><br><span class="line">       SOURCE text,               </span><br><span class="line">       description text,               </span><br><span class="line">       url text, PRIMARY KEY (DATE,publishedAt)) </span><br><span class="line">       WITH CLUSTERING ORDER BY (publishedAt ASC);&quot;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this project, I developed a financial data processing and visualization platform using &lt;strong&gt;&lt;u&gt;Apache Kafka&lt;/u&gt;&lt;/strong&gt;, &lt;u&gt;&lt;strong&gt;Apache Cassandra&lt;/strong&gt;&lt;/u&gt;, and &lt;strong&gt;&lt;u&gt;Bokeh&lt;/u&gt;&lt;/strong&gt;. I used Kafka for realtime stock price and market news streaming, Cassandra for historical and realtime stock data warehousing, and Bokeh for visualization on web browsers. I also wrote a web crawler to scrape companys&#39; financial statements and basic information from Yahoo Finance, and played with various economy data APIs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please check the &lt;a href=&quot;https://github.com/nancyyanyu/kafka_stock&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub repo&lt;/a&gt; of this project, and &lt;strong&gt;most importantly, please check this platform&#39;s website and play with each plot&lt;/strong&gt; &lt;span class=&quot;math inline&quot;&gt;\(\rightarrow\)&lt;/span&gt; &lt;a href=&quot;http://magiconch.me/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;magiconch.me&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
      <category term="Bokeh" scheme="https://nancyyanyu.github.io/tags/Bokeh/"/>
    
      <category term="Kafka" scheme="https://nancyyanyu.github.io/tags/Kafka/"/>
    
      <category term="Cassandra" scheme="https://nancyyanyu.github.io/tags/Cassandra/"/>
    
  </entry>
  
  <entry>
    <title>Data Analysis of K-POP: Playing with Spotify API</title>
    <link href="https://nancyyanyu.github.io/posts/63adf3bb/"/>
    <id>https://nancyyanyu.github.io/posts/63adf3bb/</id>
    <published>2019-10-20T01:51:39.386Z</published>
    <updated>2020-04-29T12:11:21.899Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>K-pop has become a phenomenon in the U.S, as evidenced by bombing number of K-pop shows across the nation. In Spotify's K-pop genre, there are more than 500 K-pop artists. Among them, the wildly popular male group BTS is certainly worth mentioning. According to the data of Top 100 selling artists in the first half of 2019, out of 10 album copies sold out, there are 4 of BTS's albums <a href="https://www.allkpop.com/article/2019/07/bts-occupies-419-of-total-album-sales-of-top-100-kpop-artists" target="_blank" rel="noopener">(Ref)</a>.</p><p>BTS is certainly a reason for the rise of K-pop, but as more and more K-pop artists successfully hit the Billboard charts and took massive world tour, K-pop is definitely a hot topic to dig into.</p><p>This K-Pop sensation draws my attention to perform data analysis using data provided by <strong><em>Spotify API</em></strong> which could be used to answer some interesting questions like:</p><ul><li>Besides BTS, who are the best K-pop artists in the U.S. market?</li><li>How do their popularity change?</li><li>How did the market of K-pop music evolve in size and audio features?</li></ul><a id="more"></a><h1 id="setup">Setup</h1><h3 id="register-an-app"><strong>Register an app</strong>:</h3><p>Just like my last project which used Twitter API, the first step is to register an app on <a href="https://developer.spotify.com/" target="_blank" rel="noopener">Spotify for Developers</a>, then copy <em>Client ID</em> and <em>Client Secret</em>.</p><p><img src="./1.jpg" width="800"></p><p>¬†</p><h3 id="install-spotify-web-api-python-library"><strong>Install Spotify Web API Python library:</strong></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install spotipy</span><br></pre></td></tr></table></figure><p>Please check <strong><em>spotipy</em></strong>'s <a href="https://spotipy.readthedocs.io/en/latest/" target="_blank" rel="noopener">document</a> for more detail.</p><p>¬†</p><h3 id="download-all-relevant-data-to-local-machine">Download all relevant data to local machine</h3><p>I firstly wrote a class <strong><em>MySpotify</em></strong> to grab all relevant data and stored the data under local machine in <em>json</em> format.</p><h4 id="relevant-data"><strong>Relevant data:</strong></h4><ul><li>All artists' information under 'k-pop' genre;</li><li>All albums' information of every k-pop artists</li><li>All tracks' information of all albums of every k-pop artists</li><li>Audio features provided by <em>Spotify</em> of all tracks</li></ul><h4 id="audio-features"><strong>Audio features:</strong></h4><p>There are 8 main audio features provided by <em>Spotify</em>. Here's the explanation from Spotify's <a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/" target="_blank" rel="noopener">document</a>.</p><ul><li><strong><em>Danceability</em></strong> describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.</li><li><strong>Energy</strong> is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity.</li><li><strong>Instrumentalness</strong> predicts whether a track contains no vocals.<br></li><li><strong>Liveness</strong> detects the presence of an audience in the recording.</li><li><strong>Loudness</strong>: the overall loudness of a track in decibels (dB).</li><li><strong>Speechiness</strong> detects the presence of spoken words in a track.</li><li><strong>Valence:</strong> A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track.</li><li><strong>Tempo:</strong> the overall estimated tempo of a track in beats per minute (BPM).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpotify</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""This class was to download data from Spotify API.</span></span><br><span class="line"><span class="string">    The download process should be: </span></span><br><span class="line"><span class="string">        artist=artist_genre -&gt; album=album_artists(artist) -&gt; songs=songs_albums(album)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,genre=<span class="string">'k-pop'</span>)</span>:</span></span><br><span class="line">        self.authenticate()</span><br><span class="line">        self.genre=genre</span><br><span class="line">                </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">authenticate</span><span class="params">(self)</span>:</span></span><br><span class="line">        auth=open_json(<span class="string">'auth'</span>)</span><br><span class="line">        my_id = auth[<span class="string">'my_id'</span>]</span><br><span class="line">        secret_key = auth[<span class="string">'secret_key'</span>]</span><br><span class="line">        </span><br><span class="line">        client_credentials_manager = SpotifyClientCredentials(client_id = my_id, client_secret = secret_key)</span><br><span class="line">        self.sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)                </span><br><span class="line">        </span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><a href="https://github.com/nancyyanyu/mini_projects/blob/master/spotify_project/spotify_app.py" target="_blank" rel="noopener">Full code</a></p><p>¬†</p><h1 id="data-analysis">Data Analysis</h1><h3 id="who-are-the-most-popular-k-pop-artists">1. Who are the most popular K-pop artists?</h3><p>Before we jump to the answer, let me take a second to introduce two measures of prevelance in Spotify.</p><ul><li><strong>Followers:</strong> The number of people following the artist.</li><li><strong>Popularity:</strong> The popularity of the artist. The value will be between 0 and 100, with 100 being the most popular. The artist‚Äôs popularity is calculated from the popularity of all the artist‚Äôs tracks.</li></ul><p>There are cases when some artists have a large amount of followers, but relatively low popularity. That's possibly because they are not active in releasing new songs recently or simply retired from musical life.</p><p>To find the hottest K-pop artists, I extracted all artists' followers and popularity data, ranked them, and visualized top 40 artists in terms of the number of followers.</p><p><img src="./2.png" alt="Fig 1" width="800"></p><p>In <em>Fig 1</em>, for artists with <font color="orange">orange</font> bar higher than <font color="blue">blue</font> bar, some of them are hot, recently debuted K-pop groups, like <strong><em>(G)I-DLE, NCT 127, Stray Kids</em></strong>; most of them released new songs in recent month, e.g. <strong>Jay Park</strong> released his new EP &quot;Nothing Matters&quot; this month.</p><p>For artists with <font color="orange">orange</font> bar lower than <font color="blue">blue</font> bar, some of them are inactive for a long time, such as <strong>G-Dragon</strong> who is serving compulsory military duty; some of them are <em>temporary</em> group or solo which only released one or two albums, like <strong>EXO-CBX, J-hope</strong>.</p><p>Ranking could be sometimes misleading. As 4 out of 10 album copies sold out are BTS's albums, what if this K-pop phenomenon is actually BTS phenomenon? Let's check the numerical number of followers and popularity of these K-pop artists.</p><p><img src="./3.png" alt="Fig 2" width="800"></p><p><img src="./4.png" alt="Fig 3" width="800"></p><p>From <em>Fig 2 &amp; 3</em>, we can see that BTS has more than 1/3 followers than the second hottest group, and more than 1/2 followers than the third one. In addition, BTS has an popularity rate of 99, very close to 100, the highest possible popularity rate.</p><p>For comparison, I listed some top artists in the U.S. market:</p><table><thead><tr class="header"><th>Artists</th><th>Followers</th><th>Popularity</th></tr></thead><tbody><tr class="odd"><td>Ed Sheeran</td><td>47,295,649</td><td>100</td></tr><tr class="even"><td>Drake</td><td>38,673,834</td><td>99</td></tr><tr class="odd"><td>Taylor Swift</td><td>21,198,450</td><td>93</td></tr><tr class="even"><td>Ariana Grande</td><td>32,355,391</td><td>95</td></tr></tbody></table><p>Comparing the top K-pop artists and the top U.S. mainstream artists, except from BTS, other K-pop artists' could hardly be labelled as top stars in the U.S. Even BTS who have amazingly high popularity, they have significantly less followers.</p><p>In my point of view, before BTS rising to fame, K-pop is only a subculture in the U.S., and not usually depicted by western mainstream media. Today, whether K-pop has become a mainstream phenomenon is still open to debate. At least the far less followers of most of the K-pop artists is an evidence against this argument.</p><h3 id="how-does-popularity-change">2. How does popularity change?</h3><p>Since <em>Spotify</em> does not provide historical data of artists' popularity and followers, I write a class <strong><em>Popularity</em></strong> to grab everyday's data, upload to AWS DynamoDB.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Popularity</span><span class="params">(MySpotify)</span>:</span>    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,table_name=<span class="string">'KPOP_POPULARITY'</span>,genre=<span class="string">'k-pop'</span>)</span>:</span></span><br><span class="line">        super().__init__(genre)</span><br><span class="line">        self.table = dynamodb.Table(table_name)</span><br><span class="line">        self.track_table=dynamodb.Table(table_name+<span class="string">'_TRACK'</span>)</span><br><span class="line">        self.data_folder=<span class="string">'%s_artists_info'</span>%self.genre</span><br><span class="line">        self.today=str(dt.datetime.now().date())</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put_data</span><span class="params">(self)</span>:</span></span><br><span class="line">        artists=open_json(<span class="string">"data/artists/"</span>+self.data_folder)            </span><br><span class="line">        info=&#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> artist <span class="keyword">in</span> artists:</span><br><span class="line">            info[artist[<span class="string">'name'</span>]]=&#123;&#125;</span><br><span class="line">            info[artist[<span class="string">'name'</span>]][<span class="string">'followers'</span>]=artist[<span class="string">'followers'</span>][<span class="string">'total'</span>]</span><br><span class="line">            info[artist[<span class="string">'name'</span>]][<span class="string">'popularity'</span>]=artist[<span class="string">'popularity'</span>]</span><br><span class="line">        </span><br><span class="line">        self.table.put_item(</span><br><span class="line">           Item=&#123;<span class="string">'time'</span>: self.today,</span><br><span class="line">               <span class="string">'info'</span>:info&#125;)</span><br><span class="line">        print(<span class="string">"Update artist data to AWS DynamoDB: &#123;&#125;"</span>.format(self.today))</span><br><span class="line">                </span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><a href="https://github.com/nancyyanyu/mini_projects/blob/master/spotify_project/popularity.py" target="_blank" rel="noopener">Full code</a></p><p>The <strong><em>ETL process</em></strong> works like this:</p><center><img src="./6.png" width="540"></center><ol type="1"><li>Grab artists' data from <em>Spotify</em> API;</li><li>Organize and clean the data;</li><li>Save data to local machine in <em>json</em> format;</li><li>Save data to <strong>AWS DynamoDB</strong>.</li></ol><p>Here is a visualization of the change of BTS' followers &amp; popularity using data collected during writing this report.</p><p><img src="./5.png" alt="Fig 4" width="800"></p><h3 id="how-many-k-pop-albums-released-every-year">3. How many K-pop albums released every year?</h3><p>As an industry, K-pop has experienced significant increase in size over the past two decades. According to each music agent companies' marketing tactics, K-pop artitists will demonstrate certain pattern in terms of the timing of releasing new albums.</p><p>Here is a visualization of the number of albums released by year and by month. We can see a sharp increase from 2016 to 2018. Also artists seem to be more active during October and November.</p><p><img src="./7.png" width="700"></p><h3 id="audios-features-analysis">4. Audios' features analysis</h3><p>To analyze audio features, I wrote a class <strong><em>Analyze</em></strong> to wrap to ETL process and analysis functions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Analyze</span><span class="params">(object)</span>:</span>       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,genre=<span class="string">'k-pop'</span>)</span>:</span></span><br><span class="line">        self.features=[<span class="string">'acousticness'</span>,<span class="string">'liveness'</span>,          </span><br><span class="line">                  <span class="string">'instrumentalness'</span>,<span class="string">'speechiness'</span>, </span><br><span class="line">                   <span class="string">'danceability'</span>,<span class="string">'energy'</span>,<span class="string">'valence'</span>,<span class="string">'tempo'</span>]</span><br><span class="line">        self.poplr=Popularity()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_data</span><span class="params">(self,artist)</span>:</span></span><br><span class="line">        df=pd.read_hdf(<span class="string">'./data/analysis/&#123;&#125;_tracks_analysis.h5'</span>.format(artist))</span><br><span class="line">        <span class="keyword">return</span> df</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><a href="https://github.com/nancyyanyu/mini_projects/blob/master/spotify_project/track_analysis.py" target="_blank" rel="noopener">Full code</a></p><p>The <strong><em>ETL process</em></strong> works like this:</p><center><img src="./10.png" width="540"></center><p>As last chapter says, I extracted 8 audio features of all tracks. I calculated the truncated average of each feature by year, and try to find some changes in the K-pop music itself.</p><p><img src="./8.png" width="800"></p><p>We can see features like <strong><em>acousticness</em></strong>, and <strong><em>instrumentalness</em></strong> have an upward trend; while <strong><em>energy</em></strong>, <strong><em>tempo</em></strong> and <strong><em>valence</em></strong> have un downward trend in recent years.</p><p>The evolution of K-pop music is not smooth. It seems it has experienced a radical shift around 2013 (2012-2014), as most of the features changed their disposition. After 2013 or 2014, the K-pop songs:</p><ul><li>Become more and more acoustic, and instrumental.</li><li>Contain more spoken words.</li><li>Convey less energy and positiveness. In other words, they sound less cheerful but more sad and depressed.</li><li>Become a bit slower in tempo.</li><li>Are still highly danceable, but not as danceable as before 2014.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;K-pop has become a phenomenon in the U.S, as evidenced by bombing number of K-pop shows across the nation. In Spotify&#39;s K-pop genre, there are more than 500 K-pop artists. Among them, the wildly popular male group BTS is certainly worth mentioning. According to the data of Top 100 selling artists in the first half of 2019, out of 10 album copies sold out, there are 4 of BTS&#39;s albums &lt;a href=&quot;https://www.allkpop.com/article/2019/07/bts-occupies-419-of-total-album-sales-of-top-100-kpop-artists&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;(Ref)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;BTS is certainly a reason for the rise of K-pop, but as more and more K-pop artists successfully hit the Billboard charts and took massive world tour, K-pop is definitely a hot topic to dig into.&lt;/p&gt;
&lt;p&gt;This K-Pop sensation draws my attention to perform data analysis using data provided by &lt;strong&gt;&lt;em&gt;Spotify API&lt;/em&gt;&lt;/strong&gt; which could be used to answer some interesting questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Besides BTS, who are the best K-pop artists in the U.S. market?&lt;/li&gt;
&lt;li&gt;How do their popularity change?&lt;/li&gt;
&lt;li&gt;How did the market of K-pop music evolve in size and audio features?&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
      <category term="AWS" scheme="https://nancyyanyu.github.io/tags/AWS/"/>
    
      <category term="Airflow" scheme="https://nancyyanyu.github.io/tags/Airflow/"/>
    
      <category term="Visualization" scheme="https://nancyyanyu.github.io/tags/Visualization/"/>
    
  </entry>
  
  <entry>
    <title>Realtime Twitter Data Analysis using Spark Streaming</title>
    <link href="https://nancyyanyu.github.io/posts/9fb5a802/"/>
    <id>https://nancyyanyu.github.io/posts/9fb5a802/</id>
    <published>2019-10-20T01:51:32.783Z</published>
    <updated>2019-10-20T01:56:57.803Z</updated>
    
    <content type="html"><![CDATA[<p>In this project, I built an application that extract streaming tweets from Twitter, transform the data, and visualize using Apache Sparking Streaming to gain the trending hashtags of a specific topic. In particular, I used a window size of 5 minutes to always get the latest 5 minutes result.</p><a id="more"></a><h1 id="apache-spark-streaming">Apache Spark Streaming</h1><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like <code>map</code>, <code>reduce</code>, <code>join</code> and <code>window</code>.</p><p>I will skip the explaination of how to set up spark in local machine, and the details of Streaming API. Please see the <a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html" target="_blank" rel="noopener">document</a> of Spark.</p><h1 id="part-i.-create-twitter-streaming">Part I. Create Twitter Streaming</h1><h2 id="register-twitter-app">Register Twitter App</h2><p>Before using Twitter's API, I registered an app <a href="https://developer.twitter.com/en/apps" target="_blank" rel="noopener">here</a>, and got the <em>Consumer API keys</em>, <em>Access token &amp; access token secret</em>.</p><p><img src="./2.jpg" width="500"></p><p>These information should be saved, as Twitter needs them to authenticate a user.</p><h2 id="extract-tweets-from-twitter-streaming">Extract Tweets from Twitter Streaming</h2><p>I used Twitter API Python wrapper <a href="https://github.com/bear/python-twitter" target="_blank" rel="noopener">python-twitter</a> to get Tweets stream. Here's a snippet of the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">twt_app</span><span class="params">(TCP_IP,TCP_PORT,keyword=KEY_WORD)</span>:</span></span><br><span class="line">    consumer_key=<span class="string">''</span></span><br><span class="line">    consumer_secret=<span class="string">''</span></span><br><span class="line">    access_token=<span class="string">''</span></span><br><span class="line">    access_token_secret=<span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    api = twitter.Api(consumer_key=consumer_key,</span><br><span class="line">                      consumer_secret=consumer_secret,</span><br><span class="line">                      access_token_key=access_token,</span><br><span class="line">                      access_token_secret=access_token_secret,</span><br><span class="line">                      sleep_on_rate_limit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    LANGUAGES = [<span class="string">'en'</span>]</span><br><span class="line">    </span><br><span class="line">    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    s.bind((TCP_IP, TCP_PORT))</span><br><span class="line">    s.listen(<span class="number">10</span>)    </span><br><span class="line">    conn, addr = s.accept()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> api.GetStreamFilter(track=[keyword],languages=LANGUAGES):</span><br><span class="line">        conn.send( line[<span class="string">'text'</span>].encode(<span class="string">'utf-8'</span>) )</span><br><span class="line">        print(line[<span class="string">'text'</span>])</span><br><span class="line">        print()</span><br></pre></td></tr></table></figure><p>Basically, I get the streaming of tweets from Twitter API, extract each tweet's text content, and send them to Spark Streaming instance via TCP connection.</p><p>Let's try a topic as <strong>&quot;Trump&quot;</strong>! Here is the result recorded in console:</p><p><img src="./3.gif" width="750"></p><h1 id="part-ii.-set-up-streaming-application">Part II. Set Up Streaming Application</h1><p>Then I set up Spark Streaming App to process tweets text, gain hashtags in every tweet mentioned <strong>&quot;Trump&quot;</strong>, and barplot the top 20 hashtags on the times they appeared in most recent 5 minutes. Here's a snippet of the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spark</span><span class="params">(TCP_IP,TCP_PORT,KEY_WORD)</span>:</span></span><br><span class="line">    sc=SparkContext(appName=<span class="string">"TwitterStreamming"</span>)</span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    ssc=StreamingContext(sc,<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    socket_stream = ssc.socketTextStream(TCP_IP,TCP_PORT)</span><br><span class="line">    </span><br><span class="line">    lines=socket_stream.window(<span class="number">300</span>)</span><br><span class="line">    df=lines.flatMap(<span class="keyword">lambda</span> x:x.split(<span class="string">" "</span>))  \</span><br><span class="line">            .filter(<span class="keyword">lambda</span> x:x.startswith(<span class="string">"#"</span>))  \</span><br><span class="line">            .filter(<span class="keyword">lambda</span> x:x!=<span class="string">'#%s'</span>%KEY_WORD)  </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(rdd)</span>:</span></span><br><span class="line">        spark=SparkSession \</span><br><span class="line">                .builder \</span><br><span class="line">                .config(conf=rdd.context.getConf()) \</span><br><span class="line">                .getOrCreate()</span><br><span class="line">    </span><br><span class="line">        rowRdd = rdd.map(<span class="keyword">lambda</span> x: Row(word=x))</span><br><span class="line">        wordsDataFrame = spark.createDataFrame(rowRdd)</span><br><span class="line">    </span><br><span class="line">        wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</span><br><span class="line">        wordCountsDataFrame = spark.sql(<span class="string">"select word, count(*) as total from words group by word order by 2 desc"</span>)       </span><br><span class="line">        pd_df=wordCountsDataFrame.toPandas()</span><br><span class="line">        </span><br><span class="line">        plt.figure( figsize = ( <span class="number">10</span>, <span class="number">8</span> ) )</span><br><span class="line">        sns.barplot( x=<span class="string">"total"</span>, y=<span class="string">"word"</span>, data=pd_df.head(<span class="number">20</span>))</span><br><span class="line">        plt.show()</span><br><span class="line">        </span><br><span class="line">    df.foreachRDD(process)</span><br><span class="line">    </span><br><span class="line">    ssc.start()</span><br><span class="line">    time.sleep(<span class="number">600</span>)</span><br><span class="line">    ssc.stop(stopSparkContext=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Let's see what I got!</p><p><img src="./4.gif" width="750"></p><h1 id="summary">Summary</h1><p>Thanks to Twitter API and its python Wrapper, I was able to easily get tweets streaming filtered on specified topic. Real-time ETL process could be used to provide instantaneous recommendation, anomaly detection, and etc. There are various projects around Twitter Streaming to be explored. I, in this post, tried a very simple application to find the real-time hashtags trending around a topic.</p><p>As I filtered tweets on topic <strong>&quot;Trump&quot;</strong> , I got <em>#WGDP</em>, <em>#USWNT</em> to be two of the most trending hashtags during the time I ran the application. This application could catch big hot news, and it also serves as a great way to know about what people are talking about in a smaller topic.</p><p>Please check the full code on <a href="https://github.com/nancyyanyu/mini_projects/tree/master/twitter_project" target="_blank" rel="noopener">GitHub</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this project, I built an application that extract streaming tweets from Twitter, transform the data, and visualize using Apache Sparking Streaming to gain the trending hashtags of a specific topic. In particular, I used a window size of 5 minutes to always get the latest 5 minutes result.&lt;/p&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
      <category term="Spark" scheme="https://nancyyanyu.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Analysis of 2018 H-1B Sponsorship for Data Science Employees</title>
    <link href="https://nancyyanyu.github.io/posts/8b70757d/"/>
    <id>https://nancyyanyu.github.io/posts/8b70757d/</id>
    <published>2019-10-20T01:51:21.518Z</published>
    <updated>2019-10-20T01:56:46.489Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>The <strong>H-1B</strong> is a visa in the U.S. that allows U.S. employers to temporarily employ foreign workers in specialty occupations. For international students who are trying to find Data science jobs in the U.S., H-1B visa is the most common working visa. Job hunting is stressful, so the tatics show more importance when selecting the companies to apply. Not saying there are no chances in companies with no H-1B sponsor records in 2018, as policies vary in each company every year. Accessing more information and prioritizing tasks in hand are what new grads need to do.</p><p>I want to dig into the data of H-1B case disclosure file in 2018 a little. The data could be found at U.S. Department of Labor's website: https://www.foreignlaborcert.doleta.gov/performancedata.cfm#dis. I would just focus on data related entry level jobs.</p><p>The <strong><em>Jupyter notebook</em></strong> is <a href="https://github.com/nancyyanyu/mini_projects/blob/master/h1b_analysis/h1b_analysis.ipynb" target="_blank" rel="noopener">here</a>.</p><a id="more"></a><h1 id="key-insights">Key Insights</h1><h4 id="companies-in-california-new-jersey-texas-new-york-illinois-like-to-sponsor-data-science-employees-most-which-takes-up-50-of-that-in-the-u.s.">Companies in California, New Jersey, Texas, New York, Illinois like to sponsor data science employees most, which takes up &gt;50% of that in the U.S.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#count number of data related job H-1B cases in each state</span></span><br><span class="line">data_geo=df[df.JOB_TITLE.isin(data_job_title)].groupby(<span class="string">'EMPLOYER_STATE'</span>)[<span class="string">'CASE_NUMBER'</span>].count().sort_values(ascending=<span class="literal">False</span>).to_frame()</span><br><span class="line">data_geo.to_csv(<span class="string">'./data_geo.csv'</span>)</span><br></pre></td></tr></table></figure><p>For any job titles containing <em>Data</em>, <em>Machine Learning</em>, <em>Research Scientist</em>, <em>Applied Scientist</em>, <em>SQL</em>, I would include them as data science related jobs. I know there are job titles like <em>Product Analyst</em> which are essentially <em>Data Analyst</em> or <em>Data Scientist</em>, but I'll ignore these cases here.</p><p>We can see the top 5 states sponsor <span class="math inline">\(9699\)</span> data science employees which take up more than <span class="math inline">\(50\%\)</span> of that in the U.S.</p><p>Here is a map in Tableau that demonstrates how many data science employees sponsored in each state.</p><iframe src="https://public.tableau.com/shared/MCQ8F254G?:showVizHome=no&amp;:embed=true" width="900" height="600"></iframe><h4 id="besides-big-tech-companies-iccs-sponsor-a-lot">Besides big tech companies, ICCs sponsor a LOT!</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_company=data_company.groupby(<span class="string">'EMPLOYER_NAME'</span>)[<span class="string">'CASE_NUMBER'</span>].sum().sort_values(ascending=<span class="literal">False</span>).to_frame()</span><br></pre></td></tr></table></figure><p>Here is a visualization of companies who sponsor H-1B (Only shows companies with ‚â• 20 cases)</p><iframe src="https://public.tableau.com/views/Book4_15618711097530/NumberofDataScienceEmployeesSponsoredinEachCompany?:showVizHome=no&amp;:embed=true" width="900" height="450"></iframe><h4 id="more-than-140-companies-in-washington-state-sponsor-and-they-locate-not-only-in-washington-state">More than 140 companies in Washington state sponsor, and they locate not only in Washington state</h4><p>Since I'm only interested in companies located near Seattle, I extract the list of companies based in Washington state. Here is a visualization showing that for employers in Wahington state,</p><ul><li>how many data science employees sponsered by each of these companies,</li><li>how many cases in each job titles, and</li><li>how many data science employees sponsored by these companies across the nation (not just in Washington state).</li></ul><p>There are several job hunting insights from these data.</p><ul><li>There are 144 companies in Washington state that sponsor data science employees. Most of these companies only sponsor 1 or 2 data science employees. These companies are mostly small-size companies which are usually ignored by job hunters.</li><li>Job titles like <em>Data Scientist</em>, <em>Applied Scientist</em>, <em>Research Scientist</em> are sponsored most. These job titles mostly required more than 5 years of professional experience or PhD degree. That's because these job titles mostly come from Microsoft and Amazon, and they rarely hire entry level people for data science.</li><li>Companies based in Washington state not only sponsored people in the state, but also in other states, like California, Texas, New Jersey, and etc.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_comp=df[(df.EMPLOYER_STATE==<span class="string">'WA'</span>) &amp; (df.JOB_TITLE.isin(data_job_title))].groupby(<span class="string">'EMPLOYER_NAME'</span>)[<span class="string">'CASE_NUMBER'</span>].count().sort_values(ascending=<span class="literal">False</span>).reset_index()</span><br><span class="line">data_comp_wa=pd.merge(data_comp,df[[<span class="string">'EMPLOYER_CITY'</span>,<span class="string">'EMPLOYER_STATE'</span>,<span class="string">'EMPLOYER_NAME'</span>]].drop_duplicates(),on=[<span class="string">'EMPLOYER_NAME'</span>],how=<span class="string">'left'</span>)</span><br></pre></td></tr></table></figure><iframe src="https://public.tableau.com/shared/82398BJSX?:showVizHome=no&amp;:embed=true" width="1000" height="800"></iframe><p><strong>Ref:</strong></p><p>https://en.wikipedia.org/wiki/H-1B_visa</p><p><a href="https://www.foreignlaborcert.doleta.gov/performancedata.cfm#dis" target="_blank" rel="noopener">U.S. Department of Labor</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;H-1B&lt;/strong&gt; is a visa in the U.S. that allows U.S. employers to temporarily employ foreign workers in specialty occupations. For international students who are trying to find Data science jobs in the U.S., H-1B visa is the most common working visa. Job hunting is stressful, so the tatics show more importance when selecting the companies to apply. Not saying there are no chances in companies with no H-1B sponsor records in 2018, as policies vary in each company every year. Accessing more information and prioritizing tasks in hand are what new grads need to do.&lt;/p&gt;
&lt;p&gt;I want to dig into the data of H-1B case disclosure file in 2018 a little. The data could be found at U.S. Department of Labor&#39;s website: https://www.foreignlaborcert.doleta.gov/performancedata.cfm#dis. I would just focus on data related entry level jobs.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;Jupyter notebook&lt;/em&gt;&lt;/strong&gt; is &lt;a href=&quot;https://github.com/nancyyanyu/mini_projects/blob/master/h1b_analysis/h1b_analysis.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Project" scheme="https://nancyyanyu.github.io/categories/Project/"/>
    
    
  </entry>
  
  <entry>
    <title>Study Note: Linear Regression Example Prostate Cancer</title>
    <link href="https://nancyyanyu.github.io/posts/2ce36d51/"/>
    <id>https://nancyyanyu.github.io/posts/2ce36d51/</id>
    <published>2019-10-19T23:04:00.161Z</published>
    <updated>2019-10-19T23:21:01.286Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">import</span> scipy.stats</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data=pd.read_csv(<span class="string">'./data/prostate.data'</span>,delimiter=<span class="string">'\t'</span>,index_col=<span class="number">0</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><a id="more"></a><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }        .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>lcavol</th><th>lweight</th><th>age</th><th>lbph</th><th>svi</th><th>lcp</th><th>gleason</th><th>pgg45</th><th>lpsa</th><th>train</th></tr></thead><tbody><tr><th>1</th><td>-0.579818</td><td>2.769459</td><td>50</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>6</td><td>0</td><td>-0.430783</td><td>T</td></tr><tr><th>2</th><td>-0.994252</td><td>3.319626</td><td>58</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>6</td><td>0</td><td>-0.162519</td><td>T</td></tr><tr><th>3</th><td>-0.510826</td><td>2.691243</td><td>74</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>7</td><td>20</td><td>-0.162519</td><td>T</td></tr><tr><th>4</th><td>-1.203973</td><td>3.282789</td><td>58</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>6</td><td>0</td><td>-0.162519</td><td>T</td></tr><tr><th>5</th><td>0.751416</td><td>3.432373</td><td>62</td><td>-1.386294</td><td>0</td><td>-1.386294</td><td>6</td><td>0</td><td>0.371564</td><td>T</td></tr></tbody></table></div><p>The correlation matrix of the predictors:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.corr()</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }        .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>lcavol</th><th>lweight</th><th>age</th><th>lbph</th><th>svi</th><th>lcp</th><th>gleason</th><th>pgg45</th><th>lpsa</th></tr></thead><tbody><tr><th>lcavol</th><td>1.000000</td><td>0.280521</td><td>0.225000</td><td>0.027350</td><td>0.538845</td><td>0.675310</td><td>0.432417</td><td>0.433652</td><td>0.734460</td></tr><tr><th>lweight</th><td>0.280521</td><td>1.000000</td><td>0.347969</td><td>0.442264</td><td>0.155385</td><td>0.164537</td><td>0.056882</td><td>0.107354</td><td>0.433319</td></tr><tr><th>age</th><td>0.225000</td><td>0.347969</td><td>1.000000</td><td>0.350186</td><td>0.117658</td><td>0.127668</td><td>0.268892</td><td>0.276112</td><td>0.169593</td></tr><tr><th>lbph</th><td>0.027350</td><td>0.442264</td><td>0.350186</td><td>1.000000</td><td>-0.085843</td><td>-0.006999</td><td>0.077820</td><td>0.078460</td><td>0.179809</td></tr><tr><th>svi</th><td>0.538845</td><td>0.155385</td><td>0.117658</td><td>-0.085843</td><td>1.000000</td><td>0.673111</td><td>0.320412</td><td>0.457648</td><td>0.566218</td></tr><tr><th>lcp</th><td>0.675310</td><td>0.164537</td><td>0.127668</td><td>-0.006999</td><td>0.673111</td><td>1.000000</td><td>0.514830</td><td>0.631528</td><td>0.548813</td></tr><tr><th>gleason</th><td>0.432417</td><td>0.056882</td><td>0.268892</td><td>0.077820</td><td>0.320412</td><td>0.514830</td><td>1.000000</td><td>0.751905</td><td>0.368987</td></tr><tr><th>pgg45</th><td>0.433652</td><td>0.107354</td><td>0.276112</td><td>0.078460</td><td>0.457648</td><td>0.631528</td><td>0.751905</td><td>1.000000</td><td>0.422316</td></tr><tr><th>lpsa</th><td>0.734460</td><td>0.433319</td><td>0.169593</td><td>0.179809</td><td>0.566218</td><td>0.548813</td><td>0.368987</td><td>0.422316</td><td>1.000000</td></tr></tbody></table></div><p><strong>Scatterplot matrix:</strong>(showing every pairwise plot between the variables)We see that <em>svi</em> is a binary variable, and <em>gleason</em> is an ordered categorical variable.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pd.plotting.scatter_matrix(data,figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="ESL-Example-Prostate%20Cancer_5_0.png" alt="png"><figcaption>png</figcaption></figure><h2 id="simple-univariate-regression">Simple univariate regression</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">y=data[<span class="string">'lpsa'</span>]</span><br><span class="line">mask_train=data[<span class="string">'train'</span>]</span><br><span class="line">X=data.drop([<span class="string">'lpsa'</span>,<span class="string">'train'</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_data</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="comment">#normalize X</span></span><br><span class="line">    X=X.apply(scipy.stats.zscore)</span><br><span class="line">    X_train=X[mask_train==<span class="string">'T'</span>]</span><br><span class="line">    y_train=y[mask_train==<span class="string">'T'</span>]</span><br><span class="line">    X_test=X[mask_train!=<span class="string">'T'</span>]</span><br><span class="line">    y_test=y[mask_train!=<span class="string">'T'</span>]</span><br><span class="line">    X_test=np.hstack((np.ones((len(X_test),<span class="number">1</span>)),X_test))</span><br><span class="line">    X_train=np.hstack((np.ones((len(X_train),<span class="number">1</span>)),X_train))</span><br><span class="line">    <span class="keyword">return</span> X_train, y_train,X_test,y_test</span><br><span class="line"></span><br><span class="line">X_train, y_train,X_test,y_test=get_train_data(X)</span><br><span class="line">X_train.shape</span><br></pre></td></tr></table></figure><p>(67, 9)</p><p><span class="math display">\[\begin{align}\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty \end{align}\]</span> Fitted values at the training inputs: <span class="math inline">\(\hat{y}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ols</span><span class="params">(X_,y_)</span>:</span></span><br><span class="line">    w=np.linalg.inv(X_.T.dot(X_)).dot(X_.T).dot(y_)</span><br><span class="line">    y_hat=X_.dot(w)</span><br><span class="line">    <span class="keyword">return</span> w,y_hat</span><br><span class="line">w,y_hat=ols(X_train,y_train)</span><br><span class="line">y_hat.shape</span><br></pre></td></tr></table></figure><p>(67,)</p><p>Unbiased estimate of <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[\begin{align} \hat{\sigma^2}=\frac{1}{N-p-1}\sum^{N}_{i=1}(y_i-\hat{y_i})^2 \end{align}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigma2_hat=sum((y_train-y_hat)**<span class="number">2</span>)/(len(y_train)<span class="number">-1</span>-len(X.columns))</span><br><span class="line">sigma2_hat</span><br></pre></td></tr></table></figure><p>0.5073514562053173</p><p>The variance‚Äìcovariance matrix of the least squares parameter estimates: <span class="math display">\[\begin{align} Var(\hat{\beta})=(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2) \end{align}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std_w=np.sqrt(np.diag(np.linalg.inv(X_train.T.dot(X_train)))*sigma2_hat)</span><br><span class="line">std_w</span><br></pre></td></tr></table></figure><p>array([ 0.08931498, 0.12597461, 0.095134 , 0.10081871, 0.10169077, 0.1229615 , 0.15373073, 0.14449659, 0.1528197 ])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">table=pd.DataFrame(columns=[<span class="string">'Term'</span>,<span class="string">'CoeÔ¨Écient'</span>,<span class="string">'Std. Error'</span>,<span class="string">'Z Score'</span>])</span><br><span class="line">table[<span class="string">'Term'</span>]=[<span class="string">'Intercept'</span>]+list(X.columns)</span><br><span class="line">table[<span class="string">'CoeÔ¨Écient'</span>]=w</span><br><span class="line">table[<span class="string">'Std. Error'</span>]=std_w</span><br><span class="line">table[<span class="string">'Z Score'</span>]=w/std_w</span><br><span class="line">table</span><br></pre></td></tr></table></figure><div><style>    .dataframe thead tr:only-child th {        text-align: right;    }    .dataframe thead th {        text-align: left;    }        .dataframe tbody tr th {        vertical-align: top;    }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th><th>Term</th><th>CoeÔ¨Écient</th><th>Std. Error</th><th>Z Score</th></tr></thead><tbody><tr><th>0</th><td>Intercept</td><td>2.464933</td><td>0.089315</td><td>27.598203</td></tr><tr><th>1</th><td>lcavol</td><td>0.676016</td><td>0.125975</td><td>5.366290</td></tr><tr><th>2</th><td>lweight</td><td>0.261694</td><td>0.095134</td><td>2.750789</td></tr><tr><th>3</th><td>age</td><td>-0.140734</td><td>0.100819</td><td>-1.395909</td></tr><tr><th>4</th><td>lbph</td><td>0.209061</td><td>0.101691</td><td>2.055846</td></tr><tr><th>5</th><td>svi</td><td>0.303623</td><td>0.122962</td><td>2.469255</td></tr><tr><th>6</th><td>lcp</td><td>-0.287002</td><td>0.153731</td><td>-1.866913</td></tr><tr><th>7</th><td>gleason</td><td>-0.021195</td><td>0.144497</td><td>-0.146681</td></tr><tr><th>8</th><td>pgg45</td><td>0.265576</td><td>0.152820</td><td>1.737840</td></tr></tbody></table></div><p>We can also test for the exclusion of a number of terms at once, using the F-statistic: <span class="math display">\[\begin{align} F=\frac{(RSS_2-RSS_1)/(p_1-p_2)}{RSS_1/(N-p_1)} \end{align}\]</span> <span class="math display">\[\begin{align}RSS(\beta)=\sum_{i=1}^{N}(y_{i}-f(x_{i}))^2=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}X_{ij}\beta_{j})^2\end{align}\]</span></p><p>we consider dropping all the non-signiÔ¨Åcant terms, namely <em>age, lcp, gleason, and pgg45</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X2=X.drop([<span class="string">'age'</span>, <span class="string">'lcp'</span>, <span class="string">'gleason'</span>, <span class="string">'pgg45'</span>],axis=<span class="number">1</span>)</span><br><span class="line">X_train2, y_train2,X_test2,y_test2=get_train_data(X2)</span><br><span class="line">w2,y_hat2=ols(X_train2,y_train2)</span><br><span class="line">RSS1=sum((y_train-y_hat)**<span class="number">2</span>)</span><br><span class="line">RSS2=sum((y_train2-y_hat2)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">p2=X_train2.shape[<span class="number">1</span>]</span><br><span class="line">N,p1=X_train.shape</span><br><span class="line">F=round(((RSS2-RSS1)/(p1-p2))/((RSS1)/(N-p1)),<span class="number">2</span>)</span><br><span class="line">p_value=round(<span class="number">1</span>-scipy.stats.f.cdf(F,p1-p2,N-p1),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"F:&#123;&#125;"</span>.format(F))</span><br><span class="line">print(<span class="string">"Pr(F(&#123;a&#125;,&#123;b&#125;)&gt;&#123;c&#125;)=&#123;d&#125;"</span>.format(a=p1-p2,b=N-p1,c=F,d=p_value))</span><br></pre></td></tr></table></figure><p>F:1.67 Pr(F(4,58)&gt;1.67)=0.17</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RSS1,RSS2</span><br></pre></td></tr></table></figure><p>(29.4263844599084, 32.81499474881556)</p><p>The mean prediction error on the test data is 0.521. In contrast, prediction using the mean training value of lpsa has a test error of 1.057, which is called the ‚Äúbase error rate.‚Äù Hence the linear model reduces the base error rate by about 50%. We will return to this example later to compare various selection and shrinkage methods.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_test_fit=X_test.dot(w)</span><br><span class="line">base_error=round(sum((y_test-y_train.mean())**<span class="number">2</span>)/len(y_test),<span class="number">3</span>)</span><br><span class="line">prediction_error=round(sum((y_test-y_test_fit)**<span class="number">2</span>)/len(y_test),<span class="number">3</span>)</span><br><span class="line">print(<span class="string">'base_error:'</span>,base_error)</span><br><span class="line">print(<span class="string">'prediction_error:'</span>,prediction_error)</span><br></pre></td></tr></table></figure><p>base_error: 1.057 prediction_error: 0.521</p><h2 id="best-subset-selection">Best-Subset Selection</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> itertools</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nCr</span><span class="params">(n,r)</span>:</span></span><br><span class="line">    <span class="string">"""calculate how many combination of (n,r)"""</span></span><br><span class="line">    f = math.factorial</span><br><span class="line">    <span class="keyword">return</span> int(f(n) / f(r) / f(n-r))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">best_subset_ols</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="string">"""calculate all ols estimate of i size subset"""</span></span><br><span class="line">    combine=itertools.combinations(range(<span class="number">1</span>,p+<span class="number">1</span>),i)</span><br><span class="line">    y_subset_train=y_train</span><br><span class="line">    result=[]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> combine:</span><br><span class="line">        X_subset_train=X_train[:,[<span class="number">0</span>]+list(k)]</span><br><span class="line">        w,y_hat=ols(X_subset_train,y_subset_train)</span><br><span class="line">        result.append((w,sum((y_subset_train-y_hat)**<span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">return</span> np.array(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p=len(X.columns)</span><br><span class="line">best=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, p+<span class="number">1</span>):</span><br><span class="line">    ncr=nCr(p,i)</span><br><span class="line">    result=best_subset_ols(i)</span><br><span class="line">    best.append(min(result[:,<span class="number">1</span>]))</span><br><span class="line">    plt.plot(np.ones(ncr)*i,result[:,<span class="number">1</span>],<span class="string">'o'</span>,c=<span class="string">'grey'</span>)</span><br><span class="line">plt.plot(range(<span class="number">0</span>, p+<span class="number">1</span>),best,<span class="string">'o--'</span>,c=<span class="string">'red'</span>)  </span><br><span class="line">plt.xlabel(<span class="string">'Subset Size k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'RSS'</span>)</span><br><span class="line">plt.savefig(<span class="string">'./images/best_subset.png'</span>)</span><br></pre></td></tr></table></figure><figure><img src="ESL-Example-Prostate%20Cancer_22_0.png" alt="png"><figcaption>png</figcaption></figure><h2 id="the-ridge-regression">The ridge regression</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">U,D,V=np.linalg.svd(X_train, full_matrices=<span class="literal">False</span>)</span><br><span class="line">p=len(D)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df</span><span class="params">(D,lam)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(D**<span class="number">2</span>/(D**<span class="number">2</span>+lam))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddf</span><span class="params">(D,lam)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -sum(D**<span class="number">2</span>/(D**<span class="number">2</span>+lam)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newton</span><span class="params">(p,D)</span>:</span></span><br><span class="line">    <span class="string">"""Calculate lambdas through eÔ¨Äective degrees of freedom """</span></span><br><span class="line">    edfs=np.linspace(<span class="number">0.5</span>, p<span class="number">-0.5</span>, (p<span class="number">-1</span>)*<span class="number">10</span>+<span class="number">1</span>)</span><br><span class="line">    threshold=<span class="number">1e-3</span></span><br><span class="line">    lambdas=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> edfs:</span><br><span class="line">        lam0=(p-i)/i</span><br><span class="line">        lam1=<span class="number">1e6</span></span><br><span class="line">        diff=lam1-lam0</span><br><span class="line">        <span class="keyword">while</span> diff&gt;threshold:</span><br><span class="line">            lam1=lam0-(df(D,lam0)-i)/ddf(D,lam0)</span><br><span class="line">            diff=lam1-lam0</span><br><span class="line">            lam0=lam1</span><br><span class="line">            </span><br><span class="line">        lambdas.append(lam1)</span><br><span class="line">    lambdas.append(<span class="number">0</span>)</span><br><span class="line">    edfs=np.concatenate(([<span class="number">0</span>],edfs,[p]))</span><br><span class="line">    <span class="keyword">return</span> edfs,np.array(lambdas)</span><br><span class="line"></span><br><span class="line">edfs,lambdas=newton(p,D)</span><br><span class="line">beta_ridge=[np.zeros(p)]</span><br><span class="line"><span class="keyword">for</span> lam <span class="keyword">in</span> lambdas:</span><br><span class="line">    beta=V.T.dot(np.diag(D/(D**<span class="number">2</span>+lam))).dot(U.T).dot(y_train)</span><br><span class="line">    beta_ridge.append(beta)</span><br><span class="line">beta_ridge=np.array(beta_ridge)</span><br><span class="line">plt.plot(edfs,beta_ridge, <span class="string">'o-'</span>, markersize=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">'df(lambda)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'beta_ridge'</span>)</span><br><span class="line">plt.legend(X.columns)</span><br><span class="line">plt.title(<span class="string">'ProÔ¨Åles of ridge coeÔ¨Écients'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="ESL-Example-Prostate%20Cancer_25_0.png" alt="png"><figcaption>png</figcaption></figure><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; scipy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; scipy.stats&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;%matplotlib inline&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;data=pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;./data/prostate.data&#39;&lt;/span&gt;,delimiter=&lt;span class=&quot;string&quot;&gt;&#39;\t&#39;&lt;/span&gt;,index_col=&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;data.head()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Study Note: Assessing Model Accuracy</title>
    <link href="https://nancyyanyu.github.io/posts/86b37baa/"/>
    <id>https://nancyyanyu.github.io/posts/86b37baa/</id>
    <published>2019-10-19T23:03:53.783Z</published>
    <updated>2019-10-19T23:20:39.552Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>no free lunch in statistics</em></strong>: no one method dominates all others over all possible data sets.</p><ul><li>Explanation: On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results.</li></ul><h1 id="measuring-the-quality-of-fit">Measuring the Quality of Fit</h1><h2 id="mean-squared-error-mse">mean squared error (MSE)</h2><p><span class="math display">\[\begin{align}MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2\end{align}\]</span></p><p><strong><em>overfitting</em></strong>: When a given method yields a small training MSE but a large test MSE.</p><ul><li>Explanation: a less flexible model would have yielded a smaller test MSE. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f.</li></ul><a id="more"></a><p><img src="./1.png" width="500"></p><h1 id="the-bias-variance-trade-off">The Bias-Variance Trade-Off</h1><h2 id="decomposition">Decomposition</h2><p>The expected test MSE, for a given value <span class="math inline">\(x_0\)</span> can always be decomposed into the sum of three fundamental quantities: <strong>the variance of <span class="math inline">\(\hat{f}(x_0)\)</span>, the squared bias of <span class="math inline">\(\hat{f}(x_0)\)</span>, and the variance of the error variance terms <span class="math inline">\(\epsilon\)</span>.</strong> <span class="math display">\[\begin{align}E(y_0-\hat{f}(x_0))^2=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)\end{align}\]</span> The overall expected test MSE can be computed by averaging <span class="math inline">\(E(y_0-\hat{f}(x_0))^2\)</span> over all possible values of x0 in the test set.</p><h3 id="variance">Variance</h3><p><strong><em>Variance</em></strong>: refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set. <strong><em>more flexible statistical methods have higher variance</em></strong> - Explanation: different training data sets will result in a different <span class="math inline">\(\hat{f}\)</span>. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in <span class="math inline">\(\hat{f}\)</span></p><h3 id="bias">Bias</h3><p><strong><em>bias</em></strong>: refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. - Explanation: As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.</p><p><img src="./2.png" width="600"> <img src="./4.png" width="400"></p><hr><h3 id="math-explanation">Math Explanation</h3><p><strong>Math Explanation</strong>: If we assume that <span class="math inline">\(Y=f(X)+\epsilon\)</span> where <span class="math inline">\(E(\epsilon)=0\)</span>, and <span class="math inline">\(Var(\epsilon)=\sigma^2_\epsilon\)</span>, we can derive an expression for the expected prediction error of a regression fit <span class="math inline">\(\hat{f}(X)\)</span> at an input point X = x0, using squared-error loss: <span class="math display">\[\begin{align}Err(x_0)&amp;=E[(Y-\hat{f}(x_0))^2|X=x_0] \\&amp;=E[(f(x_0)+\epsilon-\hat{f}(x_0))^2] \\&amp;=E[\epsilon^2+(f(x_0)-\hat{f}(x_0))^2+2\epsilon(f(x_0)-\hat{f}(x_0))] \\&amp;=\sigma^2_\epsilon+E[f(x_0)^2+\hat{f}(x_0)^2-2f(x_0)\hat{f}(x_0)] \\&amp;=\sigma^2_\epsilon+E[\hat{f}(x_0)^2]+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)]  \\&amp;=\sigma^2_\epsilon+(E[\hat{f}(x_0)])^2+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)] +E[\hat{f}(x_0)^2]-(E[\hat{f}(x_0))^2 \\&amp;=\sigma^2_\epsilon+(E\hat{f}(x_0)-f(x_0))^2+Var(\hat{f}(x_0))\\&amp;=\sigma^2_\epsilon+Bias^2(\hat{f}(x_0))+Var(\hat{f}(x_0))\\&amp;= Irreducible Error+ Bias^2 + Variance\end{align}\]</span></p><ol type="1"><li>The first term is the variance of the target around its true mean f(x0), and cannot be avoided no matter how well we estimate f(x0), unless <span class="math inline">\(\sigma^2_\epsilon=0\)</span></li><li>The second term is the squared bias, the amount by which the average of our estimate differs from the true mean</li><li>The last term is the variance; the expected squared deviation of <span class="math inline">\(\hat{f}(x_0)\)</span> around its mean.</li></ol><blockquote><p>Typically the more complex we make the model <span class="math inline">\(\hat{f}\)</span>, the lower the (squared) bias but the higher the variance.</p></blockquote><h1 id="the-classification-setting">The Classification Setting</h1><p><strong><em>training error rate</em></strong>Ôºö <span class="math inline">\(\frac{1}{n}\sum_{i=1}^nI(y_i\neq\hat{y}_i)\)</span></p><p>Here <span class="math inline">\(\hat{y}_i\)</span> is the predicted class label for the ith observation using <span class="math inline">\(\hat{f}\)</span></p><p><strong><em>test error rate</em></strong>Ôºö <span class="math inline">\(Ave (I(y_0 \neq \hat{y}_0))\)</span></p><p>where <span class="math inline">\(\hat{y}_0\)</span> is the predicted class label that results from applying the classifier to the test observation with predictor x0. A good classifier is one for which the test error is smallest.</p><h2 id="the-bayes-classifier">The Bayes Classifier</h2><p><strong>Bayes classifier</strong>: <span class="math inline">\(Pr(Y=j|X=x_0)\)</span> - <strong>Explanation</strong>: The test error rate given in <span class="math inline">\(Ave (I(y_0 \neq \hat{y}_0))\)</span> is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. - <strong>Example</strong>: In a two-class problem where there are only two possible response values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if <span class="math inline">\(Pr(Y=1|X=x_0)\)</span>&gt; 0.5, and class two otherwise. - <strong>Disadvantage</strong>: For real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods</p><p><strong>Bayes error rate</strong>: <span class="math inline">\(1-E\left(\max_jPr(Y=j|X)\right)\)</span></p><p>Since the Bayes classifier will always choose the class Bayes error <span class="math inline">\(Pr(Y=j|X=x_0)\)</span> is largest, the error rate at X = x0 will be <span class="math inline">\(1-\max_jPr(Y=j|X)\)</span></p><h2 id="k-nearest-neighbors">K-Nearest Neighbors</h2><p><strong>K-nearest neighbors (KNN) classifier</strong>: 1. Given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0\)</span>, the KNN classifier first identifies the neighbors <span class="math inline">\(K\)</span> points in the training data that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(N_0\)</span>.</p><ol start="2" type="1"><li><p>It then estimates the conditional probability for class <span class="math inline">\(j\)</span> as the fraction of points in <span class="math inline">\(N_0\)</span> whose response values equal <span class="math inline">\(j\)</span>: <span class="math display">\[  \begin{align}  Pr(Y=j|X=x_0)=\frac{1}{K}\sum_{i\in N_0} I(y_i=j)  \end{align}  \]</span></p></li><li><p>Finally, KNN applies Bayes rule and classifies the test observation <span class="math inline">\(x_0\)</span> to the class with the largest probability.</p></li></ol><p><img src="./3.png" width="600"></p><p>As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.</p><p>As we use more flexible classification methods, the training error rate will decline but the test error rate may not.</p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;no free lunch in statistics&lt;/em&gt;&lt;/strong&gt;: no one method dominates all others over all possible data sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation: On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;measuring-the-quality-of-fit&quot;&gt;Measuring the Quality of Fit&lt;/h1&gt;
&lt;h2 id=&quot;mean-squared-error-mse&quot;&gt;mean squared error (MSE)&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;overfitting&lt;/em&gt;&lt;/strong&gt;: When a given method yields a small training MSE but a large test MSE.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explanation: a less flexible model would have yielded a smaller test MSE. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Assessment" scheme="https://nancyyanyu.github.io/tags/Model-Assessment/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Bias, Variance and Model Complexity</title>
    <link href="https://nancyyanyu.github.io/posts/2a71b2a0/"/>
    <id>https://nancyyanyu.github.io/posts/2a71b2a0/</id>
    <published>2019-10-19T23:03:44.786Z</published>
    <updated>2019-10-19T23:20:36.068Z</updated>
    
    <content type="html"><![CDATA[<h1 id="bias-variance-and-model-complexity">Bias, Variance and Model Complexity</h1><p><strong>Test error</strong> (generalization error): the prediction error over an independent test sample <span class="math display">\[ùê∏ùëüùëüùúè=ùê∏[ùêø(ùëå,\hat{f} (ùëã))|ùúè]\]</span> Here the training set <span class="math inline">\(\tau\)</span> is fixed, and test error refers to the error for this specific training set.</p><a id="more"></a><p><strong>Expected test error: </strong> <span class="math display">\[Err=E[L(Y,\hat{f}(X)]=E[Err_\tau]\]</span></p><p>This expectation averages over everything that is random, including the randomness in the training set that produced <span class="math inline">\(\hat{f}\)</span></p><p><strong>Training error</strong>: the average loss over the training sample <span class="math display">\[\bar{err}=\frac{1}{N}\sum_{i=1}^NL(y_i,\hat{f}(x_i))\]</span></p><p><img src="./bv.PNG" width="370"></p><p><strong>Model selection:</strong> estimating the performance of different models in order to choose the best one.</p><p><strong>Model assessment:</strong> having chosen a final model, estimating its prediction error (generalization error) on new data.</p><p>Randomly divide the dataset into three parts: - a <strong>training set</strong>: fit the models - a <strong>validation set</strong>: estimate prediction error for model selection - a <strong>test set</strong>: assessment of the generalization error of the nal chosen model</p><p>A typical split might be 50% for training, and 25% each for validation and testing:</p><h1 id="the-bias-variance-decomposition">The Bias Variance Decomposition</h1><h2 id="general-model">General Model</h2><p>If we assume that <span class="math inline">\(Y=f(X)+\epsilon\)</span> where <span class="math inline">\(E(\epsilon)=0\)</span>, and <span class="math inline">\(Var(\epsilon)=\sigma^2_\epsilon\)</span>, we can derive an expression for the expected prediction error of a regression fit <span class="math inline">\(\hat{f}(X)\)</span> at an input point X = x0, using squared-error loss:</p><p><span class="math display">\[\begin{align}Err(x_0)&amp;=E[(Y-\hat{f}(x_0))^2|X=x_0] \\&amp;=E[(f(x_0)+\epsilon-\hat{f}(x_0))^2] \\&amp;=E[\epsilon^2+(f(x_0)-\hat{f}(x_0))^2+2\epsilon(f(x_0)-\hat{f}(x_0))] \\&amp;=\sigma^2_\epsilon+E[f(x_0)^2+\hat{f}(x_0)^2-2f(x_0)\hat{f}(x_0)] \\&amp;=\sigma^2_\epsilon+E[\hat{f}(x_0)^2]+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)]  \\&amp;=\sigma^2_\epsilon+(E[\hat{f}(x_0)])^2+f(x_0)^2-2f(x_0)E[\hat{f}(x_0)] +E[\hat{f}(x_0)^2]-(E[\hat{f}(x_0))^2 \\&amp;=\sigma^2_\epsilon+(E\hat{f}(x_0)-f(x_0))^2+Var(\hat{f}(x_0))\\&amp;=\sigma^2_\epsilon+Bias^2(\hat{f}(x_0))+Var(\hat{f}(x_0))\\&amp;= Irreducible Error+ Bias^2 + Variance\end{align}\]</span></p><ol type="1"><li>The first term is the variance of the target around its true mean f(x0), and cannot be avoided no matter how well we estimate f(x0), unless <span class="math inline">\(\sigma^2_\epsilon=0\)</span></li><li>The second term is the squared bias, the amount by which the average of our estimate differs from the true mean</li><li>The last term is the variance; the expected squared deviation of <span class="math inline">\(\hat{f}(x_0)\)</span> around its mean.</li></ol><blockquote><p>Typically the more complex we make the model <span class="math inline">\(\hat{f}\)</span>, the lower the (squared) bias but the higher the variance.</p></blockquote><h2 id="knn-regression">KNN regression</h2><p>For the k-nearest-neighbor regression t, these expressions have the sim- ple form <span class="math display">\[\begin{align}Err(x_0)&amp;=E[(Y-\hat{f}_k(x_0))^2|X=x_0] \\\end{align}\]</span> <img src="./bv2.PNG" width="470"></p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;bias-variance-and-model-complexity&quot;&gt;Bias, Variance and Model Complexity&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Test error&lt;/strong&gt; (generalization error): the prediction error over an independent test sample &lt;span class=&quot;math display&quot;&gt;\[
ùê∏ùëüùëüùúè=ùê∏[ùêø(ùëå,\hat{f} (ùëã))|ùúè]
\]&lt;/span&gt; Here the training set &lt;span class=&quot;math inline&quot;&gt;\(\tau\)&lt;/span&gt; is fixed, and test error refers to the error for this specific training set.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Assessment" scheme="https://nancyyanyu.github.io/tags/Model-Assessment/"/>
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Dimension Reduction - PCA, PCR</title>
    <link href="https://nancyyanyu.github.io/posts/cac93a23/"/>
    <id>https://nancyyanyu.github.io/posts/cac93a23/</id>
    <published>2019-10-19T23:03:29.340Z</published>
    <updated>2019-10-19T23:20:56.964Z</updated>
    
    <content type="html"><![CDATA[<h1 id="dimension-reduction-methods">Dimension Reduction Methods</h1><p>Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.</p><p>Dimension Reduction Methods <strong><em>transform</em></strong> the predictors and then fit a least squares model using the transformed variables.</p><h2 id="approach">Approach</h2><p>Let <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span> represent <span class="math inline">\(M &lt; p\)</span> linear combinations of our original <span class="math inline">\(p\)</span> predictors. That is,</p><p><span class="math display">\[\begin{align}Z_m=\sum_{j=1}^p\phi_{jm}X_j\end{align}\]</span> <a id="more"></a></p><p>for some constants <span class="math inline">\(œÜ_{1m}, œÜ_{2m} . . . , œÜ_{pm}, m = 1, . . .,M.\)</span> We can then fit the linear regression model <span class="math display">\[\begin{align}y_i=\theta_0+\sum_{m=1}^M\theta_m z_{im}+\epsilon_i  \quad  i=1,2,3,4,...,n\end{align}\]</span> <strong>Dimension reduction</strong>: reduces the problem of estimating the <span class="math inline">\(p+1\)</span> coefficients <span class="math inline">\(Œ≤_0, Œ≤_1, . . . , Œ≤_p\)</span> to the simpler problem of estimating the <span class="math inline">\(M + 1\)</span> coefficients <span class="math inline">\(Œ∏_0, Œ∏_1, . . . , Œ∏_M\)</span>, where M &lt; p. In other words, the dimension of the problem has been reduced from <span class="math inline">\(p + 1\)</span> to <span class="math inline">\(M + 1\)</span>. <span class="math display">\[\begin{align}\sum_{m=1}^M\theta_m z_{im}&amp;=\sum_{m=1}^M\theta_m \sum_{j=1}^p\phi_{jm}x_{ij}=\sum_{m=1}^M\sum_{j=1}^p\theta_m \phi_{jm}x_{ij}=\sum_{j=1}^p \beta_jx_{ij}  \\\beta_j&amp;=\sum_{m=1}^M\theta_m \phi_{jm}\end{align}\]</span> <strong>All dimension reduction methods work in two steps:</strong></p><ol type="1"><li>The transformed predictors <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>are obtained.</li><li>The model is fit using these <span class="math inline">\(M\)</span> predictors. However, the choice of <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>, or equivalently, the selection of the <span class="math inline">\(œÜ_{jm}\)</span>‚Äôs, can be achieved in different ways.</li></ol><h1 id="principal-components-regression">Principal Components Regression</h1><h2 id="an-overview-of-principal-components-analysis">An Overview of Principal Components Analysis</h2><p><strong>Principal component analysis (PCA)</strong> refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data.</p><ul><li>PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).</li></ul><h2 id="what-are-principal-components">What Are Principal Components?</h2><p><strong>PCA</strong> :finds a low-dimensional representation of a data set that contains as much as possible of the <strong>variation</strong></p><p>Each of the dimensions found by PCA is a linear combination of the <span class="math inline">\(p\)</span> features.</p><p><strong><em>The first principal component</em></strong> of a set of features <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> is the normalized linear combination of the features <span class="math display">\[\begin{align}Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p\end{align}\]</span> that has the <strong>largest variance</strong>.</p><p><strong>Normalized</strong>: <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p><p><strong>Loadings</strong>: <span class="math inline">\(\phi_{11}, . . . , \phi_{p1}\)</span> the loadings of the first principal component;</p><ul><li>Together, the loadings make up the principal component loading vector, <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span></li></ul><h3 id="st-principal-component">1st Principal Component</h3><h4 id="interpretation-1-greatest-variability">Interpretation 1: greatest variability</h4><p><strong>The first principal component</strong> direction of the data: is that along which the observations <strong>vary the most</strong>.</p><p><img src="./7.png" width="600"></p><p>The first principal component direction is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance</p><p><img src="./8.png" width="600"></p><p>The first principal component is given by the formula</p><p><span class="math display">\[\begin{align}Z_1 = 0.839 √ó (pop ‚àí \bar{pop}) + 0.544 √ó (ad ‚àí \bar{ad})\end{align}\]</span> Here <span class="math inline">\(œÜ_{11} = 0.839\)</span> and <span class="math inline">\(œÜ_{21} = 0.544\)</span> are the <strong>principal component loadings</strong>, which define the direction referred to above.</p><blockquote><p>The idea is that out of every possible linear combination of pop and ad such that <span class="math inline">\(\phi_{11}^2+\phi_{21}^2=1\)</span>, this particular linear combination yields the highest variance: i.e. this is the linear combination for which <span class="math inline">\(Var(œÜ_{11} √ó (pop ‚àí \bar{pop}) + œÜ_{21} √ó (ad ‚àí \bar{ad}))\)</span> is maximized.</p></blockquote><p><strong>Principal Component Scores</strong></p><p>The values of <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> are known as the <strong>principal component scores</strong>, and can be seen in the right-hand panel of Figure 6.15. For example, <span class="math display">\[\begin{align}z_{i1} = 0.839 √ó (pop_i ‚àí \bar{pop}) + 0.544 √ó (ad_i ‚àí \bar{ad})\end{align}\]</span></p><h4 id="interpretation-2-closest-to-data">Interpretation 2: closest to data</h4><p>There is also another interpretation for PCA: the first principal component vector defines the line that is as close as possible to the data.</p><p>In Figure 6.14, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line.</p><p>In the right-hand panel of Figure 6.15, the left-hand panel has been rotated so that the first principal component direction coincides with the x-axis. It is possible to show that the <strong><em>first principal component score</em></strong> for the ith observation is the distance in the <span class="math inline">\(x\)</span>-direction of the ith cross from zero.</p><h4 id="interpretation-3-single-number-summarization">Interpretation 3: single number summarization</h4><p>We can think of the values of the principal component <span class="math inline">\(Z_1\)</span> as single number summaries of the joint pop and ad budgets for each location.</p><p>In this example, if <span class="math inline">\(z_{i1} = 0.839 √ó (pop_i ‚àí pop) + 0.544 √ó (ad_i ‚àí ad) &lt; 0\)</span>, then this indicates a city with below-average population size and belowaverage ad spending.</p><p><img src="./9.png" width="650"></p><p>Figure 6.16 displays <span class="math inline">\(z_{i1}\)</span> versus both pop and ad. The plots show a strong relationship between the first principal component and the two features. In other words, the first principal component appears to <em>capture most of the information</em> contained in the pop and ad predictors.</p><h4 id="compute-the-first-principal-component">Compute the first principal component</h4><ul><li><p>Assume that each of the variables in <span class="math inline">\(X\)</span> has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form <span class="math display">\[\begin{align}z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+,,,+\phi_{p1}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span> that has largest sample variance, subject to the constraint that <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p></li><li><p>The first principal component loading vector solves the optimization problem <span class="math display">\[\begin{align}\max_{\phi_{11},...,\phi_{p1}}{\left\{ \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j1}x_{ij}   \right)^2 \right\}} \, subject \, to \, \sum_{j=1}^p \phi_{j1}^2=1\end{align}\]</span></p></li><li><p>Since <span class="math inline">\(\sum_{i=1}^nx_{ij}/n=1\)</span>, the average of the <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> will be zero as well. Hence the objective that we are maximizing is just the <strong>sample variance</strong> of the <span class="math inline">\(n\)</span> values of zi1</p></li><li><p><strong>Scores</strong>: We refer to <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> as the scores of the first principal component.</p></li></ul><p><strong>Geometric interpretation</strong>: for the first principal component: The loading vector <span class="math inline">\(\phi_1\)</span> with elements <span class="math inline">\(\phi_{11},\phi_{21},...,\phi_{p1}\)</span> defines a direction in feature space along which the data <strong>vary the most</strong>. If we project the n data points <span class="math inline">\(x_1, . . . , x_n\)</span> onto this direction, the projected values are the principal component scores <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> themselves.</p><h3 id="nd-principal-component">2nd Principal Component</h3><p>The s<strong>econd principal component <span class="math inline">\(Z_2\)</span></strong> is a linear combination of the variables that is uncorrelated with <span class="math inline">\(Z_1\)</span>, and has largest variance subject to this constraint.</p><p>It turns out that the zero correlation condition of <span class="math inline">\(Z_1\)</span> with <span class="math inline">\(Z_2\)</span> is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction.</p><p>The second principal component is given by the formula:</p><p><span class="math display">\[\begin{align}Z_2 = 0.544 √ó (pop ‚àí \bar{pop}) ‚àí 0.839 √ó (ad ‚àí \bar{ad}).\end{align}\]</span> Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that this component captures far less information.</p><h4 id="compute-the-second-principal-component">Compute the second principal component</h4><p><strong>The second principal component <span class="math inline">\(Z_2\)</span></strong>: the linear combination of <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> that has maximal variance out of all linear combinations that are <strong>uncorrelated with <span class="math inline">\(Z_1\)</span></strong>.</p><p>The second principal component scores <span class="math inline">\(z_{12}, . . . , z_{n2}\)</span> take the form <span class="math display">\[\begin{align}z_{i2}=\phi_{12}x_{i1}+\phi_{22}x_{i2}+,,,+\phi_{p2}x_{ip} \quad \quad i=1,2,...,n\end{align}\]</span> where <span class="math inline">\(\phi_2\)</span> is the second principal component <strong>loading</strong> vector, with elements <span class="math inline">\(\phi_{12},\phi_{22},...,\phi_{p2}\)</span>.</p><p>It turns out that constraining <span class="math inline">\(Z_2\)</span> to be uncorrelated with <span class="math inline">\(Z_1\)</span> is equivalent to constraining the direction <span class="math inline">\(\phi_2\)</span> to be <strong>orthogonal</strong> (perpendicular) to the direction <span class="math inline">\(\phi_1\)</span>.</p><p>To find <span class="math inline">\(\phi_2\)</span>, we solve a problem similar to (10.3) with <span class="math inline">\(\phi_2\)</span> replacing <span class="math inline">\(\phi_1\)</span>, and with the additional constraint that <span class="math inline">\(\phi_2\)</span> is orthogonal to <span class="math inline">\(\phi_1\)</span></p><p><img src="./1_v2.png" width="600"></p><p><strong>Interpretation:</strong></p><ul><li>1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes.</li><li>Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three.</li><li>This indicates that the crime-related variables are correlated with each other‚Äîstates with high murder rates tend to have high assault and rape rates‚Äîand that the UrbanPop variable is less correlated with the other three.</li></ul><h2 id="another-interpretation-of-principal-components">Another Interpretation of Principal Components</h2><p><strong>An alternative interpretation for principal components</strong>: principal components provide low-dimensional linear surfaces that are closest to the observations</p><ul><li><p><strong>The first principal component loading vector has a very special property</strong>: it is the line in p-dimensional space that is closest to the n observations (using average squared Euclidean distance as a measure of closeness).</p></li><li><p>The appeal of this interpretation : we seek a single dimension of the data that lies as close as possible to all of the data points, since such a line will likely provide a good summary of the data.</p></li><li><p><strong>The first two principal components</strong> of a data set <strong>span the plane</strong> that is closest to the n observations, in terms of average squared Euclidean distance</p></li><li><p>Together <strong>the first M principal component</strong> score vectors and the first M principal component loading vectors provide the best M-dimensional approximation (in terms of Euclidean distance) to the ith observation <span class="math inline">\(x_{ij}\)</span> . <span class="math display">\[\begin{align}x_{ij} \approx \sum_{m=1}^Mz_{im}\phi_{jm}\end{align}\]</span> (assuming the original data matrix X is column-centered).</p></li><li><p>When <span class="math inline">\(M = min(n ‚àí 1, p)\)</span>, then the representation is exact: <span class="math inline">\(x_{ij} = \sum_{m=1}^Mz_{im}\phi_{jm}\)</span></p></li></ul><h2 id="more-on-pca">More on PCA</h2><h3 id="scaling-the-variables">Scaling the Variables</h3><p>Before PCA is performed, the variables should be <strong>centered to have mean zero</strong>. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been <strong>individually scaled</strong> (each multiplied by a different constant)</p><p><img src="./2_v2.png" width="600"></p><h3 id="uniqueness-of-the-principal-components">Uniqueness of the Principal Components</h3><p><strong>Each principal component loading vector <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span> and the score vectors <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> is unique, up to a sign flip. </strong></p><ul><li>Two different software packages will yield the same principal component loading vectors and score vectors, although the signs of those loading vectors may differ.</li><li><strong>The signs may differ</strong> because each principal component loading vector specifies a direction in p-dimensional space: flipping the sign has no effect as the direction does not change.</li></ul><h3 id="the-proportion-of-variance-explained">The Proportion of Variance Explained</h3><p><strong>How much of the variance in the data is not contained in the first few principal components?</strong></p><p><strong>Proportion of variance explained (PVE)</strong> by each principal component:</p><ul><li>The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as</li></ul><p><span class="math display">\[\begin{align}\sum_{j=1}^pVar(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^nx_{ij}^2\end{align}\]</span></p><ul><li>The variance explained by the mth principal component is</li></ul><p><span class="math display">\[\begin{align}\frac{1}{n}\sum_{i=1}^nz_{im}^2=\frac{1}{n}\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2\end{align}\]</span></p><ul><li>Therefore, the <strong>PVE of the mth principal component</strong> is given by</li></ul><p><span class="math display">\[\begin{align}\frac{\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2}\end{align}\]</span></p><p>The PVE of each principal component is a positive quantity. In order to compute the <strong>cumulative PVE</strong> of the first <span class="math inline">\(M\)</span> principal components, we can simply sum (10.8) over each of the first <span class="math inline">\(M\)</span> PVEs. In total, there are <span class="math inline">\(min(n ‚àí 1, p)\)</span> principal components, and their PVEs sum to one.</p><p><img src="./3_v2.png" width="600"></p><h3 id="deciding-how-many-principal-components-to-use">Deciding How Many Principal Components to Use</h3><p>We would like to use the smallest number of principal components required to get a good understanding of the data.</p><p><strong>How many principal components are needed?</strong></p><ul><li>We typically decide on the number of principal components required to visualize the data by examining a <strong>scree plot</strong> (Right FIGURE 10.4)</li><li>We choose the smallest number of principal components that are required in order to explain a sizable amount of the variation in the data.</li><li>We tend to look at the first few principal components in order to find interesting patterns in the data. If no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest.</li></ul><h2 id="the-principal-components-regression-approach">The Principal Components Regression Approach</h2><p>The principal components regression (PCR) approach involves constructing the first M principal components, <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>, and then using these components as the predictors in a linear regression model that is fit using least squares</p><p><strong>The key idea</strong></p><p>Often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that <strong><em>the directions in which <span class="math inline">\(X_1, . . .,X_p\)</span> show the most variation are the directions that are associated with <span class="math inline">\(Y\)</span></em></strong></p><p><strong>Example</strong>:</p><p><img src="./10.png" width="650"></p><ul><li>Performing PCR with an appropriate choice of M can result in a substantial improvement over least squares</li><li>PCR does not perform as well as the two shrinkage methods<ul><li><strong>Reason</strong>: The data were generated in such a way that many principal components are required in order to adequately model the response. In contrast, PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response.</li></ul></li></ul><p><strong>Note</strong>: even though PCR provides a simple way to perform regression using <span class="math inline">\(M &lt; p\)</span> predictors, it is not a <em>feature selection</em> method!</p><ul><li>This is because each of the <span class="math inline">\(M\)</span> principal components used in the regression is a linear combination of all p of the original features.</li><li>PCR is more closely related to ridge regression than to the lasso. One can even think of ridge regression as a continuous version of PCR!</li></ul><p><strong>Cross-validation</strong>: In PCR, the number of principal components, <span class="math inline">\(M\)</span>, is typically chosen by cross-validation.</p><p><img src="./11.png" width="650"></p><p><strong>Standardisation</strong>: When performing PCR, we generally recommend standardizing each predictor, prior to generating the principal components. - In the absence of standardization, the <em>high-variance variables</em> will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model.</p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;dimension-reduction-methods&quot;&gt;Dimension Reduction Methods&lt;/h1&gt;
&lt;p&gt;Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.&lt;/p&gt;
&lt;p&gt;Dimension Reduction Methods &lt;strong&gt;&lt;em&gt;transform&lt;/em&gt;&lt;/strong&gt; the predictors and then fit a least squares model using the transformed variables.&lt;/p&gt;
&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&quot;math inline&quot;&gt;\(Z_1,Z_2, . . . ,Z_M\)&lt;/span&gt; represent &lt;span class=&quot;math inline&quot;&gt;\(M &amp;lt; p\)&lt;/span&gt; linear combinations of our original &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; predictors. That is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
\]&lt;/span&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
      <category term="PCA" scheme="https://nancyyanyu.github.io/tags/PCA/"/>
    
      <category term="Dimension Reduction" scheme="https://nancyyanyu.github.io/tags/Dimension-Reduction/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Decision Trees, Random Forest, and Boosting</title>
    <link href="https://nancyyanyu.github.io/posts/6b588a86/"/>
    <id>https://nancyyanyu.github.io/posts/6b588a86/</id>
    <published>2019-10-19T23:02:53.811Z</published>
    <updated>2019-10-19T23:20:48.887Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction-to-descision-tree">Introduction to Descision Tree</h1><h2 id="regression-trees">Regression Trees</h2><h3 id="predicting-baseball-players-salaries-using-regression-trees">Predicting Baseball Players‚Äô Salaries Using Regression Trees</h3><p><strong>Terminal nodes</strong>: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.</p><p><strong>Internal nodes</strong>: The points along the tree where the predictor space is split are referred to as internal nodes.</p><p><strong>Branches</strong>: The segments of the trees that connect the nodes as branches</p><a id="more"></a><p><img src="./2.png" width="500"> <img src="./1.png" width="500"></p><h3 id="prediction-via-stratification-of-the-feature-space">Prediction via Stratification of the Feature Space</h3><p><strong>Process of building a regression tree</strong></p><p><strong>Step 1</strong>: We divide the predictor space‚Äîthat is, the set of possible values for X1,X2, . . .,Xp‚Äîinto J distinct and non-overlapping regions, R1,R2, . . . , RJ .</p><p><strong>Step 2</strong>: For every observation that falls into the region Rj, we make the same prediction, which is simply the <em>mean of the response values</em> for the training observations in Rj .</p><h4 id="step-1">Step 1</h4><p><strong>How do we construct the regions R1, . . .,RJ?</strong></p><ul><li><p>We choose to divide the predictor space into high-dimensional rectangles, or <strong>boxes</strong>, for ease of interpretation of the resulting predictive model.</p></li><li><p>The goal is to find boxes R1, . . . , RJ that <strong>minimize the RSS</strong>, given by <span class="math display">\[\begin{align}\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2\end{align}\]</span></p><p>where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean response for the training observations within the jth box.</p></li></ul><p><strong>Recursive Binary Splitting</strong>: a <em>top-down, greedy</em> approach - <strong>Top-down</strong>: begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. - <strong>Greedy</strong>: at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</p><p><strong>Methods</strong>: 1. Select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into the regions <span class="math inline">\({X|X_j &lt; s}\)</span> and <span class="math inline">\({X|X_j ‚â• s}\)</span> leads to the greatest possible reduction in RSS - In greater detail, for any <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>, we define the pair of half-planes <span class="math display">\[  \begin{align}  R_1(j, s) = {X|X_j &lt; s} ,\quad R_2(j, s) = {X|X_j ‚â• s}  \end{align}\]</span> and we seek the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that <strong>minimize</strong> the equation <span class="math display">\[  \begin{align}  \sum_{:x_i \in R_1(j,s)}(y_i-\hat{y}_{R_1})^2+\sum_{:x_i \in R_2(j,s)}(y_i-\hat{y}_{R_2})^2  \end{align}\]</span> where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><p>where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><p>and we seek the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that <strong>minimize</strong> the equation <span class="math display">\[  \begin{align}  \sum_{:x_i \in R_1(j,s)}(y_i-\hat{y}_{R_1})^2+\sum_{:x_i \in R_2(j,s)}(y_i-\hat{y}_{R_2})^2  \end{align}\]</span> where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><p>where <span class="math inline">\(\hat{y}_{R_1}\)</span>is the mean response for the training observations in <span class="math inline">\(R_1(j, s)\)</span>,</p><ol start="2" type="1"><li>Repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.</li></ol><ul><li><p>However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions.</p></li><li><p>We now have three regions. Again, we look to split one of these three regions further, so as to minimize the RSS.</p></li></ul><ol start="3" type="1"><li>The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.</li></ol><p><img src="./3.png" width="600"></p><h4 id="step-2">Step 2</h4><p>Predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.</p><h3 id="tree-pruning">Tree Pruning</h3><p>A better strategy is to grow a very large tree <span class="math inline">\(T_0\)</span>, and then <strong>prune</strong> it back in order to obtain a <strong>subtree</strong></p><h4 id="cost-complexity-pruning">Cost complexity pruning</h4><p>a.k.a.: <strong>weakest link pruning</strong></p><p>Consider a sequence of trees indexed by a nonnegative tuning parameter Œ±</p><p><img src="./4.png" width="600"></p><p>For each value of Œ± there corresponds a subtree <span class="math inline">\(T ‚äÇ T_0\)</span> such that</p><p><span class="math display">\[\begin{align}\sum_{m=1}^T\sum_{i:x_i \in R_m}(y_i ‚àí \hat{y}_{R_m})^2 + \alpha|T|  \quad \quad (8.4)\end{align}\]</span> is as small as possible.</p><ul><li><span class="math inline">\(|T|\)</span>: the number of terminal nodes of the tree T ,</li><li><span class="math inline">\(R_m\)</span>: the rectangle (i.e. the subset of predictor space) corresponding to the m-th <strong>terminal node</strong>,</li><li><span class="math inline">\(\hat{y}_{R_m}\)</span>: the predicted response associated with <span class="math inline">\(R_m\)</span>‚Äîthat is, the mean of the training observations in <span class="math inline">\(R_m\)</span>.</li></ul><p>The tuning parameter <span class="math inline">\(Œ±\)</span> controls a <em>trade-off</em> between the subtree‚Äôs <strong>complexity</strong> and its <strong>fit to the training data</strong>. When Œ± = 0, then the subtree T will simply equal T0, because then (8.4) just measures the training error. However, as Œ± increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4) will tend to be minimized for a smaller subtree.</p><p>Equation 8.4 is reminiscent of the lasso, in which a similar formulation was used in order to control the complexity of a linear model.</p><p><img src="./5.png" width="600"> <img src="./6.png" width="600"></p><h2 id="classification-trees">Classification Trees</h2><p>For a classification tree, - We predict that each observation belongs to the <strong>most commonly occurring class</strong> of training observations in the region to which it belongs. - RSS cannot be used as a criterion for making the binary splits <span class="math inline">\(\Rightarrow\)</span> <strong>classification error rate</strong>.</p><h3 id="classification-error-rate">Classification Error Rate</h3><ul><li>Since we plan to assign an observation in a given region to the most commonly occurring class of training observations in that region, the classification error rate is simply the <strong>fraction of the training observations in that region that do not belong to the most common class</strong>:</li></ul><p><span class="math display">\[\begin{align}E=1-\max_k(\hat{p}_{mk})\end{align}\]</span></p><ul><li><span class="math inline">\(\hat{p}_{mk}\)</span> : the proportion of training observations in the mth region that are from the kth class.</li><li>classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable: <strong>Gini index, cross-entropy.</strong></li></ul><h3 id="gini-index">Gini index</h3><p><span class="math display">\[\begin{align}G=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\end{align}\]</span></p><ul><li>A measure of total variance across the K classes. It is not hard to see that the Gini index takes on a small value if all of the <span class="math inline">\(\hat{p}_{mk}\)</span>‚Äôs are close to zero or one.</li><li>For this reason the Gini index is referred to as a measure of node <strong>purity</strong>‚Äîa small value indicates that a node contains predominantly observations from a single class.</li></ul><h3 id="cross-entropy">Cross-Entropy</h3><p><span class="math display">\[\begin{align}D=-\sum_{k=1}^K\hat{p}_{mk}\log{\hat{p}_{mk}}\end{align}\]</span></p><ul><li>Since 0 ‚â§ <span class="math inline">\(\hat{p}_{mk}\)</span> ‚â§ 1, it follows that <span class="math inline">\(0 ‚â§ ‚àí\hat{p}_{mk}\log{\hat{p}_{mk}}\)</span>.</li><li>Cross-entropy will take on a value near zero if the <span class="math inline">\(\hat{p}_{mk}\)</span>‚Äôs are all near zero or near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the mth node is <strong>pure</strong>.</li></ul><hr><p><strong>Cross-Entropy v.s. Gini index v.s. Classification Error Rate</strong> - When building a classification tree, either the Gini index or the crossentropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.</p><p><img src="./9.png" width="800"></p><ul><li><strong>A surprising characteristic</strong>: some of the splits yield two terminal nodes that have the same predicted value.</li><li><strong>Why is the split performed at all?</strong> The split is performed because it leads to <strong>increased node purity.</strong></li><li><strong>Why is node purity important?</strong> Suppose that we have a test observation that belongs to the region given by that right-hand leaf. Then we can be pretty certain that its response value is Yes. In contrast, if a test observation belongs to the region given by the left-hand leaf, then its response value is probably Yes, but we are much less certain. Even though the split RestECG&lt;1 does not reduce the classification error, it improves the <strong>Gini index and the cross-entropy</strong>, which are more sensitive to node purity.</li></ul><h2 id="trees-versus-linear-models">Trees Versus Linear Models</h2><p>Linear regression assumes a model of the form <span class="math display">\[\begin{align}f(X)=\beta_0+\sum_{i=1}^p\beta_iX_i\end{align}\]</span> Regression trees assume a model of the form <span class="math display">\[\begin{align}f(X)=\sum_{m=1}^Mc_m \cdot I_{X \in R_m}\end{align}\]</span> where R1, . . .,RM represent a partition of feature space</p><p>where R1, . . .,RM represent a partition of feature space</p><p><strong>Linear regression works better</strong>: If the relationship between the features and the response is well approximated by a linear model; regression tree does not exploit this linear structure.</p><p><strong>Regression tree works better</strong>: If instead there is a highly non-linear and complex relationship between the features and the response.</p><h2 id="advantages-and-disadvantages-of-trees">Advantages and Disadvantages of Trees</h2><p><strong>Advantages of decision trees for regression and classification:</strong></p><p>‚ñ≤ <strong>Interpretation</strong>: Trees are very <strong>easy to explain</strong> to people. In fact, they are even easier to explain than linear regression!</p><p>‚ñ≤ Some people believe that decision trees more closely <strong>mirror human decision-making</strong> than do the regression and classification approaches.</p><p>‚ñ≤ <strong>Visualization</strong>: Trees can be <strong>displayed graphically</strong>, and are easily interpreted even by a non-expert.</p><p>‚ñ≤ Trees can easily handle qualitative predictors without the need to create dummy variables.</p><p><strong>Disadvantages of decision trees for regression and classification:</strong></p><p>‚ñº Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.</p><h1 id="bagging">Bagging</h1><p><strong>Bootstrap aggregation</strong>, or <strong>bagging</strong>, is a general-purpose procedure for reducing the variance of a statistical learning method, frequently used in the context of decision trees.</p><p><strong>Averaging a set of observations reduces variance</strong>: Recall that given a set of n independent observations Z1, . . . , Zn, each with variance <span class="math inline">\(œÉ^2\)</span>, the variance of the mean <span class="math inline">\(\bar{Z}\)</span> of the observations is given by <span class="math inline">\(œÉ^2/n\)</span>.</p><ul><li>A natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to <strong>take many training sets from the population</strong>, build a separate prediction model using each training set, and average the resulting predictions.</li></ul><p><strong>Bootstrap</strong> taking repeated samples from the (single) training data set</p><p><strong>Bagging</strong></p><ul><li>Generate B different bootstrapped training data sets.</li><li>Train our method on the bth bootstrapped training set in order to get <span class="math inline">\(\hat{f}^{*b}(x)\)</span></li><li>Finally average all the predictions, to obtain</li></ul><p><span class="math display">\[\begin{align}\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x)\end{align}\]</span></p><p><strong>Apply bagging to regression trees</strong></p><ul><li>Construct B regression trees using B bootstrapped training sets</li><li>Average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance.</li></ul><p><strong>Bagging on Classification Tree</strong></p><ul><li>For a given test observation, we can record the class predicted by each of the B trees, and take a <strong>majority vote</strong>: the overall prediction is the most commonly occurring class among the B predictions.</li></ul><p><strong>B</strong></p><ul><li>In practice weuse a value of B sufficiently large that the error has settled down, like B=100.</li></ul><h2 id="out-of-bag-error-estimation">Out-of-Bag Error Estimation</h2><p>Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around 2/3 of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the <strong>out-of-bag (OOB)</strong> observations.</p><blockquote><p>We can predict the response for the ith observation using each of the trees inwhich that observation was OOB.</p></blockquote><ul><li>This will yield around B/3 predictions for the ith observation.</li><li>To obtain a single prediction for the ith observation, we can <strong>average</strong> these predicted responses (regression) or can take a <strong>majority vote</strong> (classification).</li><li>This leads to a single OOB prediction for the ith observation.</li></ul><p>The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which <strong>cross-validation</strong> would be computationally onerous.</p><h2 id="variable-importance-measures">Variable Importance Measures</h2><p><strong>Bagging improves prediction accuracy at the expense of interpretability</strong></p><ul><li>When we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure</li></ul><p><strong>Variable Importance</strong></p><ul><li><p>One can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees).</p></li><li><p><strong>Bagging regression trees</strong>: Record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. <span class="math display">\[\begin{align}RSS=\sum_{j=1}^J\sum_{i \in R_j} (y_i-\hat{y}_{R_j})^2\end{align}\]</span></p></li><li><p><strong>Bagging classification trees</strong>: Add up the total amount that the <strong>Gini index</strong> is decreased by splits over a given predictor, averaged over all B trees.</p></li></ul><p><img src="./11.png" width="600"></p><h1 id="random-forest">Random Forest</h1><p><strong>Random forests</strong> provide an improvement over bagged trees by way of a small tweak that <strong>decorrelates</strong> the trees.</p><p>As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, <em>a random sample of m predictors is chosen as split candidates</em> from the full set of p predictors.</p><p><strong>The split is allowed to use only one of those m predictors.</strong> A fresh sample of m predictors is taken at each split, and typically we choose <span class="math inline">\(m ‚âà\sqrt{p}\)</span></p><p><strong>Rationale</strong>:</p><ul><li>Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, <em>all of the bagged trees will look quite similar to each other.</em></li><li>Hence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities.</li></ul><p><strong>Decorrelating</strong> the trees: Random forests forces each split to consider only a subset of the predictors, making the average of the resulting trees less variable and hence more reliable.</p><h1 id="boosting">Boosting</h1><p><strong>Boosting</strong>: another approach for improving the predictions resulting from a decision tree.</p><ul><li>Trees are grown <strong>sequentially</strong>: each tree is grown using information from previously grown trees.</li><li>Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.</li></ul><p><img src="./12.png" width="600"></p><p><strong>Idea behind this procedure</strong></p><ul><li>Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead <strong>learns slowly</strong>.</li><li>Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response.</li><li>We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter <strong>d</strong> in the algorithm.</li><li>By fitting small trees to the residuals, we slowly improve <span class="math inline">\(\hat{f}\)</span> in areas where it does not perform well.</li><li>The shrinkage parameter <strong>Œª</strong> slows the process down even further, allowing more and different shaped trees to attack the residuals.</li></ul><blockquote><p>Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.</p></blockquote><p><strong>Boosting has three tuning parameters:</strong></p><ol type="1"><li>The number of trees <span class="math inline">\(B\)</span>.</li><li>The shrinkage parameter <span class="math inline">\(Œª\)</span>, a small positive number. This controls the rate at which boosting learns.</li><li>The number <span class="math inline">\(d\)</span> of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a <strong>stump</strong>, consisting of a single split. In this case, the boosted ensemble is fitting an <strong>additive model</strong>, since each term involves only a single variable. More generally <span class="math inline">\(d\)</span> is the <strong>interaction depth</strong>, and controls the interaction order of the boosted model, since <span class="math inline">\(d\)</span> splits can involve at most d variables.</li></ol><p><strong>Boosting V.S. Random forests:</strong></p><ul><li>In boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient.</li><li>Using smaller trees can aid in interpretability as well; for instance, using <strong>stumps</strong> leads to an additive model.</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction-to-descision-tree&quot;&gt;Introduction to Descision Tree&lt;/h1&gt;
&lt;h2 id=&quot;regression-trees&quot;&gt;Regression Trees&lt;/h2&gt;
&lt;h3 id=&quot;predicting-baseball-players-salaries-using-regression-trees&quot;&gt;Predicting Baseball Players‚Äô Salaries Using Regression Trees&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Terminal nodes&lt;/strong&gt;: The regions R1, R2, and R3 are known as terminal nodes or leaves of the tree.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Internal nodes&lt;/strong&gt;: The points along the tree where the predictor space is split are referred to as internal nodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Branches&lt;/strong&gt;: The segments of the trees that connect the nodes as branches&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Trees" scheme="https://nancyyanyu.github.io/tags/Trees/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Model Selection and Regularization (Ridge &amp; Lasso)</title>
    <link href="https://nancyyanyu.github.io/posts/a065f58f/"/>
    <id>https://nancyyanyu.github.io/posts/a065f58f/</id>
    <published>2019-10-19T23:02:42.273Z</published>
    <updated>2019-10-19T23:21:22.427Z</updated>
    
    <content type="html"><![CDATA[<h1 id="introduction-to-model-selection">Introduction to Model Selection</h1><p><strong>Setting:</strong></p><ul><li><p>In the regression setting, the standard linear model <span class="math inline">\(Y = Œ≤_0 + Œ≤_1X_1 + ¬∑ ¬∑ ¬∑ + Œ≤_pX_p + \epsilon\)</span></p></li><li><p>In the chapters that follow, we consider some approaches for extending the linear model framework.</p></li></ul><p><strong>Reason of using other fitting procedure than lease squares</strong>:</p><ul><li><strong><em>Prediction Accuracy:</em></strong><ul><li>Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias.</li><li>If n <span class="math inline">\(\gg\)</span> p, least squares estimates tend to also have low variance <span class="math inline">\(\Rightarrow\)</span> perform well on test data.</li><li>If n is not much larger than p, least squares fit has large variance <span class="math inline">\(\Rightarrow\)</span> overfitting <span class="math inline">\(\Rightarrow\)</span> consequently poor predictions on test data</li><li>If p &gt; n, no more unique least squares coefficient estimate: the <strong>variance is infinite</strong> so the method cannot be used at all</li></ul><p>By <strong>constraining</strong> or <strong>shrinking</strong> the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.</p></li><li><strong><em>Model Interpretability</em></strong>Ôºö<ul><li>irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables‚Äîthat is, by setting the corresponding coefficient estimates to zero‚Äîwe can obtain a model that is more easily interpreted.</li><li>least squares is extremely unlikely to yield any coefficient estimates that are exactly zero <span class="math inline">\(\Rightarrow\)</span> <strong>feature selection</strong></li></ul></li></ul><p><strong>Alternatives of lease squares:</strong></p><ol type="1"><li>Subset Selection</li><li>Shrinkage</li><li><a href="https://nancyyanyu.github.io/posts/cac93a23/">Dimension Reduction</a></li></ol><a id="more"></a><h1 id="subset-selection">Subset Selection</h1><h4 id="drawbacks-of-least-squares-estimates">Drawbacks of least squares estimates:</h4><ul><li><em>prediction accuracy</em>: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeÔ¨Écients to zero.By doing so we sacriÔ¨Åce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.</li><li><em>interpretation</em>: With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eÔ¨Äects. In order to get the ‚Äúbig picture,‚Äù we are willing to sacriÔ¨Åce some of the small details.</li></ul><h2 id="best-subset-selection">Best Subset Selection</h2><p>Best subset regression Ô¨Ånds for each k ‚àà{0, 1, 2,...,p} the subset of size k that gives smallest residual sum of squares.</p><p>We choose the smallest model that minimizes an estimate of the expected prediction error.</p><p><img src="./best_subset.png" width="600"></p><blockquote><p>FIGURE 3.5.All possible subset models for the prostate cancer example. At each subset size is shown the residual sum-of-squares for each model of that size.</p></blockquote><p><strong>Approach</strong></p><ol type="1"><li>fit a separate least squares regression best subset for each possible combination of the p predictors. That is, we fit all p models selection that contain exactly one predictor, all <span class="math inline">\(\left(\begin{array}{c}p\\ 2\end{array}\right)= p(p‚àí1)/2\)</span> models that contain exactly two predictors, and so forth.</li><li>We then look at all of the resulting models, with the goal of identifying the one that is best.</li></ol><p><img src="./1.png" width="600"></p><p><strong>Note</strong></p><ul><li><span class="math inline">\(RSS\)</span> of these p + 1 models decreases monotonically, and the <span class="math inline">\(R2\)</span> increases monotonically, as the number of features included in the models increases. Therefore, if we use these statistics to select the best model, then we will always end up with a model involving all of the variables</li><li>The problem of selecting the best model from among the <span class="math inline">\(2^p\)</span> possibilities considered by best subset selection is not trivial.</li></ul><h2 id="stepwise-selection">Stepwise Selection</h2><blockquote><p>Rather than search through all possible subsets (which becomes infeasible for p much larger than 40), we can seek a good path through them.</p></blockquote><h3 id="forward-stepwise-selection">Forward Stepwise Selection</h3><p><strong>Forward-stepwise selection</strong> starts with the intercept, and then sequentially adds into the model the predictor that most improves the Ô¨Åt.</p><p>Forward-stepwise selection is a <em>greedy algorithm</em>, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection.</p><h4 id="advantages">Advantages:</h4><ul><li><strong>Computational</strong>: for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence</li><li><strong>Statistical</strong>: a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias</li></ul><p><strong>Approach</strong></p><ol type="1"><li><strong>Forward stepwise selection</strong> begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.</li><li>In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.</li></ol><p><img src="./2.png" width="600"></p><p><strong>Forward Stepwise Selection V.S. Best Subset Selection</strong></p><ul><li>Forward stepwise selection‚Äôs computational advantage over best subset selection is clear.</li><li>Forward stepwise is not guaranteed to find the best possible model out of all <span class="math inline">\(2^p\)</span> models containing subsets of the p predictors.</li></ul><h3 id="backward-stepwise-selection">Backward Stepwise Selection</h3><p><strong>Backward-stepwise selection</strong> starts with the full model, and sequentially deletes the predictor that has the least impact on the Ô¨Åt. The candidate for dropping is the variable with the smallest Z-score</p><p><strong>Approach</strong></p><ol type="1"><li><strong>Backward Stepwise Selection</strong> begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time</li></ol><p><img src="./3.png" width="600"></p><p><strong>Backward Stepwise Selection V.S. Forward Stepwise Selection</strong>:</p><ul><li>Like forward stepwise selection, the backward selection approach searches through only 1+p(p+1)/2 models, and so can be applied in settings where p is too large to apply best subset selection.</li><li>Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the p predictors.</li><li>Backward selection requires that the number of samples n is larger than the number of variables p (so that the full model can be fit). In contrast, forward stepwise can be used even when n &lt; p, and so is the only viable subset method when p is very large.</li></ul><h3 id="hybrid-approaches">Hybrid Approaches</h3><p><strong>Approach</strong></p><ol type="1"><li>Variables are added to the model sequentially, in analogy to forward selection.</li><li>However, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit.</li></ol><p><strong>Note</strong></p><p>Such an approach attempts to more closely mimic best subset selection while retaining the computational advantages of forward and backward stepwise selection.</p><h2 id="choosing-the-optimal-model">Choosing the Optimal Model</h2><p>The training error can be a poor estimate of the test error. Therefore, RSS and R2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.</p><p><strong>2 Methods</strong>:</p><ol type="1"><li><em>indirectly</em> estimate test error by making an adjustment to the training error to account for the bias due to overfitting.</li><li><em>directly</em> estimate the test error, using either a validation set approach or a cross-validation approach</li></ol><h3 id="c_p-aic-bic-adjusted-r2"><span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span>, <span class="math inline">\(BIC\)</span>, Adjusted <span class="math inline">\(R^2\)</span></h3><ul><li>the training set MSE is generally an underestimate of the test MSE. (Recall that MSE = RSS/n.)</li><li>the training error will decrease as more variables are included in the model, but the test error may not.</li><li>Therefore, training set RSS and training set R2 cannot be used to select from among a set of models with different numbers of variables.</li></ul><h4 id="c_p"><span class="math inline">\(C_p\)</span></h4><p><span class="math inline">\(C_p\)</span> estimate of test MSE: <span class="math display">\[\begin{align}C_p=\frac{1}{n}(RSS+2d\hat{\sigma}^2)\end{align}\]</span> where <span class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the variance of the error <span class="math inline">\(\epsilon\)</span></p><p><strong>Note</strong>:</p><ul><li>The <span class="math inline">\(C_p\)</span> statistic adds a penalty of <span class="math inline">\(2d\hat{\sigma}^2\)</span> to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error.</li><li>The penalty increases as the number of predictors in the model increases; this is intended to adjust for the corresponding decrease in training RSS.</li><li>If <span class="math inline">\(\hat{\sigma}^2\)</span> is an unbiased estimate of <span class="math inline">\(\sigma^2\)</span> in, then <span class="math inline">\(C_p\)</span> is an unbiased estimate of test MSE</li><li>When determining which of a set of models is best, we choose the model with the lowest <span class="math inline">\(C_p\)</span> value.</li></ul><h4 id="aic">AIC</h4><p>The AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model <span class="math inline">\(Y = Œ≤_0 + Œ≤_1X_1 + ¬∑ ¬∑ ¬∑ + Œ≤_pX_p + \epsilon\)</span> with Gaussian errors, maximum likelihood and least squares are the same thing.</p><p>In this case AIC is given by <span class="math display">\[\begin{align}AIC=\frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2)\end{align}\]</span> For least squares models, Cp and AIC are proportional to each other</p><h4 id="bic">BIC</h4><p>For the least squares model with d predictors, the BIC is, up to irrelevant constants, given by <span class="math display">\[\begin{align}BIC=\frac{1}{n}(RSS+\log(n)d\hat{\sigma}^2)\end{align}\]</span> Since log(n) &gt; 2 for any n &gt; 7, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than Cp.</p><h4 id="adjusted-r2">Adjusted <span class="math inline">\(R^2\)</span></h4><p>Recall: <span class="math display">\[\begin{align}R^2=1 ‚àí RSS/TSS=1-\frac{RSS}{\sum(y_i-\bar{y})^2}\end{align}\]</span> <strong>TSS</strong>: total sum of squares for the response</p><p>For a least squares model with d variables, <strong>the adjusted R2</strong> statistic is calculated as <span class="math display">\[Adjusted  \, R^2=1 ‚àí \frac{RSS/(n-d-1)}{TSS/(n-1)}\]</span> <strong>Note</strong>:</p><ul><li>a large value of adjusted R2 indicates a model with a small test error. Maximizing the adjusted R2 is equivalent to minimizing <span class="math inline">\(RSS/(n‚àíd‚àí1)\)</span></li><li><span class="math inline">\(RSS/(n‚àíd‚àí1)\)</span> may increase or decrease, due to the presence of d in the denominator.</li></ul><p><strong>Intuition</strong>:</p><ul><li>once all of the correct variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS</li><li>Unlike the R2 statistic, the adjusted R2 statistic pays a price for the inclusion of unnecessary variables in the model</li></ul><h3 id="validation-and-cross-validation">Validation and Cross-Validation</h3><p>As an alternative to the approaches just discussed, we can compute the validation set error or the cross-validation error for each model under consideration, and then select the model for which the resulting estimated test error is smallest.</p><p><strong>Advantage over <span class="math inline">\(C_p, AIC, BIC\)</span></strong>:</p><ul><li>Direct estimate of the test error, and makes fewer assumptions about the true underlying model.</li><li>Used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom or hard to estimate the error variance œÉ2.</li></ul><p><strong>One-standard-error rule</strong>: We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.</p><ul><li><strong>Rationale</strong>: if a set of models appear to be more or less equally good, then we might as well choose the simplest model</li></ul><h1 id="shrinage-methods">Shrinage Methods</h1><h2 id="ridge-regression">Ridge Regression</h2><p><strong>Ridge regression</strong> shrinks the regression coeÔ¨Écients by imposing a penalty on their size.The ridge coeÔ¨Écients minimize a penalized residual sum of squares: <span class="math display">\[\begin{align}\hat{\beta}^{ridge}=argmin_\beta {\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p\beta_j^2}\end{align}\]</span></p><ul><li>Œª ‚â• 0 is a complexity parameter that controls the amount of shrinkage</li></ul><p>Writing the criterion in matrix form: <span class="math display">\[\begin{align}RSS(\lambda)=(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)+\lambda\beta^T\beta\end{align}\]</span> The ridge regression solutions: <span class="math display">\[\begin{align}\hat{\beta}^{ridge}=(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{I}\)</span> is the p√óp identity matrix</li></ul><p>Note:</p><ul><li>the ridge regression solution is again a linear function of <span class="math inline">\(\mathbf{y}\)</span>;</li><li>The solution adds a positive constant to the diagonal of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> before inversion, which makes the problem nonsingular.</li></ul><p>Recall least squares: <span class="math display">\[\begin{align}RSS=\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\end{align}\]</span> <strong>Ridge regression</strong> coefficient estimates <span class="math inline">\(\hat{\beta}^R\)</span> are the values that minimize <span class="math display">\[\begin{align}\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2+\lambda\sum_{j=1}^p\beta_j^2=RSS+\lambda\sum_{j=1}^p\beta_j^2\end{align}\]</span></p><p><strong>Trade-off:</strong></p><ol type="1"><li>Ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.</li><li><strong>shrinkage penalty</strong> <span class="math inline">\(\lambda\sum_{j=1}^p\beta_j^2\)</span> is small when Œ≤1, . . . , Œ≤p are close to zero, and so it has the effect of shrinking the estimates of Œ≤j towards zero</li></ol><p><strong>Standardization</strong>:</p><ul><li><p><strong>scale equivariant</strong>: The standard least squares coefficient estimates are scale equivariant: multiplying Xj by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of 1/c.</p></li><li><p><span class="math inline">\(X_{j,\lambda}^\beta\)</span> will depend not only on the value of Œª, but also on the scaling of the jth predictor, and the scaling of the other predictors. It is best to apply ridge regression after standardizing the predictors <span class="math display">\[\begin{align}\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}}\end{align}\]</span> The denominator is the estimated standard deviation of the jth predictor</p><p>The denominator is the estimated standard deviation of the jth predictor</p></li></ul><h3 id="ridge-regression-improves-over-least-squares">Ridge Regression Improves Over Least Squares</h3><ol type="1"><li><strong>bias-variance trade-off</strong></li></ol><ul><li>Ridge regression‚Äôs advantage over least squares is rooted in the bias-variance trade-off. As Œª increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.</li><li>At the least squares coefficient estimates, which correspond to ridge regression with Œª = 0, the variance is high but there is no bias. But as Œª increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias. <img src="./4.png" width="600"></li></ul><blockquote><p>Ridge regression works best in situations where the least squares estimates have high variance</p></blockquote><ol start="2" type="1"><li><strong>computational advantages over best subset selection</strong></li></ol><h3 id="singular-value-decomposition-svd">Singular value decomposition (SVD)</h3><p>The <strong>singular value decomposition (SVD)</strong> of the centered input matrix X gives us some additional insight into the nature of ridge regression. The SVD of the N √ó p matrix X has the form: <span class="math display">\[\begin{align}X=UDV^T\end{align}\]</span></p><ul><li>U: N√óp orthogonal matrices, with the columns of U spanning the column space of X</li><li>V: p√óp orthogonal matrices, the columns of V spanning the row space of X</li><li>D: p√óp diagonal matrix, with diagonal entries d1 ‚â• d2 ‚â•¬∑¬∑¬∑‚â• dp ‚â• 0 called the singular values of X. If one or more values dj =0,X is singular</li></ul><p>least squares Ô¨Åtted vector: <span class="math display">\[\begin{align}\mathbf{X}\hat{\beta}^{ls}&amp;=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \\&amp;=UDV^T (VD^TU^TUDV^T)^{-1}VD^TU^Ty \\&amp;=UDV^T (VD^TDV^T)^{-1}VD^TU^Ty \\&amp;=UDV^T (V^T)^{-1}D^{-1}(D^T)^{-1}V^{-1}VD^TU^Ty \\&amp;=\mathbf{U}\mathbf{U}^T\mathbf{y}\end{align}\]</span> Note: <span class="math inline">\(\mathbf{U}^T\mathbf{y}\)</span> are the coordinates of y with respect to the orthonormal basis U.</p><p>The ridge solutions: <span class="math display">\[\begin{align}\mathbf{X}\hat{\beta}^{ridge}&amp;=\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y} \\&amp;=UD(D^2+\lambda\mathbf{I})^{-1}D^TU^Ty \\&amp;=\sum_{j=1}^p\mathbf{u}_j\frac{d^2_j}{d^2_j+\lambda}\mathbf{u}^T_j\mathbf{y}\end{align}\]</span></p><ul><li><span class="math inline">\(\mathbf{u}_j\)</span> are the columns of U</li></ul><p>Note: ridge regression computes the coordinates of y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors <span class="math inline">\(\frac{d^2_j}{d^2_j+\lambda}\)</span></p><h4 id="what-does-a-small-value-of-d2_j-mean">What does a small value of <span class="math inline">\(d^2_j\)</span> mean?</h4><p>The SVD of the centered matrix X is another way of expressing the <strong>principal components</strong> of the variables in X. The sample covariance matrix is given by <span class="math inline">\(S=X^TX/N\)</span>, we have</p><p><strong>Eigen decomposition of <span class="math inline">\(X^TX\)</span>:</strong> <span class="math display">\[\begin{align}\mathbf{X}^T\mathbf{X}=VD^TU^TUDV^T=VD^2V^T\end{align}\]</span> The eigenvectors <span class="math inline">\(v_j\)</span> (columns of V) are also called the <strong>principal components</strong> (or Karhunen‚ÄìLoeve) directions of X. The Ô¨Årst principal component direction <span class="math inline">\(v_1\)</span> has the property that <span class="math inline">\(z_1=Xv_1\)</span> has the largest sample variance amongst all normalized linear combinations of the columns of X, which is: <span class="math display">\[\begin{align}Var(z_1)=Var(Xv_1)=\frac{d^2_1}{N}\end{align}\]</span> and in fact <span class="math inline">\(z_1=Xv_1=u_1d_1\)</span>. The derived variable <span class="math inline">\(z_1\)</span> is called the Ô¨Årst principal component of X, and hence <span class="math inline">\(u_1\)</span> is the normalized Ô¨Årst principal component.Subsequent principal components <span class="math inline">\(z_j\)</span> have maximum variance <span class="math inline">\(\frac{d^2_j}{N}\)</span>, subject to being orthogonal to the earlier ones.</p><p>Hence the small singular values <span class="math inline">\(d_j\)</span> correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most.</p><h3 id="eÔ¨Äective-degrees-of-freedom">EÔ¨Äective degrees of freedom</h3><p><span class="math display">\[\begin{align}df(\lambda)&amp;=tr[\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T] \\&amp;=tr[\mathbf{H}\lambda] \\&amp;=\sum^p_{j=1}\frac{d^2_j}{d^2_j+\lambda}\end{align}\]</span></p><p>This monotone decreasing function of Œª is the eÔ¨Äective degrees of freedom of the ridge regression Ô¨Åt. Usually in a linear-regression Ô¨Åt with p variables,the degrees-of-freedom of the Ô¨Åt is p, the number of free parameters.</p><p>Note that</p><blockquote><p>df(Œª)= p as Œª = 0 (no regularization)</p></blockquote><blockquote><p>df(Œª) ‚Üí 0 as Œª ‚Üí‚àû.</p></blockquote><h2 id="the-lasso">The Lasso</h2><p>The lasso coefficients, <span class="math inline">\(\hat{\beta}_\lambda^L\)</span>, minimize the quantity <span class="math display">\[\begin{align}\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2+\lambda\sum_{j=1}^p|\beta_j|=RSS+\lambda\sum_{j=1}^p|\beta_j|\end{align}\]</span></p><p>The lasso is a shrinkage method like ridge, with subtle but important differences.The lasso estimate is deÔ¨Åned by: <span class="math display">\[\begin{align}\hat{\beta}^{lasso}&amp;=argmin_\beta\sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2 \\&amp; s.t. \sum_{j=1}^p|\beta_j|\leq t\end{align}\]</span> Lasso problem in <em>Lagrangian form</em>: <span class="math display">\[\begin{align}\hat{\beta}^{lasso}&amp;=argmin_\beta\{ \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j| \}\end{align}\]</span></p><h4 id="section"></h4><h3 id="another-formulation-for-ridge-regression-and-the-lasso">Another Formulation for Ridge Regression and the Lasso</h3><p>The lasso and ridge regression coefficient estimates solve the problems <span class="math display">\[\begin{align}minimize_\beta \left\{\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right\}\,\, subject\, to \, \sum_{j=1}^p|\beta_j|\leq s \\minimize_beta \left\{\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right\}\,\, subject\, to \, \sum_{j=1}^p\beta_j^2\leq s\end{align}\]</span> When we perform the lasso we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large <span class="math inline">\(\sum_{j=1}^p|\beta_j|\)</span> can be. When s is extremely large, then this budget is not very restrictive, and so the coefficient estimates can be large</p><p><strong>A close connection between the lasso, ridge regression, and best subset selection</strong>:</p><p>best subset selection is equivelant to : <span class="math display">\[\begin{align}minimize_{beta} \left\{\sum_{i=1}^n\left( y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j \right)^2\right\}\,\, subject\, to \, \sum_{j=1}^pI(\beta_j\neq 0)\leq s\end{align}\]</span> Therefore, we can interpret <strong>ridge regression</strong> and <strong>the lasso</strong> as computationally feasible alternatives to <strong>best subset selection</strong>.</p><h3 id="the-variable-selection-property-of-the-lasso">The Variable Selection Property of the Lasso</h3><p>The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.</p><p><strong>ridge regression</strong>: <strong>circular</strong> constraint with no sharp points, so the ridge regression coefficient estimates will be exclusively non-zero.</p><p><strong>the lasso</strong>: constraint has <strong>corners</strong> at each of the axes, and so the ellipse will often intersect the constraint region at an axis.</p><ul><li>the <span class="math inline">\(l_1\)</span> penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter Œª is sufficiently large.</li><li>Hence, much like best subset selection, the lasso performs <strong>variable selection</strong></li></ul><blockquote><p>lasso yields <strong>sparse</strong> models</p></blockquote><p><img src="./5.png" width="600"></p><h3 id="comparing-the-lasso-and-ridge-regression">Comparing the Lasso and Ridge Regression</h3><p><strong>SAME</strong>: Ridge &amp; Lasso all can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions.</p><p><strong>DIFFERENCES</strong>:</p><ul><li>Unlike ridge regression, the <strong>lasso performs variable selection</strong>, and hence results in models that are easier to interpret.</li><li>Ridge regression outperforms the lasso in terms of prediction error in this setting</li></ul><p><strong>Suitable setting</strong>:</p><ul><li><strong>Lasso</strong>: perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero.</li><li><strong>Ridge regression</strong>: perform better when the response is a function of many predictors, all with coefficients of roughly equal size.</li><li>The number of predictors that is related to the response is never known a <strong>priori</strong> for real data sets. Cross-validation can be used in order to determine which approach is better on a particular data set.</li></ul><p><img src="./6.png" width="600"></p><p>The L2 ridge penalty <span class="math inline">\(\sum_{j=1}^p\beta_j^2\)</span> is replaced by the L1 lasso penalty <span class="math inline">\(\sum_{j=1}^p|\beta_j|\)</span>. This latter constraint makes the solutions nonlinear in the <span class="math inline">\(y_i\)</span>, and there is no closed form expression as in ridge regression.</p><blockquote><p>t should be adaptively chosen to minimize an estimate of expected prediction error.</p></blockquote><ul><li>if <span class="math inline">\(t&gt;t_0=\sum_{j=1}^p|\hat{\beta_j^{ls}}|\)</span>, then the lasso estimates are the <span class="math inline">\(\hat{\beta_j^{ls}}\)</span></li><li>if <span class="math inline">\(t&gt;t_0/2\)</span>, the least squares coeÔ¨Écients are shrunk by about 50% on average</li></ul><p>The standardized parameter: <span class="math inline">\(s=t/\sum_1^p|\hat{\beta_j}|\)</span></p><ul><li>s=1.0, the lasso coeÔ¨Écients are the least squares estimates</li><li>s-&gt;0, as the lasso coeÔ¨Écients -&gt;0</li></ul><h2 id="shrinkage-methods-v.s.-subset-selection"><strong>Shrinkage Methods v.s. Subset Selection</strong>:</h2><ul><li><strong>Subset selection</strong><ul><li>described involve using least squares to fit a linear model that contains a subset of the predictors.</li><li>are discrete process‚Äîvariables are either retained or discarded‚Äîit often exhibits high variance,and so doesn‚Äôt reduce the prediction error of the full model.</li></ul></li><li><strong>Shrinkage Methods</strong><ul><li>fit a model containing all p predictors by constraining or regularizing the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.</li><li>are more continuous, and don‚Äôt suÔ¨Äer as much from high variability.</li></ul></li></ul><h2 id="discussion-subset-selection-ridge-regression-and-the-lasso">Discussion: Subset Selection, Ridge Regression and the Lasso</h2><ul><li>Ridge regression: does a proportional shrinkage</li><li>Lasso: translates each coeÔ¨Écient by a constant factor Œª, truncating at zero --‚Äúsoft thresholding,‚Äù</li><li>Best-subset selection: drops all variables with coeÔ¨Écients smaller than the Mth largest --‚Äúhard-thresholding.‚Äù <img src="./lass_ridge.png" width="550"></li></ul><h3 id="bayes-view">Bayes View</h3><p>Consider the criterion <span class="math display">\[\begin{align}\tilde{\beta}&amp;=argmin_\beta\{ \sum_{i=1}^N(y_i-\beta_0-\sum_{j=1}^px_{ij}\beta_j)^2+\lambda\sum_{j=1}^p|\beta_j|^q \}\end{align}\]</span> for q ‚â• 0. The contours of constant value of <span class="math inline">\(\sum_{j=1}^p|\beta_j|^q\)</span> are shown in Figure 3.12, for the case of two inputs. &lt;img src=&quot;./images/q.png&quot;,width=550&gt;</p><p><font color="red">The lasso, ridge regression and best subset selection are Bayes estimates with diÔ¨Äerent priors:</font>Thinking of <span class="math inline">\(\sum_{j=1}^p|\beta_j|^q\)</span> as the log-prior density for Œ≤j , these are also the equi-contours of the prior distribution of the parameters.</p><ul><li>q = 0 :variable subset selection, as the penalty simply counts the number of nonzero parameters;</li><li>q = 1 :the lasso, also Laplace distribution for each input, with density <span class="math inline">\(\frac{1}{2\tau}exp(-|\beta|/\tau)\)</span>, where <span class="math inline">\(\tau=1/\lambda\)</span></li><li>q = 2 :the ridge</li></ul><h1 id="considerations-in-high-dimensions">Considerations In High Dimensions</h1><h2 id="high-dimensional-data">High-Dimensional Data</h2><p><strong>High-dimensional</strong>: Data sets containing more features than observations are often referred to as high-dimensional.</p><ul><li>Classical approaches such as least squares linear regression are not appropriate in this setting</li></ul><h2 id="what-goes-wrong-in-high-dimensions">What Goes Wrong in High Dimensions?</h2><ol type="1"><li>When the number of features p is as large as, or &gt;n, least squares cannot be performed.</li></ol><p><strong>Reason</strong>: regardless of whether or not there truly is a relationship between the features and the response, least squares will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero.</p><ul><li>This perfect fit will almost certainly lead to overfitting of the data</li><li>The problem is simple: when p &gt; n or p ‚âà n, a simple least squares regression line is too <strong><em>flexible</em></strong> and hence overfits the data.</li></ul><ol start="2" type="1"><li>Examines only the R2 or the training set MSE might erroneously conclude that the model with the greatest number of variables is best. <img src="./12 2.png" width="650"></li></ol><ul><li><strong>Cp, AIC, and BIC</strong> approaches are not appropriate in the high-dimensional setting, because estimating <span class="math inline">\(\hat{œÉ}^2\)</span> is problematic.(For instance, the formula for <span class="math inline">\(\hat{œÉ}^2\)</span> from Chapter 3 yields an estimate <span class="math inline">\(\hat{œÉ}^2\)</span> = 0 in this setting.)</li><li><strong>Adjusted <span class="math inline">\(R^2\)</span> </strong> in the high-dimensional setting is problematic, since one can easily obtain a model with an adjusted <span class="math inline">\(R^2\)</span> value of 1.</li></ul><h2 id="regression-in-high-dimensions">Regression in High Dimensions</h2><p><strong>Alternative approaches better-suited to the high-dimensional setting:</strong></p><ul><li>Forward stepwise selection</li><li>Ridge regression</li><li>The lasso</li><li>Principal components regression</li></ul><p><strong>Reason:</strong> These approaches avoid overfitting by using a less flexible fitting approach than least squares.</p><p><strong>Three important points:</strong> (1) Regularization or shrinkage plays a key role in high-dimensional problems,</p><ol start="2" type="1"><li><p>Appropriate tuning parameter selection is crucial for good predictive performance, and</p></li><li><p>The test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.<span class="math inline">\(\Rightarrow\)</span> <strong>curse of dimensionality</strong></p></li></ol><h3 id="curse-of-dimensionality">Curse of dimensionality</h3><p>Adding additional signal features that are truly associated with the response will improve the fitted model; However, adding noise features that are not truly associated with the response will lead to a deterioration in the fitted model.</p><p><strong>Reason</strong>: This is because noise features increase the dimensionality of the problem, exacerbating the risk of overfitting (since noise features may be assigned nonzero coefficients due to chance associations with the response on the training set) without any potential upside in terms of improved test set error.</p><h2 id="interpreting-results-in-high-dimensions">Interpreting Results in High Dimensions</h2><ol type="1"><li>Be cautious in reporting the results obtained when we perform the lasso, ridge regression, or other regression procedures in the high-dimensional setting.</li></ol><ul><li>In the high-dimensional setting, the <strong>multicollinearity</strong> problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model. This means that we can never know exactly which variables (if any) truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression.</li></ul><ol start="2" type="1"><li>Be cautious in reporting errors and measures of model fit in the high-dimensional setting</li></ol><ul><li>e.g.: when p &gt; n, it is easy to obtain a useless model that has zero residuals.</li><li><strong>One should never use sum of squared errors, p-values, R2 statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting</strong></li><li>It is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or R2 on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not.</li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;introduction-to-model-selection&quot;&gt;Introduction to Model Selection&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Setting:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In the regression setting, the standard linear model &lt;span class=&quot;math inline&quot;&gt;\(Y = Œ≤_0 + Œ≤_1X_1 + ¬∑ ¬∑ ¬∑ + Œ≤_pX_p + \epsilon\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the chapters that follow, we consider some approaches for extending the linear model framework.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Reason of using other fitting procedure than lease squares&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Prediction Accuracy:&lt;/em&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias.&lt;/li&gt;
&lt;li&gt;If n &lt;span class=&quot;math inline&quot;&gt;\(\gg\)&lt;/span&gt; p, least squares estimates tend to also have low variance &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; perform well on test data.&lt;/li&gt;
&lt;li&gt;If n is not much larger than p, least squares fit has large variance &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; overfitting &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; consequently poor predictions on test data&lt;/li&gt;
&lt;li&gt;If p &amp;gt; n, no more unique least squares coefficient estimate: the &lt;strong&gt;variance is infinite&lt;/strong&gt; so the method cannot be used at all&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By &lt;strong&gt;constraining&lt;/strong&gt; or &lt;strong&gt;shrinking&lt;/strong&gt; the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Model Interpretability&lt;/em&gt;&lt;/strong&gt;Ôºö
&lt;ul&gt;
&lt;li&gt;irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables‚Äîthat is, by setting the corresponding coefficient estimates to zero‚Äîwe can obtain a model that is more easily interpreted.&lt;/li&gt;
&lt;li&gt;least squares is extremely unlikely to yield any coefficient estimates that are exactly zero &lt;span class=&quot;math inline&quot;&gt;\(\Rightarrow\)&lt;/span&gt; &lt;strong&gt;feature selection&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Alternatives of lease squares:&lt;/strong&gt;&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Subset Selection&lt;/li&gt;
&lt;li&gt;Shrinkage&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://nancyyanyu.github.io/posts/cac93a23/&quot;&gt;Dimension Reduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Resampling Methods - Cross Validation, Bootstrap</title>
    <link href="https://nancyyanyu.github.io/posts/6d11b2f4/"/>
    <id>https://nancyyanyu.github.io/posts/6d11b2f4/</id>
    <published>2019-10-19T23:01:12.678Z</published>
    <updated>2019-10-19T23:21:29.596Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Resampling methods</strong>:involve repeatedly drawing samples from a training set and refitting a mode of interest on each sample in order to obtain additional information about the fitted model.</p><p><strong>model assessment</strong>Ôºö The process of evaluating a model‚Äôs performance</p><p><strong>model selection</strong>ÔºöThe process of selecting the proper level of flexibility for a model</p><p><strong>cross-validation</strong>: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.</p><p><strong>bootstrap</strong>:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method.</p><a id="more"></a><h1 id="cross-validation">Cross Validation</h1><h2 id="the-validation-set-approach">The Validation Set Approach</h2><p><strong>The Validation Set Approach</strong>:</p><ol type="1"><li><p>Randomly dividing the available set of observations into two parts, a <strong>training set</strong> and a <strong>validation set</strong> or hold-out set.</p></li><li><p>The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.</p></li><li><p>The resulting validation set error rate‚Äîtypically assessed using MSE in the case of a quantitative response‚Äîprovides an estimate of the test error rate.</p></li></ol><p><strong>Disadvantage</strong>:</p><ol type="1"><li><p>The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p></li><li><p>In the validation approach, only a subset of the observations‚Äîthose that are included in the training set rather than in the validation set‚Äîare used to fit the model. Since statistical methods tend to perform worse when trained on <em>fewer observations</em>, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.</p></li></ol><h2 id="k-fold-cross-validation">K-Fold Cross-Validation</h2><p><strong>Approach</strong>:</p><ol type="1"><li><p>Randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size.</p></li><li><p>The first fold is treated as a validation set, and the method is fit on the remaining k ‚àí 1 folds.</p></li><li><p>The mean squared error, MSE1, is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set.</p></li><li><p>This process results in k estimates of the test error, MSE1,MSE2, . . . ,MSEk.</p></li><li><p>The k-fold CV estimate is computed by averaging these values,</p></li></ol><p><span class="math display">\[\begin{align}CV_{(k)}=\frac{1}{k}\sum_{i=1}^kMSE_i\end{align}\]</span></p><p><strong>Goal</strong>Ôºö</p><ol type="1"><li><p>Determine how well a given statistical learning procedure can be expected to perform on independent data</p></li><li><p>We are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error.</p></li></ol><h2 id="bias-variance-trade-off-for-k-fold-cross-validation">Bias-Variance Trade-Off for k-Fold Cross-Validation</h2><p><strong>Leave-One-Out Cross-Validation V.S. k-Fold Cross-Validation</strong>: - k-Fold more biased than LOOCV - LOOCV will give approximately unbiased estimates of the test error, since each training set contains n ‚àí 1 observations, which is almost as many as the number of observations in the full data set. - k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias</p><ul><li>k-Fold less variance than LOOCV</li><li>When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other.</li><li>the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated</li></ul><h1 id="bootstrap">Bootstrap</h1><p><strong>Approach</strong>:</p><ol type="1"><li>A data set, which we call Z, that contains n observations. We randomly select n observations from the data set in order to produce a bootstrap data set, <span class="math inline">\(Z^{‚àó1}\)</span>.</li><li>The sampling is performed with <strong>replacement</strong>, which means that the replacement same observation can occur more than once in the bootstrap data set.</li></ol><ul><li>In this example, <span class="math inline">\(Z^{‚àó1}\)</span> contains the third observation twice, the first observation once, and no instances of the second observation.</li><li>Note that if an observation is contained in <span class="math inline">\(Z^{‚àó1}\)</span>, then both its X and Y values are included.</li></ul><ol start="3" type="1"><li><p>We can use <span class="math inline">\(Z^{‚àó1}\)</span> to produce a new bootstrap estimate for Œ±, which we call <span class="math inline">\(\alpha^{‚àó1}\)</span>. This procedure is repeated B times for some large value of B, in order to produce B different bootstrap data sets, <span class="math inline">\(Z^{‚àó1}\)</span>,<span class="math inline">\(Z^{‚àó2}\)</span>, . . . , <span class="math inline">\(Z^{‚àóB}\)</span>, and B corresponding Œ± estimates, <span class="math inline">\(\alpha^{‚àó1}\)</span>, <span class="math inline">\(\alpha^{‚àó2}\)</span>, . . . , <span class="math inline">\(\alpha^{‚àóB}\)</span>.</p></li><li><p>We can compute the standard error of these bootstrap estimates using the formula <span class="math display">\[\begin{align}SE_B(\hat{\alpha})=\sqrt{\frac{1}{B-1}\sum_{i=1}^B\left( \hat{\alpha}^{*i}-\frac{1}{B}\sum^{B}_{j=1}\hat{\alpha}^{*j} \right)}\end{align}\]</span> This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set.</p><p>This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set.</p></li></ol><p><img src="./1.png" width="600"></p><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Resampling methods&lt;/strong&gt;:involve repeatedly drawing samples from a training set and refitting a mode of interest on each sample in order to obtain additional information about the fitted model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model assessment&lt;/strong&gt;Ôºö The process of evaluating a model‚Äôs performance&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;model selection&lt;/strong&gt;ÔºöThe process of selecting the proper level of flexibility for a model&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;cross-validation&lt;/strong&gt;: can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;:provide a measure of accuracy of a parameter estimate or of a given selection statistical learning method.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Model Selection" scheme="https://nancyyanyu.github.io/tags/Model-Selection/"/>
    
      <category term="Cross Validation" scheme="https://nancyyanyu.github.io/tags/Cross-Validation/"/>
    
  </entry>
  
  <entry>
    <title>Study Note: Comparing Logistic Regression, LDA, QDA, and KNN</title>
    <link href="https://nancyyanyu.github.io/posts/6084c2b2/"/>
    <id>https://nancyyanyu.github.io/posts/6084c2b2/</id>
    <published>2019-10-19T23:00:54.556Z</published>
    <updated>2019-10-19T23:20:44.353Z</updated>
    
    <content type="html"><![CDATA[<h2 id="logistic-regression-and-lda-methods-are-closely-connected.">Logistic regression and LDA methods are closely connected.</h2><p><strong>Setting</strong>: Consider the two-class setting with \(p = 1\) predictor, and let \(p_1(x)\) and \(p_2(x) = 1‚àíp_1(x)\) be the probabilities that the observation \(X = x\) belongs to class 1 and class 2, respectively.</p><p>In LDA, from</p><p><span class="math display">\[\begin{align} p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_k)^2 \right)}}{\sum_{l=1}^K\pi\_l\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_l)^2 \right)}} \end{align}\]</span></p><p><span class="math display">\[\begin{align} \delta\_k(x)=x\frac{\\mu\_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k) \end{align}\]</span> The <strong>log odds</strong> is given by</p><p><span class="math display">\[\begin{align}\log{\frac{p_1(x)}{1-p_1(x)}}=\log{\frac{p_1(x)}{p_2(x)}}=c_0+c_1x \end{align}\]</span> where c0 and c1 are functions of Œº1, Œº2, and œÉ2.</p><p>In Logistic Regression,</p><p><span class="math display">\[\begin{align} \log{\frac{p_1}{1-p_1}}=\beta\_0+\beta_1x \end{align}\]</span> <a id="more"></a></p><p><strong>SAME</strong></p><ul><li><strong>Both logistic regression and LDA produce linear decision boundaries.</strong></li></ul><p><strong>DIFFERENCES</strong></p><ul><li><p>The only difference between the two approaches lies in the fact that Œ≤0 and Œ≤1 are estimated using maximum likelihood, whereas c0 and c1 are computed using the estimated mean and variance from a normal distribution. This same connection between LDA and logistic regression also holds for multidimensional data with p &gt; 1.</p></li><li><p>LDA assumes that the observations are drawn from a Gaussian distribution with a common covariance matrix in each class, and so can provide some improvements over logistic regression when this assumption approximately holds. Conversely, logistic regression can outperform LDA if these Gaussian assumptions are not met.</p></li></ul><h2 id="knn-dominate-lda-and-logistic-in-non-linear-setting">KNN dominate LDA and Logistic in non-linear setting</h2><p>In order to make a prediction for an observation X = x, the K training observations that are closest to x are identified. Then X is assigned to the class to which the plurality of these observations belong. Hence KNN is a completely <strong>non-parametric</strong> approach: <em>no assumptions are made about the shape of the decision boundary</em>.</p><blockquote><p>Therefore, we can expect KNN to dominate LDA and logistic regression when the decision boundary is highly non-linear.</p></blockquote><p>On the other hand, KNN does not tell us which predictors are important</p><h2 id="qda-serves-as-a-compromise-between-knn-lda-and-logistic-regression">QDA serves as a compromise between KNN, LDA and logistic regression</h2><p>QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. Since QDA assumes a quadratic decision boundary, it can accurately model a wider range of problems than can the linear methods. Though not as flexible as KNN, QDA can perform better in the presence of a <em>limited number of training observations</em> because it does make some assumptions about the form of the decision boundary.</p><p><img src="./images/17.png"></p><p><strong>Scenario 1</strong>: - 20 training observations in each of two classes. The observations within each class were uncorrelated random normal variables with a different mean in each class. - LDA performed well in this setting. KNN performed poorly because it paid a price in terms of variance that was not offset by a reduction in bias.</p><p><strong>Scenario 2</strong>: - Details are as in Scenario 1, except that within each class, the two predictors had a correlation of ‚àí0.5. - Little change in the relative performances of the methods as compared to the previous scenario.</p><p><strong>Scenario 3</strong>: - X1 and X2 are from the t-distribution, with 50 observations per class.</p><blockquote><p>The <strong>t-distribution</strong> has a similar shape to the normal distribution, but it has a tendency to yield more extreme points‚Äîthat is, more points that are far from the mean.</p></blockquote><ul><li>The decision boundary was still linear, and so fit into the logistic regression framework. The set-up violated the assumptions of LDA, since the observations were not drawn from a normal distribution. QDA results deteriorated considerably as a consequence of non-normality.</li></ul><p><strong>Scenario 4</strong>: - The data were generated from a normal distribution, with a correlation of 0.5 between the predictors in the first class, and correlation of ‚àí0.5 between the predictors in the second class. - This setup corresponded to the QDA assumption, and resulted in quadratic decision boundaries.</p><p><strong>Scenario 5</strong>: - Within each class, the observations were generated from a normal distribution with uncorrelated predictors. However, the responses were sampled from the logistic function using \(X^2_1 , X^2_2, and \, X1 √ó X2\) as predictors. - Consequently, there is a quadratic decision boundary. QDA once again performed best, followed closely by KNN-CV. The linear methods had poor performance.</p><p><strong>Scenario 6</strong>: - Details are as in the previous scenario, but the responses were sampled from a more complicated non-linear function. - Even the quadratic decision boundaries of QDA could not adequately model the data. - Much more flexible KNN-CV method gave the best results. But KNN with K = 1 gave the worst results out of all methods.</p><blockquote><p>This highlights the fact that <strong>even when the data exhibits a complex nonlinear relationship, a non-parametric method such as KNN can still give poor results if the level of smoothness is not chosen correctly.</strong></p></blockquote><h2 id="conclusion">Conclusion</h2><ul><li><p>When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well.</p></li><li><p>When the boundaries are moderately non-linear, QDA may give better results.</p></li><li><p>For much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. But the level of smoothness for a non-parametric approach must be chosen carefully.</p></li></ul><hr><p><strong>Ref:</strong></p><p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p><p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;logistic-regression-and-lda-methods-are-closely-connected.&quot;&gt;Logistic regression and LDA methods are closely connected.&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Setting&lt;/strong&gt;: Consider the two-class setting with \(p = 1\) predictor, and let \(p_1(x)\) and \(p_2(x) = 1‚àíp_1(x)\) be the probabilities that the observation \(X = x\) belongs to class 1 and class 2, respectively.&lt;/p&gt;
&lt;p&gt;In LDA, from&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align} p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_k)^2 \right)}}{\sum_{l=1}^K\pi\_l\frac{1}{\sqrt{2\pi}\sigma}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu_l)^2 \right)}} \end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align} \delta\_k(x)=x\frac{\\mu\_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k) \end{align}
\]&lt;/span&gt; The &lt;strong&gt;log odds&lt;/strong&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}\log{\frac{p_1(x)}{1-p_1(x)}}=\log{\frac{p_1(x)}{p_2(x)}}=c_0+c_1x \end{align}
\]&lt;/span&gt; where c0 and c1 are functions of Œº1, Œº2, and œÉ2.&lt;/p&gt;
&lt;p&gt;In Logistic Regression,&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align} \log{\frac{p_1}{1-p_1}}=\beta\_0+\beta_1x \end{align}
\]&lt;/span&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://nancyyanyu.github.io/categories/Machine-Learning/"/>
    
    
  </entry>
  
</feed>
