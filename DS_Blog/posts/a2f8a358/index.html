<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">

<script>
    (function(){
        if(''){
            if (prompt('Show me your password') !== ''){
                alert('Blah, wrong.');
                history.back();
            }
        }
    })();
</script>


<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" href="/lib/Han/dist/han.min.css?v=3.3">













  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">







  

<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"right","display":"hide","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: true,
    fastclick: true,
    lazyload: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="[TOC] Curse of dimensionality 1. Describe the curse of dimensionality with examples. Curse of dimensionality: as the dimensionality of the features space increases, the number configurations can grow">
<meta name="keywords" content="Interview">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Q&amp;A Part II: COD, Reg, Model Evaluation, Dimensionality Reduction">
<meta property="og:url" content="https://nancyyanyu.github.io/posts/a2f8a358/index.html">
<meta property="og:site_name" content="Nancy&#39;s Notes">
<meta property="og:description" content="[TOC] Curse of dimensionality 1. Describe the curse of dimensionality with examples. Curse of dimensionality: as the dimensionality of the features space increases, the number configurations can grow">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/a2f8a358/1.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/a2f8a358/2.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/a2f8a358/3.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/a2f8a358/5.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/a2f8a358/6.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/a2f8a358/7.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/a2f8a358/8.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/a2f8a358/9.png">
<meta property="og:updated_time" content="2019-10-19T23:13:26.292Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning Q&amp;A Part II: COD, Reg, Model Evaluation, Dimensionality Reduction">
<meta name="twitter:description" content="[TOC] Curse of dimensionality 1. Describe the curse of dimensionality with examples. Curse of dimensionality: as the dimensionality of the features space increases, the number configurations can grow">
<meta name="twitter:image" content="https://nancyyanyu.github.io/posts/a2f8a358/1.png">



  <link rel="alternate" href="/atom.xml" title="Nancy's Notes" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://nancyyanyu.github.io/posts/a2f8a358/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Machine Learning Q&A Part II: COD, Reg, Model Evaluation, Dimensionality Reduction | Nancy's Notes</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Nancy's Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Code changes world!</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-ml">

    
    
    
      
    

    

    <a href="/categories/Machine-Learning" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>ML</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-big-data">

    
    
    
      
    

    

    <a href="/categories/Big-Data" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Big Data</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-journal">

    
    
    
      
    

    

    <a href="/categories/Journal/" rel="section"><i class="menu-item-icon fa fa-fw fa-coffee"></i> <br>Journal</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/posts/a2f8a358/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Machine Learning Q&A Part II: COD, Reg, Model Evaluation, Dimensionality Reduction

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-15 05:53:38" itemprop="dateCreated datePublished" datetime="2019-06-15T05:53:38-05:00">2019-06-15</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-10-19 18:13:26" itemprop="dateModified" datetime="2019-10-19T18:13:26-05:00">2019-10-19</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">16k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">15 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <p>[TOC]</p>
<h2 id="curse-of-dimensionality">Curse of dimensionality</h2>
<h3 id="describe-the-curse-of-dimensionality-with-examples.">1. Describe the curse of dimensionality with examples.</h3>
<p><strong><em>Curse of dimensionality:</em></strong> as the dimensionality of the features space increases, the number configurations can grow exponentially, and thus the number of configurations covered by an observation decreases.</p>
<p><strong>As the number of feature or dimensions grows, the amount of data we need to generalise accurately grows exponentially.</strong></p>
<p>（fun example: It's easy to hunt a dog and maybe catch it if it were running around on the plain (two dimensions). It's much harder to hunt birds, which now have an extra dimension they can move in. If we pretend that ghosts are higher-dimensional beings ）</p>
<a id="more"></a>
<h3 id="what-is-local-constancy-or-smoothness-prior-or-regularization">2. What is local constancy or smoothness prior or regularization?</h3>
<p>(See DL Book 5.11.2)</p>
<p><strong>Smoothness prior</strong> or <strong>local constancy prior</strong>: This prior states that the function we learn should not change very much within a small region.</p>
<p>Many simpler algorithms rely exclusively on this prior to generalize well, and as a result they fail to scale to the statistical challenges involved in solving AIlevel tasks.</p>
<ul>
<li>KNN, decision trees, local kernel</li>
</ul>
<p>All of these different methods are designed to encourage the learning process to learn a function <span class="math inline">\(f^*\)</span> that satisfies the condition <span class="math display">\[
f^*(x)\approx f^*(x+\epsilon)
\]</span> In other words, if we know a good answer for an input x (for example, if x is a labeled training example) then that answer is probably good in the neighborhood of x.</p>
<p>Assuming only <strong>smoothness</strong> of the underlying function will not allow a learnerto represent a complex function that has many more regions to be distinguished than the number of training examples</p>
<h2 id="regularization">Regularization</h2>
<h3 id="what-is-l1-regularization">1. What is L1 regularization?</h3>
<p>L1 lasso penalty: <span class="math inline">\(\sum_{j=1}^p |\beta_j|\)</span></p>
<p>A type of regularization that penalizes weights in proportion to the <strong>sum of the absolute values</strong> of the weights. In models relying on <strong>sparse features</strong>, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model.</p>
<h3 id="what-is-l2-regularization">2. What is L2 regularization?</h3>
<p>L2 ridge penalty : <span class="math inline">\(\sum_{j=1}^p\beta_j^2\)</span></p>
<p>A type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. L2 regularization always improves generalization in linear models.</p>
<h3 id="compare-l1-and-l2-regularization.">3. Compare L1 and L2 regularization.</h3>
<p><strong>SAME</strong>: Ridge &amp; Lasso all can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions.</p>
<p><strong>DIFFERENCES</strong>:</p>
<ul>
<li>Unlike ridge regression, the <strong>lasso performs variable selection</strong>, and hence results in models that are easier to interpret.</li>
<li>ridge regression outperforms the lasso in terms of prediction error in this setting</li>
</ul>
<p><strong>Suitable setting</strong>:</p>
<ul>
<li><strong>Lasso</strong>: perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero.</li>
<li><strong>Ridge regression</strong>: perform better when the response is a function of many predictors, all with coefficients of roughly equal size.</li>
<li>The number of predictors that is related to the response is never known a <strong>priori</strong> for real data sets. Cross-validation can be used in order to determine which approach is better on a particular data set.</li>
</ul>
<h3 id="why-does-l1-regularization-result-in-sparse-models">4. Why does L1 regularization result in sparse models?</h3>
<p>The lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.</p>
<p><strong>Ridge regression</strong>: <strong>circular</strong> constraint with no sharp points, so the ridge regression coefficient estimates will be exclusively non-zero.</p>
<p><strong>The lasso</strong>: constraint has <strong>corners</strong> at each of the axes, and so the ellipse will often intersect the constraint region at an axis.</p>
<ul>
<li>The <span class="math inline">\(l_1\)</span> penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large.</li>
<li>Hence, much like best subset selection, the lasso performs <strong>variable selection</strong></li>
</ul>
<blockquote>
<p>Lasso yields <strong>sparse</strong> models</p>
</blockquote>
<p><img src="./1.png" width="600"></p>
<h2 id="evaluation-of-machine-learning-systems">Evaluation of Machine Learning systems</h2>
<h3 id="what-are-accuracy-sensitivity-specificity-roc-auc-confusion-matrix-f1-score">1. What are accuracy, sensitivity, specificity, ROC, AUC, Confusion matrix, F1-Score?</h3>
<p>(see ISLR Note - Linear Discriminant Analysis, ROC &amp; AUC, Confusion Matrix)</p>
<p><strong><em>Confusion matrix</em></strong>: An NxN table that summarizes how successful a <a href="https://developers.google.com/machine-learning/glossary/#classification_model" target="_blank" rel="noopener"><strong>classification model's</strong></a> predictions were</p>
<p><img src="./2.png"></p>
<p><strong><em>Accuracy</em></strong>: The fraction of predictions that a classification model got right.</p>
<ul>
<li>In multi-class classification, accuracy is defined as follows: <span class="math display">\[
Accuracy=\frac{Correct Predictions}{Total Number Of Observations}
\]</span></li>
</ul>
<p>In binary classification, accuracy has the following definition: <span class="math display">\[
Accuracy=\frac{TruePositives+TrueNegatives}{Total Number Of Observations}
\]</span> <strong><em>Sensitivity/ Recall/ TPR</em></strong>: A metric for classification models that answers the following question: Out of all the possible <em>positive labels</em>(positive under true condition), how many did the model correctly identify? <span class="math display">\[
Sensitivity=\frac{TruePositives}{TruePositives+FalseNegatives}
\]</span> <strong><em>Specificity/ Selectivity/ TNR</em></strong>: Out of all the possible <em>negative labels</em> (negative under true condition), how many did the model correctly identify? <span class="math display">\[
Specificity=\frac{TrueNegatives}{TrueNegatives+FalsePositives}
\]</span> <strong><em>Precision/ PPV</em></strong>: <span class="math display">\[
Precision=\frac{TruePositives}{TruePositives+FalsePositives}
\]</span> <strong><em>Type I and Type II Errors</em></strong>:</p>
<p>According to me, the null hypothesis in this case is that this call is a hoax. As a matter of fact, if Jack would have believed the stranger and provided his bank details, and the call was in fact a hoax, he would have committed a type I error, also known as a false positive. On the other hand, had he ignored the stranger’s request, but later found out that he actually had won the lottery and the call was not a hoax, he would have committed a Type II error, or a false negative.</p>
<p><strong>Recall v.s. Precision</strong>:</p>
<ul>
<li>Recall refers to the percentage of total relevant results correctly classified by your algorithm</li>
<li>Precision means the percentage of your results which are relevant.</li>
</ul>
<p><img src="./3.png" width="800"></p>
<blockquote>
<p><em>…Feeling a bit panicky, Jack called up his bank to ensure his existing accounts were safe and all his credits were secure. After listening to Jack’s story, the bank executive informed Jack that all his accounts were safe. However, in order to ensure that there is no future risk, the bank manager asked Jack to recall all instances in the last six months wherein he might have shared his account details with another person for any kind of transaction, or may have accessed his online account from a public system, etc…</em></p>
</blockquote>
<p>What are the chances that Jack will be able to <em>recall</em> all such instances <em>precisely</em>?</p>
<ul>
<li><p>If Jack had let’s say 10 such instances in reality, and he narrated 20 instances to finally spell out the 10 correct instances, then his <em>recall</em> will be a 100%, but his <em>precision</em> will only be 50%.</p></li>
<li><p><strong>Trade-off</strong>: If you have to <em>recall</em> everything, you will have to keep generating results which are not accurate, hence lowering your <em>precision</em>.</p></li>
</ul>
<p><strong><em>AUC - ROC curve</em></strong> is a performance measurement for classification problem at <strong>various thresholds settings</strong>.</p>
<p><strong><em>ROC</em></strong> is a probability curve and <strong><em>AUC</em></strong> represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. <span class="math display">\[
Recall=\frac{TP}{TP+FN} \\
Specificity=\frac{TN}{FP+TN} \\
FPR=1-Specificity=\frac{FP}{FP+TN}
\]</span></p>
<ul>
<li>An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to <em>have an AUC of 0.5</em></li>
<li>ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds.</li>
</ul>
<p><img src="./5.png" width="600"></p>
<p><strong><em>F1-Score</em></strong>: <span class="math display">\[
Recall=\frac{2 \cdot Recall \cdot Precision}{Recall+Precision}
\]</span></p>
<ul>
<li><strong>F1 Score</strong> : the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.
<ul>
<li>needed when you want to <em>seek a balance between Precision and Recall</em>, and there is an <strong>uneven class distribution</strong> <em>(large number of Actual Negatives</em>)</li>
</ul></li>
<li><strong>Difference between F1 Score and Accuracy</strong> : Accuracy can be largely contributed by a large number of <em>True Negatives</em> which in most business circumstances, we do not focus on much whereas False Negative and False Positive usually has business costs (tangible &amp; intangible) .Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall.</li>
</ul>
<h3 id="describe-t-test-in-the-context-of-machine-learning.">2. Describe t-test in the context of Machine Learning.</h3>
<p><strong>Hypothesis Tests</strong>:</p>
<p>The most common hypothesis test involves testing the <strong>null test hypothesis</strong> of</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_0: There is no relationship between X and Y or β1=0</span><br></pre></td></tr></table></figure>
<p>versus the <strong>alternative hypothesis</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H_a : There is some relationship between X and Y or β1≠0</span><br></pre></td></tr></table></figure>
<p>To test the null hypothesis, we need to determine whether <span class="math inline">\(\hat{\beta_1}\)</span>, our estimate for <span class="math inline">\(\beta_1\)</span>, is sufficiently far from zero that we can be confident that <span class="math inline">\(\beta_1\)</span> is non-zero <span class="math inline">\(\Rightarrow\)</span> it depends on SE( <span class="math inline">\(\hat{\beta_1}\)</span>)</p>
<ul>
<li>If SE( <span class="math inline">\(\hat{\beta_1}\)</span>) is small, then even relatively small values of <span class="math inline">\(\hat{\beta_1}\)</span> may provide strong evidence that <span class="math inline">\(\beta_1 \neq 0\)</span>, and hence that there is a relationship between X and Y</li>
</ul>
<p><strong>t-statistic</strong> <span class="math display">\[
\begin{align}
t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})} 
\end{align}
\]</span> which measures <strong>the number of standard deviations that <span class="math inline">\(\hat{\beta_1}\)</span> is away from 0.</strong>If there really is no relationship between X and Y , then we expect it will have a t-distribution with n−2 degrees of freedom.</p>
<p><strong>Standard error of <span class="math inline">\(\hat{\mu}\)</span> (SE(<span class="math inline">\(\hat{\mu}\)</span>)</strong>): average amount that this estimate <span class="math inline">\(\hat{\mu}\)</span> differs from the actual value of μ. <span class="math display">\[
\begin{align}
Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}
\end{align}
\]</span> where σ is the standard deviation of each of the realizations <span class="math inline">\(y_i\)</span> of <span class="math inline">\(Y\)</span> provided that the <span class="math inline">\(n\)</span> observations are <strong>uncorrelated</strong>.</p>
<p><strong>Standard Deviation V.S. Standard Error</strong></p>
<ul>
<li>The standard deviation (SD) measures the amount of variability, or dispersion, for a subject set of data from the mean</li>
<li>The standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean.</li>
</ul>
<p><img src="./6.png" width="300"></p>
<p><strong>P-value</strong></p>
<ul>
<li>The probability of observing any value <span class="math inline">\(≥ t\)</span> or <span class="math inline">\(≤ -t\)</span>, assuming <span class="math inline">\(β_1 = 0\)</span>.</li>
</ul>
<p><img src="./7.png" width="300"> (Here <span class="math inline">\(|t|=2.17\)</span>, p-value<span class="math inline">\(=0.015\)</span>.The area in red is <span class="math inline">\(0.015 + 0.015 = 0.030\)</span>, <span class="math inline">\(3\%\)</span>. If we had chosen a significance level of <span class="math inline">\(5\%\)</span>, this would mean that we had achieved statistical significance. We would reject the null hypothesis in favor of the alternative hypothesis.)</p>
<ul>
<li><strong>Interpretation</strong>:a small p-value indicates
<ul>
<li>It is unlikely to observe such a substantial association between the predictor and the response due to LUCK, in the absence of any real association between the predictor and the response.</li>
<li>There is an association between the predictor and the response.</li>
<li>We reject the null hypothesis—that is, we declare a relationship to exist between X and Y</li>
</ul></li>
</ul>
<h2 id="dimensionality-reduction">Dimensionality Reduction</h2>
<h3 id="why-do-we-need-dimensionality-reduction-techniques">1. Why do we need dimensionality reduction techniques?</h3>
<p>Because the <strong>curse of dimensionality</strong> demands that we do.</p>
<ol type="1">
<li>Less misleading data means <em>model accuracy</em> improves.</li>
<li>Less dimensions mean <em>less computing</em>. Less data means that <em>algorithms train faster</em>.</li>
<li>Less data means <em>less storage space</em> required.</li>
<li>Less dimensions allow usage of algorithms unfit for a large number of dimensions</li>
<li><p>Removes redundant features and noise.</p></li>
<li><p>PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).</p></li>
</ol>
<h3 id="what-do-we-need-pca-and-what-does-it-do">2. What do we need PCA and what does it do?</h3>
<p><strong><em>PCA</em></strong> : find a lower dimensional surface such the <strong>sum of the squared projection error</strong> is minimized</p>
<p><strong>PCA</strong> :finds a low-dimensional representation of a data set that contains as much as possible of the <strong>variation</strong></p>
<p>Each of the dimensions found by PCA is a linear combination of the <span class="math inline">\(p\)</span> features.</p>
<p><strong><em>The first principal component</em></strong> of a set of features <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> is the <strong>normalized</strong> linear combination of the features <span class="math display">\[
\begin{align}
Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p
\end{align}
\]</span> that has the <strong>largest variance</strong>.</p>
<!--more-->
<p><strong>Normalized</strong>: <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p>
<p><strong>Loadings</strong>: <span class="math inline">\(\phi_{11}, . . . , \phi_{p1}\)</span> the loadings of the first principal component;</p>
<ul>
<li>Together, the loadings make up the principal component loading vector, <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span></li>
</ul>
<h4 id="compute-the-first-principal-component">Compute the first principal component</h4>
<ul>
<li><p>Assume that each of the variables in <span class="math inline">\(X\)</span> has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form <span class="math display">\[
\begin{align}
z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+,,,+\phi_{p1}x_{ip} \quad \quad i=1,2,...,n
\end{align}
\]</span> that has largest sample variance, subject to the constraint that <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p></li>
<li><p>The first principal component loading vector solves the optimization problem <span class="math display">\[
\begin{align}
\max_{\phi_{11},...,\phi_{p1}}{\left\{ \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j1}x_{ij}   \right)^2 \right\}} \, subject \, to \, \sum_{j=1}^p \phi_{j1}^2=1
\end{align}
\]</span></p></li>
<li><p>Since <span class="math inline">\(\sum_{i=1}^nx_{ij}/n=1\)</span>, the average of the <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> will be zero as well. Hence the objective that we are maximizing is just the <strong>sample variance</strong> of the <span class="math inline">\(n\)</span> values of zi1</p></li>
<li><p><strong>Scores</strong>: We refer to <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> as the scores of the first principal component.</p></li>
</ul>
<p><strong>Geometric interpretation</strong>: for the first principal component: The loading vector <span class="math inline">\(\phi_1\)</span> with elements <span class="math inline">\(\phi_{11},\phi_{21},...,\phi_{p1}\)</span> defines a direction in feature space along which the data <strong>vary the most</strong>. If we project the <span class="math inline">\(n\)</span> data points <span class="math inline">\(x_1, . . . , x_n\)</span> onto this direction, the projected values are the principal component scores <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> themselves.</p>
<h4 id="compute-the-second-principal-component">Compute the second principal component</h4>
<p><strong>The second principal component <span class="math inline">\(Z_2\)</span></strong>: the linear combination of <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> that has maximal variance out of all linear combinations that are <strong>uncorrelated with <span class="math inline">\(Z_1\)</span></strong>.</p>
<p>The second principal component scores <span class="math inline">\(z_{12}, . . . , z_{n2}\)</span> take the form <span class="math display">\[
\begin{align}
z_{i2}=\phi_{12}x_{i1}+\phi_{22}x_{i2}+,,,+\phi_{p2}x_{ip} \quad \quad i=1,2,...,n
\end{align}
\]</span> where <span class="math inline">\(\phi_2\)</span> is the second principal component <strong>loading</strong> vector, with elements <span class="math inline">\(\phi_{12},\phi_{22},...,\phi_{p2}\)</span>.</p>
<p>It turns out that constraining <span class="math inline">\(Z_2\)</span> to be uncorrelated with <span class="math inline">\(Z_1\)</span> is equivalent to constraining the direction <span class="math inline">\(\phi_2\)</span> to be <strong>orthogonal</strong> (perpendicular) to the direction <span class="math inline">\(\phi_1\)</span>.</p>
<p>To find <span class="math inline">\(\phi_2\)</span>, we solve a problem similar to (10.3) with <span class="math inline">\(\phi_2\)</span> replacing <span class="math inline">\(\phi_1\)</span>, and with the additional constraint that <span class="math inline">\(\phi_2\)</span> is orthogonal to <span class="math inline">\(\phi_1\)</span></p>
<p><img src="./8.png" width="600"></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes.</li>
<li>Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three.</li>
<li>This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the UrbanPop variable is less correlated with the other three.</li>
</ul>
<h3 id="what-is-the-difference-between-logistic-regression-and-pca">3. What is the difference between logistic regression and PCA?</h3>
<ul>
<li>PCA will <strong>NOT consider</strong> the response variable but only the variance of the independent variables.</li>
<li>Logistic Regression will <strong>consider</strong> how each independent variable impact on response variable.</li>
</ul>
<h3 id="what-are-the-two-pre-processing-steps-that-should-be-applied-before-doing-pca">4. What are the two pre-processing steps that should be applied before doing PCA?</h3>
<p>Mean normalization, feature scaling: Before PCA is performed, the variables should be <strong>centered to have mean zero</strong>. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been <strong>individually scaled</strong> to have standard deviation one. (each multiplied by a different constant)</p>
<p><img src="./9.png" width="600"></p>
<p><strong>Ref</strong>:</p>
<p><a href="https://github.com/Sroy20/machine-learning-interview-questions" target="_blank" rel="noopener">machine-learning-interview-questions</a></p>
<p><a href="https://towardsdatascience.com/precision-vs-recall-386cf9f89488" target="_blank" rel="noopener">Precision vs Recall</a></p>
<p><a href="https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e" target="_blank" rel="noopener">A beginner’s guide to dimensionality reduction in Machine Learning</a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/Interview/" rel="tag"># Interview</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/6bd38994/" rel="next" title="Machine Learning Q&A Part I: Learning Theory & Model Selection">
                <i class="fa fa-chevron-left"></i> Machine Learning Q&A Part I: Learning Theory & Model Selection
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/posts/9c99c8b6/" rel="prev" title="Study Note: Clustering">
                Study Note: Clustering <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Nancy Yan</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">44</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">8</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">34</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://github.com/nancyyanyu" title="GitHub &rarr; https://github.com/nancyyanyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#curse-of-dimensionality"><span class="nav-number">1.</span> <span class="nav-text">Curse of dimensionality</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#describe-the-curse-of-dimensionality-with-examples."><span class="nav-number">1.1.</span> <span class="nav-text">1. Describe the curse of dimensionality with examples.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-local-constancy-or-smoothness-prior-or-regularization"><span class="nav-number">1.2.</span> <span class="nav-text">2. What is local constancy or smoothness prior or regularization?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regularization"><span class="nav-number">2.</span> <span class="nav-text">Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-l1-regularization"><span class="nav-number">2.1.</span> <span class="nav-text">1. What is L1 regularization?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-l2-regularization"><span class="nav-number">2.2.</span> <span class="nav-text">2. What is L2 regularization?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#compare-l1-and-l2-regularization."><span class="nav-number">2.3.</span> <span class="nav-text">3. Compare L1 and L2 regularization.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-does-l1-regularization-result-in-sparse-models"><span class="nav-number">2.4.</span> <span class="nav-text">4. Why does L1 regularization result in sparse models?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#evaluation-of-machine-learning-systems"><span class="nav-number">3.</span> <span class="nav-text">Evaluation of Machine Learning systems</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#what-are-accuracy-sensitivity-specificity-roc-auc-confusion-matrix-f1-score"><span class="nav-number">3.1.</span> <span class="nav-text">1. What are accuracy, sensitivity, specificity, ROC, AUC, Confusion matrix, F1-Score?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#describe-t-test-in-the-context-of-machine-learning."><span class="nav-number">3.2.</span> <span class="nav-text">2. Describe t-test in the context of Machine Learning.</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dimensionality-reduction"><span class="nav-number">4.</span> <span class="nav-text">Dimensionality Reduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#why-do-we-need-dimensionality-reduction-techniques"><span class="nav-number">4.1.</span> <span class="nav-text">1. Why do we need dimensionality reduction techniques?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-do-we-need-pca-and-what-does-it-do"><span class="nav-number">4.2.</span> <span class="nav-text">2. What do we need PCA and what does it do?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#compute-the-first-principal-component"><span class="nav-number">4.2.1.</span> <span class="nav-text">Compute the first principal component</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#compute-the-second-principal-component"><span class="nav-number">4.2.2.</span> <span class="nav-text">Compute the second principal component</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-the-difference-between-logistic-regression-and-pca"><span class="nav-number">4.3.</span> <span class="nav-text">3. What is the difference between logistic regression and PCA?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-are-the-two-pre-processing-steps-that-should-be-applied-before-doing-pca"><span class="nav-number">4.4.</span> <span class="nav-text">4. What are the two pre-processing steps that should be applied before doing PCA?</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nancy Yan</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="Symbols count total">518k</span>
  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





  



  






  



  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script>



  
  



  
  



  
  



  
  
  <script id="ribbon" size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>





  
  <script src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>

  
  <script src="/lib/three/three.min.js"></script>

  
  <script src="/lib/three/three-waves.min.js"></script>

  
  <script src="/lib/three/canvas_lines.min.js"></script>

  
  <script src="/lib/three/canvas_sphere.min.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  


  
    
<!-- LOCAL: You can save these files to your site and update links -->

  
  <script src="https://www.wenjunjiang.win/js/gitment.js"></script>

<link rel="stylesheet" href="https://www.wenjunjiang.win/css/gitment.css">
<!-- END LOCAL -->


<script>
  function renderGitment() {
    var gitment = new Gitment({
      id: window.location.pathname,
      owner: 'nancyyanyu',
      repo: 'nancyyanyu.github.io',
      
      oauth: {
      
      
        client_secret: '75adc257166813deff478053f3f05133285d6cf0',
      
        client_id: '90ddd3d00d8930cb0d84'
      }
    });
    gitment.render('gitment-container');
  }

  
    renderGitment();
  
</script>

  




  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>

<script type="text/javascript" src="/js/src/dynamic_bg.js"></script>

