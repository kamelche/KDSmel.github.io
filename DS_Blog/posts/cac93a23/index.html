<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">

<script>
    (function(){
        if(''){
            if (prompt('Show me your password') !== ''){
                alert('Blah, wrong.');
                history.back();
            }
        }
    })();
</script>


<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" href="/lib/Han/dist/han.min.css?v=3.3">













  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">







  

<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"right","display":"hide","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: true,
    fastclick: true,
    lazyload: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Dimension Reduction Methods Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp. Dimension Reduction Methods transform the predictors and then fit a least squares">
<meta name="keywords" content="Model Selection,PCA,Dimension Reduction">
<meta property="og:type" content="article">
<meta property="og:title" content="Study Note: Dimension Reduction - PCA, PCR">
<meta property="og:url" content="https://nancyyanyu.github.io/posts/cac93a23/index.html">
<meta property="og:site_name" content="Nancy&#39;s Notes">
<meta property="og:description" content="Dimension Reduction Methods Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp. Dimension Reduction Methods transform the predictors and then fit a least squares">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/cac93a23/7.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/cac93a23/8.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/cac93a23/9.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/cac93a23/1_v2.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/cac93a23/2_v2.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/cac93a23/3_v2.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/cac93a23/10.png">
<meta property="og:image" content="https://nancyyanyu.github.io/posts/cac93a23/11.png">
<meta property="og:updated_time" content="2019-10-19T23:20:56.964Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Study Note: Dimension Reduction - PCA, PCR">
<meta name="twitter:description" content="Dimension Reduction Methods Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp. Dimension Reduction Methods transform the predictors and then fit a least squares">
<meta name="twitter:image" content="https://nancyyanyu.github.io/posts/cac93a23/7.png">



  <link rel="alternate" href="/atom.xml" title="Nancy's Notes" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://nancyyanyu.github.io/posts/cac93a23/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Study Note: Dimension Reduction - PCA, PCR | Nancy's Notes</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Nancy's Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Code changes world!</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-ml">

    
    
    
      
    

    

    <a href="/categories/Machine-Learning" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>ML</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-big-data">

    
    
    
      
    

    

    <a href="/categories/Big-Data" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Big Data</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-journal">

    
    
    
      
    

    

    <a href="/categories/Journal/" rel="section"><i class="menu-item-icon fa fa-fw fa-coffee"></i> <br>Journal</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nancyyanyu.github.io/posts/cac93a23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nancy Yan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Nancy's Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Study Note: Dimension Reduction - PCA, PCR

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-10-19 18:03:29 / Modified: 18:20:56" itemprop="dateCreated datePublished" datetime="2019-10-19T18:03:29-05:00">2019-10-19</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">15k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">14 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h1 id="dimension-reduction-methods">Dimension Reduction Methods</h1>
<p>Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp.</p>
<p>Dimension Reduction Methods <strong><em>transform</em></strong> the predictors and then fit a least squares model using the transformed variables.</p>
<h2 id="approach">Approach</h2>
<p>Let <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span> represent <span class="math inline">\(M &lt; p\)</span> linear combinations of our original <span class="math inline">\(p\)</span> predictors. That is,</p>
<p><span class="math display">\[
\begin{align}
Z_m=\sum_{j=1}^p\phi_{jm}X_j
\end{align}
\]</span> <a id="more"></a></p>
<p>for some constants <span class="math inline">\(φ_{1m}, φ_{2m} . . . , φ_{pm}, m = 1, . . .,M.\)</span> We can then fit the linear regression model <span class="math display">\[
\begin{align}
y_i=\theta_0+\sum_{m=1}^M\theta_m z_{im}+\epsilon_i  \quad  i=1,2,3,4,...,n
\end{align}
\]</span> <strong>Dimension reduction</strong>: reduces the problem of estimating the <span class="math inline">\(p+1\)</span> coefficients <span class="math inline">\(β_0, β_1, . . . , β_p\)</span> to the simpler problem of estimating the <span class="math inline">\(M + 1\)</span> coefficients <span class="math inline">\(θ_0, θ_1, . . . , θ_M\)</span>, where M &lt; p. In other words, the dimension of the problem has been reduced from <span class="math inline">\(p + 1\)</span> to <span class="math inline">\(M + 1\)</span>. <span class="math display">\[
\begin{align}
\sum_{m=1}^M\theta_m z_{im}&amp;=\sum_{m=1}^M\theta_m \sum_{j=1}^p\phi_{jm}x_{ij}=\sum_{m=1}^M\sum_{j=1}^p\theta_m \phi_{jm}x_{ij}=\sum_{j=1}^p \beta_jx_{ij}  \\
\beta_j&amp;=\sum_{m=1}^M\theta_m \phi_{jm}
\end{align}
\]</span> <strong>All dimension reduction methods work in two steps:</strong></p>
<ol type="1">
<li>The transformed predictors <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>are obtained.</li>
<li>The model is fit using these <span class="math inline">\(M\)</span> predictors. However, the choice of <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>, or equivalently, the selection of the <span class="math inline">\(φ_{jm}\)</span>’s, can be achieved in different ways.</li>
</ol>
<h1 id="principal-components-regression">Principal Components Regression</h1>
<h2 id="an-overview-of-principal-components-analysis">An Overview of Principal Components Analysis</h2>
<p><strong>Principal component analysis (PCA)</strong> refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data.</p>
<ul>
<li>PCA also serves as a tool for data visualization (visualization of the observations or visualization of the variables).</li>
</ul>
<h2 id="what-are-principal-components">What Are Principal Components?</h2>
<p><strong>PCA</strong> :finds a low-dimensional representation of a data set that contains as much as possible of the <strong>variation</strong></p>
<p>Each of the dimensions found by PCA is a linear combination of the <span class="math inline">\(p\)</span> features.</p>
<p><strong><em>The first principal component</em></strong> of a set of features <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> is the normalized linear combination of the features <span class="math display">\[
\begin{align}
Z_1=\phi_{11}X_1+\phi_{21}X_2+,,,+\phi_{p1}X_p
\end{align}
\]</span> that has the <strong>largest variance</strong>.</p>
<p><strong>Normalized</strong>: <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p>
<p><strong>Loadings</strong>: <span class="math inline">\(\phi_{11}, . . . , \phi_{p1}\)</span> the loadings of the first principal component;</p>
<ul>
<li>Together, the loadings make up the principal component loading vector, <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span></li>
</ul>
<h3 id="st-principal-component">1st Principal Component</h3>
<h4 id="interpretation-1-greatest-variability">Interpretation 1: greatest variability</h4>
<p><strong>The first principal component</strong> direction of the data: is that along which the observations <strong>vary the most</strong>.</p>
<p><img src="./7.png" width="600"></p>
<p>The first principal component direction is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance</p>
<p><img src="./8.png" width="600"></p>
<p>The first principal component is given by the formula</p>
<p><span class="math display">\[
\begin{align}
Z_1 = 0.839 × (pop − \bar{pop}) + 0.544 × (ad − \bar{ad})
\end{align}
\]</span> Here <span class="math inline">\(φ_{11} = 0.839\)</span> and <span class="math inline">\(φ_{21} = 0.544\)</span> are the <strong>principal component loadings</strong>, which define the direction referred to above.</p>
<blockquote>
<p>The idea is that out of every possible linear combination of pop and ad such that <span class="math inline">\(\phi_{11}^2+\phi_{21}^2=1\)</span>, this particular linear combination yields the highest variance: i.e. this is the linear combination for which <span class="math inline">\(Var(φ_{11} × (pop − \bar{pop}) + φ_{21} × (ad − \bar{ad}))\)</span> is maximized.</p>
</blockquote>
<p><strong>Principal Component Scores</strong></p>
<p>The values of <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> are known as the <strong>principal component scores</strong>, and can be seen in the right-hand panel of Figure 6.15. For example, <span class="math display">\[
\begin{align}
z_{i1} = 0.839 × (pop_i − \bar{pop}) + 0.544 × (ad_i − \bar{ad})
\end{align}
\]</span></p>
<h4 id="interpretation-2-closest-to-data">Interpretation 2: closest to data</h4>
<p>There is also another interpretation for PCA: the first principal component vector defines the line that is as close as possible to the data.</p>
<p>In Figure 6.14, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line.</p>
<p>In the right-hand panel of Figure 6.15, the left-hand panel has been rotated so that the first principal component direction coincides with the x-axis. It is possible to show that the <strong><em>first principal component score</em></strong> for the ith observation is the distance in the <span class="math inline">\(x\)</span>-direction of the ith cross from zero.</p>
<h4 id="interpretation-3-single-number-summarization">Interpretation 3: single number summarization</h4>
<p>We can think of the values of the principal component <span class="math inline">\(Z_1\)</span> as single number summaries of the joint pop and ad budgets for each location.</p>
<p>In this example, if <span class="math inline">\(z_{i1} = 0.839 × (pop_i − pop) + 0.544 × (ad_i − ad) &lt; 0\)</span>, then this indicates a city with below-average population size and belowaverage ad spending.</p>
<p><img src="./9.png" width="650"></p>
<p>Figure 6.16 displays <span class="math inline">\(z_{i1}\)</span> versus both pop and ad. The plots show a strong relationship between the first principal component and the two features. In other words, the first principal component appears to <em>capture most of the information</em> contained in the pop and ad predictors.</p>
<h4 id="compute-the-first-principal-component">Compute the first principal component</h4>
<ul>
<li><p>Assume that each of the variables in <span class="math inline">\(X\)</span> has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form <span class="math display">\[
\begin{align}
z_{i1}=\phi_{11}x_{i1}+\phi_{21}x_{i2}+,,,+\phi_{p1}x_{ip} \quad \quad i=1,2,...,n
\end{align}
\]</span> that has largest sample variance, subject to the constraint that <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span></p></li>
<li><p>The first principal component loading vector solves the optimization problem <span class="math display">\[
\begin{align}
\max_{\phi_{11},...,\phi_{p1}}{\left\{ \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j1}x_{ij}   \right)^2 \right\}} \, subject \, to \, \sum_{j=1}^p \phi_{j1}^2=1
\end{align}
\]</span></p></li>
<li><p>Since <span class="math inline">\(\sum_{i=1}^nx_{ij}/n=1\)</span>, the average of the <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> will be zero as well. Hence the objective that we are maximizing is just the <strong>sample variance</strong> of the <span class="math inline">\(n\)</span> values of zi1</p></li>
<li><p><strong>Scores</strong>: We refer to <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> as the scores of the first principal component.</p></li>
</ul>
<p><strong>Geometric interpretation</strong>: for the first principal component: The loading vector <span class="math inline">\(\phi_1\)</span> with elements <span class="math inline">\(\phi_{11},\phi_{21},...,\phi_{p1}\)</span> defines a direction in feature space along which the data <strong>vary the most</strong>. If we project the n data points <span class="math inline">\(x_1, . . . , x_n\)</span> onto this direction, the projected values are the principal component scores <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> themselves.</p>
<h3 id="nd-principal-component">2nd Principal Component</h3>
<p>The s<strong>econd principal component <span class="math inline">\(Z_2\)</span></strong> is a linear combination of the variables that is uncorrelated with <span class="math inline">\(Z_1\)</span>, and has largest variance subject to this constraint.</p>
<p>It turns out that the zero correlation condition of <span class="math inline">\(Z_1\)</span> with <span class="math inline">\(Z_2\)</span> is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction.</p>
<p>The second principal component is given by the formula:</p>
<p><span class="math display">\[
\begin{align}
Z_2 = 0.544 × (pop − \bar{pop}) − 0.839 × (ad − \bar{ad}).
\end{align}
\]</span> Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that this component captures far less information.</p>
<h4 id="compute-the-second-principal-component">Compute the second principal component</h4>
<p><strong>The second principal component <span class="math inline">\(Z_2\)</span></strong>: the linear combination of <span class="math inline">\(X_1,X_2, . . . , X_p\)</span> that has maximal variance out of all linear combinations that are <strong>uncorrelated with <span class="math inline">\(Z_1\)</span></strong>.</p>
<p>The second principal component scores <span class="math inline">\(z_{12}, . . . , z_{n2}\)</span> take the form <span class="math display">\[
\begin{align}
z_{i2}=\phi_{12}x_{i1}+\phi_{22}x_{i2}+,,,+\phi_{p2}x_{ip} \quad \quad i=1,2,...,n
\end{align}
\]</span> where <span class="math inline">\(\phi_2\)</span> is the second principal component <strong>loading</strong> vector, with elements <span class="math inline">\(\phi_{12},\phi_{22},...,\phi_{p2}\)</span>.</p>
<p>It turns out that constraining <span class="math inline">\(Z_2\)</span> to be uncorrelated with <span class="math inline">\(Z_1\)</span> is equivalent to constraining the direction <span class="math inline">\(\phi_2\)</span> to be <strong>orthogonal</strong> (perpendicular) to the direction <span class="math inline">\(\phi_1\)</span>.</p>
<p>To find <span class="math inline">\(\phi_2\)</span>, we solve a problem similar to (10.3) with <span class="math inline">\(\phi_2\)</span> replacing <span class="math inline">\(\phi_1\)</span>, and with the additional constraint that <span class="math inline">\(\phi_2\)</span> is orthogonal to <span class="math inline">\(\phi_1\)</span></p>
<p><img src="./1_v2.png" width="600"></p>
<p><strong>Interpretation:</strong></p>
<ul>
<li>1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes.</li>
<li>Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three.</li>
<li>This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the UrbanPop variable is less correlated with the other three.</li>
</ul>
<h2 id="another-interpretation-of-principal-components">Another Interpretation of Principal Components</h2>
<p><strong>An alternative interpretation for principal components</strong>: principal components provide low-dimensional linear surfaces that are closest to the observations</p>
<ul>
<li><p><strong>The first principal component loading vector has a very special property</strong>: it is the line in p-dimensional space that is closest to the n observations (using average squared Euclidean distance as a measure of closeness).</p></li>
<li><p>The appeal of this interpretation : we seek a single dimension of the data that lies as close as possible to all of the data points, since such a line will likely provide a good summary of the data.</p></li>
<li><p><strong>The first two principal components</strong> of a data set <strong>span the plane</strong> that is closest to the n observations, in terms of average squared Euclidean distance</p></li>
<li><p>Together <strong>the first M principal component</strong> score vectors and the first M principal component loading vectors provide the best M-dimensional approximation (in terms of Euclidean distance) to the ith observation <span class="math inline">\(x_{ij}\)</span> . <span class="math display">\[
\begin{align}
x_{ij} \approx \sum_{m=1}^Mz_{im}\phi_{jm}
\end{align}
\]</span> (assuming the original data matrix X is column-centered).</p></li>
<li><p>When <span class="math inline">\(M = min(n − 1, p)\)</span>, then the representation is exact: <span class="math inline">\(x_{ij} = \sum_{m=1}^Mz_{im}\phi_{jm}\)</span></p></li>
</ul>
<h2 id="more-on-pca">More on PCA</h2>
<h3 id="scaling-the-variables">Scaling the Variables</h3>
<p>Before PCA is performed, the variables should be <strong>centered to have mean zero</strong>. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been <strong>individually scaled</strong> (each multiplied by a different constant)</p>
<p><img src="./2_v2.png" width="600"></p>
<h3 id="uniqueness-of-the-principal-components">Uniqueness of the Principal Components</h3>
<p><strong>Each principal component loading vector <span class="math inline">\(\phi_1=(\phi_{11},\phi_{21},...,\phi_{p1})^T\)</span> and the score vectors <span class="math inline">\(z_{11}, . . . , z_{n1}\)</span> is unique, up to a sign flip. </strong></p>
<ul>
<li>Two different software packages will yield the same principal component loading vectors and score vectors, although the signs of those loading vectors may differ.</li>
<li><strong>The signs may differ</strong> because each principal component loading vector specifies a direction in p-dimensional space: flipping the sign has no effect as the direction does not change.</li>
</ul>
<h3 id="the-proportion-of-variance-explained">The Proportion of Variance Explained</h3>
<p><strong>How much of the variance in the data is not contained in the first few principal components?</strong></p>
<p><strong>Proportion of variance explained (PVE)</strong> by each principal component:</p>
<ul>
<li>The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as</li>
</ul>
<p><span class="math display">\[
\begin{align}
\sum_{j=1}^pVar(X_j)=\sum_{j=1}^p\frac{1}{n}\sum_{i=1}^nx_{ij}^2
\end{align}
\]</span></p>
<ul>
<li>The variance explained by the mth principal component is</li>
</ul>
<p><span class="math display">\[
\begin{align}
\frac{1}{n}\sum_{i=1}^nz_{im}^2=\frac{1}{n}\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2
\end{align}
\]</span></p>
<ul>
<li>Therefore, the <strong>PVE of the mth principal component</strong> is given by</li>
</ul>
<p><span class="math display">\[
\begin{align}
\frac{\sum_{i=1}^n \left( \sum_{j=1}^p \phi_{jm}x_{ij} \right)^2}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2}
\end{align}
\]</span></p>
<p>The PVE of each principal component is a positive quantity. In order to compute the <strong>cumulative PVE</strong> of the first <span class="math inline">\(M\)</span> principal components, we can simply sum (10.8) over each of the first <span class="math inline">\(M\)</span> PVEs. In total, there are <span class="math inline">\(min(n − 1, p)\)</span> principal components, and their PVEs sum to one.</p>
<p><img src="./3_v2.png" width="600"></p>
<h3 id="deciding-how-many-principal-components-to-use">Deciding How Many Principal Components to Use</h3>
<p>We would like to use the smallest number of principal components required to get a good understanding of the data.</p>
<p><strong>How many principal components are needed?</strong></p>
<ul>
<li>We typically decide on the number of principal components required to visualize the data by examining a <strong>scree plot</strong> (Right FIGURE 10.4)</li>
<li>We choose the smallest number of principal components that are required in order to explain a sizable amount of the variation in the data.</li>
<li>We tend to look at the first few principal components in order to find interesting patterns in the data. If no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest.</li>
</ul>
<h2 id="the-principal-components-regression-approach">The Principal Components Regression Approach</h2>
<p>The principal components regression (PCR) approach involves constructing the first M principal components, <span class="math inline">\(Z_1,Z_2, . . . ,Z_M\)</span>, and then using these components as the predictors in a linear regression model that is fit using least squares</p>
<p><strong>The key idea</strong></p>
<p>Often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that <strong><em>the directions in which <span class="math inline">\(X_1, . . .,X_p\)</span> show the most variation are the directions that are associated with <span class="math inline">\(Y\)</span></em></strong></p>
<p><strong>Example</strong>:</p>
<p><img src="./10.png" width="650"></p>
<ul>
<li>Performing PCR with an appropriate choice of M can result in a substantial improvement over least squares</li>
<li>PCR does not perform as well as the two shrinkage methods
<ul>
<li><strong>Reason</strong>: The data were generated in such a way that many principal components are required in order to adequately model the response. In contrast, PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response.</li>
</ul></li>
</ul>
<p><strong>Note</strong>: even though PCR provides a simple way to perform regression using <span class="math inline">\(M &lt; p\)</span> predictors, it is not a <em>feature selection</em> method!</p>
<ul>
<li>This is because each of the <span class="math inline">\(M\)</span> principal components used in the regression is a linear combination of all p of the original features.</li>
<li>PCR is more closely related to ridge regression than to the lasso. One can even think of ridge regression as a continuous version of PCR!</li>
</ul>
<p><strong>Cross-validation</strong>: In PCR, the number of principal components, <span class="math inline">\(M\)</span>, is typically chosen by cross-validation.</p>
<p><img src="./11.png" width="650"></p>
<p><strong>Standardisation</strong>: When performing PCR, we generally recommend standardizing each predictor, prior to generating the principal components. - In the absence of standardization, the <em>high-variance variables</em> will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model.</p>
<hr>
<p><strong>Ref:</strong></p>
<p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p>
<p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/Model-Selection/" rel="tag"># Model Selection</a>
          
            <a href="/tags/PCA/" rel="tag"># PCA</a>
          
            <a href="/tags/Dimension-Reduction/" rel="tag"># Dimension Reduction</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/6b588a86/" rel="next" title="Study Note: Decision Trees, Random Forest, and Boosting">
                <i class="fa fa-chevron-left"></i> Study Note: Decision Trees, Random Forest, and Boosting
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/posts/2a71b2a0/" rel="prev" title="Study Note: Bias, Variance and Model Complexity">
                Study Note: Bias, Variance and Model Complexity <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Nancy Yan</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">44</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">8</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">34</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://github.com/nancyyanyu" title="GitHub &rarr; https://github.com/nancyyanyu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#dimension-reduction-methods"><span class="nav-number">1.</span> <span class="nav-text">Dimension Reduction Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#approach"><span class="nav-number">1.1.</span> <span class="nav-text">Approach</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#principal-components-regression"><span class="nav-number">2.</span> <span class="nav-text">Principal Components Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#an-overview-of-principal-components-analysis"><span class="nav-number">2.1.</span> <span class="nav-text">An Overview of Principal Components Analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#what-are-principal-components"><span class="nav-number">2.2.</span> <span class="nav-text">What Are Principal Components?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#st-principal-component"><span class="nav-number">2.2.1.</span> <span class="nav-text">1st Principal Component</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#interpretation-1-greatest-variability"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">Interpretation 1: greatest variability</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#interpretation-2-closest-to-data"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">Interpretation 2: closest to data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#interpretation-3-single-number-summarization"><span class="nav-number">2.2.1.3.</span> <span class="nav-text">Interpretation 3: single number summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#compute-the-first-principal-component"><span class="nav-number">2.2.1.4.</span> <span class="nav-text">Compute the first principal component</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nd-principal-component"><span class="nav-number">2.2.2.</span> <span class="nav-text">2nd Principal Component</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#compute-the-second-principal-component"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">Compute the second principal component</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#another-interpretation-of-principal-components"><span class="nav-number">2.3.</span> <span class="nav-text">Another Interpretation of Principal Components</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#more-on-pca"><span class="nav-number">2.4.</span> <span class="nav-text">More on PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#scaling-the-variables"><span class="nav-number">2.4.1.</span> <span class="nav-text">Scaling the Variables</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#uniqueness-of-the-principal-components"><span class="nav-number">2.4.2.</span> <span class="nav-text">Uniqueness of the Principal Components</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-proportion-of-variance-explained"><span class="nav-number">2.4.3.</span> <span class="nav-text">The Proportion of Variance Explained</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deciding-how-many-principal-components-to-use"><span class="nav-number">2.4.4.</span> <span class="nav-text">Deciding How Many Principal Components to Use</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-principal-components-regression-approach"><span class="nav-number">2.5.</span> <span class="nav-text">The Principal Components Regression Approach</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nancy Yan</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="Symbols count total">518k</span>
  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





  



  






  



  
    
    
      
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script>



  
  



  
  



  
  



  
  
  <script id="ribbon" size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>





  
  <script src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script>

  
  <script src="//cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>

  
  <script src="/lib/three/three.min.js"></script>

  
  <script src="/lib/three/three-waves.min.js"></script>

  
  <script src="/lib/three/canvas_lines.min.js"></script>

  
  <script src="/lib/three/canvas_sphere.min.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  


  
    
<!-- LOCAL: You can save these files to your site and update links -->

  
  <script src="https://www.wenjunjiang.win/js/gitment.js"></script>

<link rel="stylesheet" href="https://www.wenjunjiang.win/css/gitment.css">
<!-- END LOCAL -->


<script>
  function renderGitment() {
    var gitment = new Gitment({
      id: window.location.pathname,
      owner: 'nancyyanyu',
      repo: 'nancyyanyu.github.io',
      
      oauth: {
      
      
        client_secret: '75adc257166813deff478053f3f05133285d6cf0',
      
        client_id: '90ddd3d00d8930cb0d84'
      }
    });
    gitment.render('gitment-container');
  }

  
    renderGitment();
  
</script>

  




  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>

<script type="text/javascript" src="/js/src/dynamic_bg.js"></script>

